{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"PSplines: Penalized B-Spline Smoothing for Python","text":"<p>PSplines is a high-performance Python library for univariate penalized B-spline (P-spline) smoothing, implementing the methods described in Eilers &amp; Marx (2021). It provides efficient sparse-matrix implementations with analytical uncertainty quantification, parametric bootstrap, and Bayesian inference capabilities.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Fast Sparse Implementation: Uses SciPy sparse matrices and optimized solvers</li> <li>Multiple Uncertainty Methods: Analytical (delta method), bootstrap, and Bayesian approaches  </li> <li>Flexible Configuration: Customizable basis functions, penalty orders, and constraints</li> <li>Derivative Computation: Efficient computation of spline derivatives with uncertainty</li> <li>Automatic Parameter Selection: Cross-validation, AIC, L-curve, and V-curve methods</li> <li>Boundary Constraints: Support for derivative boundary conditions</li> <li>Comprehensive Validation: Extensive input validation and error handling</li> </ul>"},{"location":"#quick-example","title":"Quick Example","text":"<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom psplines import PSpline\n\n# Generate sample data\nnp.random.seed(42)\nx = np.linspace(0, 2*np.pi, 100)\ny = np.sin(x) + 0.1 * np.random.randn(100)\n\n# Create and fit P-spline\nspline = PSpline(x, y, nseg=20, lambda_=1.0)\nspline.fit()\n\n# Make predictions with uncertainty\nx_new = np.linspace(0, 2*np.pi, 200)\ny_pred, se = spline.predict(x_new, return_se=True)\n\n# Plot results\nplt.figure(figsize=(10, 6))\nplt.scatter(x, y, alpha=0.5, label='Data')\nplt.plot(x_new, y_pred, 'r-', label='P-spline fit')\nplt.fill_between(x_new, y_pred - 1.96*se, y_pred + 1.96*se, \n                 alpha=0.3, label='95% CI')\nplt.legend()\nplt.show()\n</code></pre>"},{"location":"#installation","title":"Installation","text":""},{"location":"#using-pip","title":"Using pip","text":"<pre><code>pip install psplines\n</code></pre>"},{"location":"#from-source","title":"From source","text":"<pre><code>git clone https://github.com/graysonbellamy/psplines.git\ncd psplines\npip install -e .\n</code></pre>"},{"location":"#documentation-structure","title":"Documentation Structure","text":"<ul> <li>User Guide: Start here if you're new to P-splines</li> <li>Tutorials: Step-by-step examples and walkthroughs</li> <li>Examples: Complete application examples</li> <li>API Reference: Detailed documentation of all classes and functions</li> <li>Mathematical Background: Theory and algorithms</li> </ul>"},{"location":"#what-are-p-splines","title":"What are P-Splines?","text":"<p>P-splines (Penalized B-splines) are a powerful smoothing technique that combines:</p> <ul> <li>B-spline basis functions: Flexible, local basis functions</li> <li>Difference penalties: Regularization to control smoothness</li> <li>Automatic parameter selection: Data-driven smoothing parameter choice</li> </ul> <p>The method solves the penalized least squares problem:</p> \\[\\min_\\alpha \\|y - B\\alpha\\|^2 + \\lambda \\|D_p \\alpha\\|^2\\] <p>where: - \\(B\\) is the B-spline basis matrix - \\(\\alpha\\) are the B-spline coefficients - \\(D_p\\) is the \\(p\\)-th order difference matrix - \\(\\lambda\\) is the smoothing parameter</p>"},{"location":"#use-cases","title":"Use Cases","text":"<p>PSplines are ideal for:</p> <ul> <li>Signal processing: Noise reduction and trend extraction</li> <li>Time series analysis: Smooth trend estimation and forecasting</li> <li>Scientific computing: Data smoothing and derivative estimation  </li> <li>Statistics: Nonparametric regression and curve fitting</li> <li>Engineering: Control system design and signal analysis</li> </ul>"},{"location":"#performance","title":"Performance","text":"<p>PSplines leverages sparse matrix operations for efficiency:</p> <ul> <li>Memory efficient: Uses sparse matrices throughout</li> <li>Fast computation: Optimized linear algebra operations</li> <li>Scalable: Handles large datasets effectively</li> <li>Parallel processing: Bootstrap uncertainty with multiprocessing</li> </ul>"},{"location":"#getting-help","title":"Getting Help","text":"<ul> <li>Issues: Report bugs and request features on GitHub Issues</li> <li>Discussions: Ask questions on GitHub Discussions</li> <li>Documentation: Browse this documentation for guides and examples</li> </ul>"},{"location":"#citation","title":"Citation","text":"<p>If you use PSplines in your research, please cite:</p> <pre><code>@software{bellamy2024psplines,\n  author = {Bellamy, Grayson},\n  title = {PSplines: Penalized B-Spline Smoothing for Python},\n  year = {2024},\n  url = {https://github.com/graysonbellamy/psplines}\n}\n</code></pre>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the MIT License - see the LICENSE file for details.</p>"},{"location":"contributing/","title":"Contributing to PSplines","text":"<p>Thank you for your interest in contributing to PSplines! This guide will help you get started with contributing to the project.</p>"},{"location":"contributing/#ways-to-contribute","title":"Ways to Contribute","text":"<p>There are many ways to contribute to PSplines:</p> <ul> <li>Report bugs or suggest features via GitHub Issues</li> <li>Improve documentation by fixing typos, adding examples, or clarifying explanations</li> <li>Add new features such as additional optimization methods or smoothing techniques</li> <li>Fix bugs identified in the issue tracker</li> <li>Add examples demonstrating real-world applications</li> <li>Improve performance through code optimization or algorithmic improvements</li> </ul>"},{"location":"contributing/#development-setup","title":"Development Setup","text":""},{"location":"contributing/#1-fork-and-clone","title":"1. Fork and Clone","text":"<ol> <li>Fork the repository on GitHub</li> <li>Clone your fork locally: <pre><code>git clone https://github.com/YOUR_USERNAME/psplines.git\ncd psplines\n</code></pre></li> </ol>"},{"location":"contributing/#2-development-environment","title":"2. Development Environment","text":"<p>We recommend using uv for development:</p> <pre><code># Install uv if you haven't already\npip install uv\n\n# Create development environment\nuv sync --dev\n\n# Activate the environment\nsource .venv/bin/activate  # Linux/macOS\n# or\n.venv\\Scripts\\activate  # Windows\n</code></pre> <p>Alternatively, with pip: <pre><code>pip install -e .[dev]\n</code></pre></p>"},{"location":"contributing/#3-verify-installation","title":"3. Verify Installation","text":"<p>Run the tests to make sure everything is working: <pre><code>pytest tests/\n</code></pre></p>"},{"location":"contributing/#development-workflow","title":"Development Workflow","text":""},{"location":"contributing/#1-create-a-branch","title":"1. Create a Branch","text":"<p>Create a new branch for your contribution: <pre><code>git checkout -b feature/your-feature-name\n# or\ngit checkout -b fix/issue-number\n</code></pre></p>"},{"location":"contributing/#2-make-changes","title":"2. Make Changes","text":"<p>Make your changes following the project's coding standards:</p> <ul> <li>Code Style: We use ruff for linting and formatting</li> <li>Type Hints: Use type hints for all new code</li> <li>Docstrings: Follow NumPy docstring conventions</li> <li>Tests: Add tests for new functionality</li> </ul>"},{"location":"contributing/#3-run-quality-checks","title":"3. Run Quality Checks","text":"<p>Before submitting, run the quality checks:</p> <pre><code># Lint code\nruff check src/ tests/\n\n# Format code\nruff format src/ tests/\n\n# Type checking\nmypy src/psplines\n\n# Run tests\npytest tests/ -v\n\n# Check test coverage\npytest tests/ --cov=psplines --cov-report=html\n</code></pre>"},{"location":"contributing/#4-commit-changes","title":"4. Commit Changes","text":"<p>Make clear, descriptive commit messages: <pre><code>git add .\ngit commit -m \"Add cross-validation method for parameter selection\n\n- Implement k-fold and leave-one-out CV\n- Add comprehensive tests\n- Update documentation with examples\"\n</code></pre></p>"},{"location":"contributing/#5-push-and-create-pull-request","title":"5. Push and Create Pull Request","text":"<pre><code>git push origin your-branch-name\n</code></pre> <p>Then create a pull request on GitHub with: - Clear description of changes - Reference to related issues (if any) - Screenshots for UI changes (if applicable)</p>"},{"location":"contributing/#coding-standards","title":"Coding Standards","text":""},{"location":"contributing/#code-style","title":"Code Style","text":"<p>We use <code>ruff</code> for both linting and formatting:</p> <pre><code># Check for issues\nruff check .\n\n# Auto-fix issues\nruff check --fix .\n\n# Format code\nruff format .\n</code></pre> <p>Configuration is in <code>pyproject.toml</code>.</p>"},{"location":"contributing/#type-hints","title":"Type Hints","text":"<p>Use type hints for all functions and methods:</p> <pre><code>from typing import Optional, Tuple, Union\nimport numpy as np\nfrom numpy.typing import ArrayLike\n\ndef predict(\n    self, \n    x: ArrayLike, \n    return_se: bool = False,\n    se_method: str = 'analytic'\n) -&gt; Union[np.ndarray, Tuple[np.ndarray, np.ndarray]]:\n    \"\"\"Predict values at new points.\n\n    Parameters\n    ----------\n    x : ArrayLike\n        Points at which to evaluate the spline\n    return_se : bool, default False\n        Whether to return standard errors\n    se_method : str, default 'analytic'\n        Method for computing standard errors\n\n    Returns\n    -------\n    predictions : np.ndarray or tuple\n        Predicted values, optionally with standard errors\n    \"\"\"\n</code></pre>"},{"location":"contributing/#documentation","title":"Documentation","text":""},{"location":"contributing/#docstrings","title":"Docstrings","text":"<p>Use NumPy-style docstrings:</p> <pre><code>def cross_validation(\n    spline: PSpline,\n    lambda_min: float = 1e-6,\n    lambda_max: float = 1e3,\n    n_lambda: int = 50\n) -&gt; Tuple[float, float]:\n    \"\"\"Find optimal smoothing parameter using cross-validation.\n\n    Parameters\n    ----------\n    spline : PSpline\n        P-spline object to optimize\n    lambda_min : float, default 1e-6\n        Minimum lambda value to test\n    lambda_max : float, default 1e3\n        Maximum lambda value to test\n    n_lambda : int, default 50\n        Number of lambda values to test\n\n    Returns\n    -------\n    optimal_lambda : float\n        Optimal smoothing parameter\n    cv_score : float\n        Cross-validation score at optimal lambda\n\n    Examples\n    --------\n    &gt;&gt;&gt; from psplines import PSpline\n    &gt;&gt;&gt; from psplines.optimize import cross_validation\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; x = np.linspace(0, 1, 50)\n    &gt;&gt;&gt; y = np.sin(2*np.pi*x) + 0.1*np.random.randn(50)\n    &gt;&gt;&gt; spline = PSpline(x, y, nseg=20)\n    &gt;&gt;&gt; optimal_lambda, score = cross_validation(spline)\n    &gt;&gt;&gt; spline.lambda_ = optimal_lambda\n    &gt;&gt;&gt; spline.fit()\n    \"\"\"\n</code></pre>"},{"location":"contributing/#comments","title":"Comments","text":"<p>Use clear, concise comments for complex algorithms:</p> <pre><code># Solve the penalized least squares system:\n# (B^T B + lambda * P^T P) alpha = B^T y\nA = self.basis_matrix.T @ self.basis_matrix + self.lambda_ * penalty_matrix\nb = self.basis_matrix.T @ self.y\nself.alpha = spsolve(A, b)\n</code></pre>"},{"location":"contributing/#testing","title":"Testing","text":""},{"location":"contributing/#test-structure","title":"Test Structure","text":"<p>Place tests in <code>tests/</code> directory with files matching <code>test_*.py</code>:</p> <pre><code>tests/\n\u251c\u2500\u2500 test_core.py          # Tests for core PSpline class\n\u251c\u2500\u2500 test_basis.py         # Tests for basis functions\n\u251c\u2500\u2500 test_penalty.py       # Tests for penalty matrices\n\u251c\u2500\u2500 test_optimize.py      # Tests for optimization functions\n\u2514\u2500\u2500 conftest.py           # Shared test fixtures\n</code></pre>"},{"location":"contributing/#writing-tests","title":"Writing Tests","text":"<p>Use pytest conventions:</p> <pre><code>import pytest\nimport numpy as np\nfrom psplines import PSpline\n\nclass TestPSpline:\n    \"\"\"Test the PSpline class.\"\"\"\n\n    def test_basic_fitting(self):\n        \"\"\"Test basic spline fitting functionality.\"\"\"\n        # Generate test data\n        x = np.linspace(0, 1, 20)\n        y = np.sin(2*np.pi*x) + 0.1*np.random.randn(20)\n\n        # Create and fit spline\n        spline = PSpline(x, y, nseg=10, lambda_=1.0)\n        spline.fit()\n\n        # Check basic properties\n        assert spline.alpha is not None\n        assert len(spline.alpha) == spline.nb\n        assert spline.ED &gt; 0\n        assert spline.sigma2 &gt; 0\n\n    def test_prediction(self):\n        \"\"\"Test prediction functionality.\"\"\"\n        x = np.linspace(0, 1, 20)\n        y = np.sin(2*np.pi*x)\n\n        spline = PSpline(x, y, nseg=10, lambda_=1.0)\n        spline.fit()\n\n        # Test prediction\n        x_new = np.linspace(0, 1, 10)\n        y_pred = spline.predict(x_new)\n\n        assert len(y_pred) == len(x_new)\n        assert np.all(np.isfinite(y_pred))\n\n    def test_input_validation(self):\n        \"\"\"Test input validation.\"\"\"\n        x = np.linspace(0, 1, 20)\n        y = np.sin(2*np.pi*x)\n\n        # Test invalid nseg\n        with pytest.raises(ValueError, match=\"nseg must be positive\"):\n            PSpline(x, y, nseg=0)\n\n        # Test mismatched array lengths\n        with pytest.raises(ValueError, match=\"x and y must have the same length\"):\n            PSpline(x, y[:-1])\n</code></pre>"},{"location":"contributing/#test-coverage","title":"Test Coverage","text":"<p>Aim for high test coverage:</p> <pre><code># Run tests with coverage\npytest tests/ --cov=psplines --cov-report=html\n\n# View coverage report\nopen htmlcov/index.html\n</code></pre>"},{"location":"contributing/#documentation_1","title":"Documentation","text":""},{"location":"contributing/#building-documentation","title":"Building Documentation","text":"<p>The documentation uses MkDocs with Material theme:</p> <pre><code># Install documentation dependencies\npip install mkdocs mkdocs-material mkdocstrings[python]\n\n# Serve locally\nmkdocs serve\n\n# Build static site\nmkdocs build\n</code></pre>"},{"location":"contributing/#adding-examples","title":"Adding Examples","text":"<p>When adding new examples:</p> <ol> <li>Create the example script in <code>examples/</code></li> <li>Add it to the Examples Gallery</li> <li>Include clear docstrings and comments</li> <li>Test the example thoroughly</li> </ol>"},{"location":"contributing/#updating-api-documentation","title":"Updating API Documentation","text":"<p>API documentation is auto-generated from docstrings. To update:</p> <ol> <li>Ensure your docstrings follow NumPy conventions</li> <li>The documentation will automatically include new public functions</li> <li>For new modules, add them to the appropriate <code>docs/api/*.md</code> file</li> </ol>"},{"location":"contributing/#release-process","title":"Release Process","text":""},{"location":"contributing/#version-management","title":"Version Management","text":"<p>We use semantic versioning (MAJOR.MINOR.PATCH):</p> <ul> <li>MAJOR: Breaking changes</li> <li>MINOR: New features (backward compatible)</li> <li>PATCH: Bug fixes (backward compatible)</li> </ul>"},{"location":"contributing/#pre-release-checklist","title":"Pre-release Checklist","text":"<p>Before creating a release:</p> <ol> <li>Update version in <code>src/psplines/__init__.py</code></li> <li>Update CHANGELOG.md with new features and fixes</li> <li>Run full test suite: <code>pytest tests/ -v</code></li> <li>Check code quality: <code>ruff check . &amp;&amp; mypy src/psplines</code></li> <li>Build documentation: <code>mkdocs build</code></li> <li>Test examples: Run all example scripts</li> <li>Update dependencies if needed</li> </ol>"},{"location":"contributing/#getting-help","title":"Getting Help","text":""},{"location":"contributing/#where-to-ask-questions","title":"Where to Ask Questions","text":"<ul> <li>General usage questions: GitHub Discussions</li> <li>Bug reports: GitHub Issues</li> <li>Feature requests: GitHub Issues</li> </ul>"},{"location":"contributing/#development-questions","title":"Development Questions","text":"<p>If you're working on a contribution and need help:</p> <ol> <li>Check existing issues and discussions</li> <li>Look at the codebase for similar implementations</li> <li>Create a draft pull request for early feedback</li> <li>Reach out via GitHub Issues with the <code>question</code> label</li> </ol>"},{"location":"contributing/#code-of-conduct","title":"Code of Conduct","text":"<p>Please be respectful and constructive in all interactions. We want to maintain a welcoming environment for contributors of all backgrounds and experience levels.</p>"},{"location":"contributing/#guidelines","title":"Guidelines","text":"<ul> <li>Be respectful of different opinions and approaches</li> <li>Be constructive in feedback and criticism</li> <li>Be patient with newcomers and questions</li> <li>Be collaborative in finding solutions</li> </ul>"},{"location":"contributing/#recognition","title":"Recognition","text":"<p>Contributors will be recognized in:</p> <ul> <li>README.md contributor list</li> <li>CHANGELOG.md for significant contributions</li> <li>GitHub contributor graphs</li> </ul> <p>Thank you for contributing to PSplines! Your efforts help make this a better tool for the scientific and data science communities.</p>"},{"location":"contributing/#quick-reference","title":"Quick Reference","text":""},{"location":"contributing/#common-commands","title":"Common Commands","text":"<pre><code># Development setup\ngit clone https://github.com/YOUR_USERNAME/psplines.git\ncd psplines\nuv sync --dev\n\n# Quality checks\nruff check . &amp;&amp; ruff format .\nmypy src/psplines\npytest tests/ -v --cov=psplines\n\n# Documentation\nmkdocs serve\n\n# Commit workflow\ngit checkout -b feature/my-feature\n# make changes\ngit add . &amp;&amp; git commit -m \"Clear commit message\"\ngit push origin feature/my-feature\n# create pull request\n</code></pre>"},{"location":"contributing/#file-structure","title":"File Structure","text":"<pre><code>psplines/\n\u251c\u2500\u2500 src/psplines/           # Source code\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 core.py            # Main PSpline class\n\u2502   \u251c\u2500\u2500 basis.py           # B-spline basis functions\n\u2502   \u251c\u2500\u2500 penalty.py         # Penalty matrices\n\u2502   \u251c\u2500\u2500 optimize.py        # Parameter optimization\n\u2502   \u2514\u2500\u2500 utils.py           # Utility functions\n\u251c\u2500\u2500 tests/                  # Test suite\n\u251c\u2500\u2500 docs/                   # Documentation source\n\u251c\u2500\u2500 examples/               # Example scripts\n\u251c\u2500\u2500 pyproject.toml          # Project configuration\n\u2514\u2500\u2500 README.md\n</code></pre>"},{"location":"api/basis/","title":"Basis Functions API","text":"<p>The basis module provides functions for constructing B-spline basis matrices and their derivatives.</p>"},{"location":"api/basis/#functions","title":"Functions","text":"<p>Compute B-spline regression basis on [xl, xr].</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>(array - like, shape(n))</code>)           \u2013            </li> <li> <code>xl</code>               (<code>floats</code>)           \u2013            <p>Domain endpoints (xl &lt; xr).</p> </li> <li> <code>xr</code>               (<code>floats</code>)           \u2013            <p>Domain endpoints (xl &lt; xr).</p> </li> <li> <code>nseg</code>               (<code>int</code>)           \u2013            <p>Number of equal-length segments.</p> </li> <li> <code>degree</code>               (<code>int</code>, default:                   <code>3</code> )           \u2013            <p>Degree of the spline.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>B</code> (              <code>(csr_matrix, shape(n, nseg + degree))</code> )          \u2013            <p>Sparse design matrix, each row has at most degree+1 nonzeros.</p> </li> <li> <code>knots</code> (              <code>ndarray</code> )          \u2013            <p>Full knot vector (length nseg + degree + 1 + degree).</p> </li> </ul> Source code in <code>src/psplines/basis.py</code> <pre><code>def b_spline_basis(\n    x: ArrayLike, xl: float, xr: float, nseg: int, degree: int = 3\n) -&gt; tuple[sp.csr_matrix, NDArray]:\n    \"\"\"\n    Compute B-spline regression basis on [xl, xr].\n\n    Parameters\n    ----------\n    x : array-like, shape (n,)\n    xl, xr : floats\n        Domain endpoints (xl &lt; xr).\n    nseg : int\n        Number of equal-length segments.\n    degree : int\n        Degree of the spline.\n\n    Returns\n    -------\n    B : csr_matrix, shape (n, nseg+degree)\n        Sparse design matrix, each row has at most degree+1 nonzeros.\n    knots : ndarray\n        Full knot vector (length nseg + degree + 1 + degree).\n    \"\"\"\n    x = np.asarray(x, float)\n    knots = _make_knots(xl, xr, nseg, degree)\n    # number of basis functions\n    nb = nseg + degree\n    # clip to domain\n    x_clipped = np.clip(x, xl, xr)\n    # build BSpline object: vectorized multi-coeff basis\n    # coefficients as identity to extract each basis\n    coeffs = np.eye(nb)\n    spline = BSpline(knots, coeffs, degree, extrapolate=False)\n    # evaluate basis at all x in one call\n    B_full = spline(x_clipped)\n    # convert to CSR for sparsity\n    B = sp.csr_matrix(B_full)\n    return B, knots\n</code></pre> <p>Compute k-th derivative of B-spline basis on [xl, xr].</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>array - like</code>)           \u2013            </li> <li> <code>xl</code>               (<code>floats</code>)           \u2013            </li> <li> <code>xr</code>               (<code>floats</code>)           \u2013            </li> <li> <code>nseg</code>               (<code>int</code>)           \u2013            </li> <li> <code>degree</code>               (<code>int</code>, default:                   <code>3</code> )           \u2013            </li> <li> <code>deriv_order</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>Order of derivative (0 returns same as b_spline_basis).</p> </li> <li> <code>knots</code>               (<code>ndarray or None</code>, default:                   <code>None</code> )           \u2013            <p>Precomputed knot vector.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>B_deriv</code> (              <code>(csr_matrix, shape(n, nseg + degree))</code> )          \u2013            <p>Sparse derivative basis.</p> </li> <li> <code>knots</code> (              <code>ndarray</code> )          \u2013            </li> </ul> Source code in <code>src/psplines/basis.py</code> <pre><code>def b_spline_derivative_basis(\n    x: ArrayLike,\n    xl: float,\n    xr: float,\n    nseg: int,\n    degree: int = 3,\n    deriv_order: int = 1,\n    knots: NDArray | None = None,\n) -&gt; tuple[sp.csr_matrix, NDArray]:\n    \"\"\"\n    Compute k-th derivative of B-spline basis on [xl, xr].\n\n    Parameters\n    ----------\n    x : array-like\n    xl, xr : floats\n    nseg : int\n    degree : int\n    deriv_order : int\n        Order of derivative (0 returns same as b_spline_basis).\n    knots : ndarray or None\n        Precomputed knot vector.\n\n    Returns\n    -------\n    B_deriv : csr_matrix, shape (n, nseg+degree)\n        Sparse derivative basis.\n    knots : ndarray\n    \"\"\"\n    if deriv_order &lt; 0:\n        raise ValueError(\"deriv_order must be non-negative\")\n    if knots is None:\n        knots = _make_knots(xl, xr, nseg, degree)\n    # nothing to do for zero-order\n    if deriv_order == 0:\n        return b_spline_basis(x, xl, xr, nseg, degree)\n    # if derivative exceeds degree, result is zero\n    if deriv_order &gt; degree:\n        n = np.atleast_1d(x).shape[0]\n        return sp.csr_matrix((n, nseg + degree)), knots\n\n    x = np.asarray(x, float)\n    x_clipped = np.clip(x, xl, xr)\n    nb = nseg + degree\n    # build multi-coefficient BSpline\n    coeffs = np.eye(nb)\n    spline = BSpline(knots, coeffs, degree, extrapolate=False)\n    # evaluate derivative in one call\n    B_deriv_full = spline(x_clipped, nu=deriv_order)\n    B_deriv = sp.csr_matrix(B_deriv_full)\n    return B_deriv, knots\n</code></pre>"},{"location":"api/basis/#usage-examples","title":"Usage Examples","text":""},{"location":"api/basis/#basic-b-spline-basis","title":"Basic B-spline Basis","text":"<pre><code>import numpy as np\nfrom psplines.basis import b_spline_basis\n\n# Create evaluation points\nx = np.linspace(0, 1, 50)\n\n# Generate B-spline basis matrix\nB, knots = b_spline_basis(x, xl=0, xr=1, nseg=10, degree=3)\n\nprint(f\"Basis shape: {B.shape}\")  # (50, 13) for degree=3, nseg=10\nprint(f\"Number of knots: {len(knots)}\")  # 17 knots total\n</code></pre>"},{"location":"api/basis/#derivative-basis","title":"Derivative Basis","text":"<pre><code>from psplines.basis import b_spline_derivative_basis\n\n# Generate first derivative basis\nB_deriv, knots = b_spline_derivative_basis(\n    x, xl=0, xr=1, nseg=10, degree=3, \n    derivative_order=1, knots=knots\n)\n\nprint(f\"Derivative basis shape: {B_deriv.shape}\")\n</code></pre>"},{"location":"api/basis/#mathematical-background","title":"Mathematical Background","text":""},{"location":"api/basis/#b-spline-basis-construction","title":"B-spline Basis Construction","text":"<p>B-splines of degree \\(d\\) are defined recursively:</p> \\[B_{i,0}(x) = \\begin{cases} 1 &amp; \\text{if } t_i \\leq x &lt; t_{i+1} \\\\ 0 &amp; \\text{otherwise} \\end{cases}\\] \\[B_{i,d}(x) = \\frac{x - t_i}{t_{i+d} - t_i} B_{i,d-1}(x) + \\frac{t_{i+d+1} - x}{t_{i+d+1} - t_{i+1}} B_{i+1,d-1}(x)\\] <p>where \\(t_i\\) are the knots.</p>"},{"location":"api/basis/#knot-vector-construction","title":"Knot Vector Construction","text":"<p>For \\(n\\) segments and degree \\(d\\):</p> <ul> <li>Interior knots: \\(n+1\\) equally spaced points</li> <li>Boundary knots: \\(d\\) repeated knots at each end</li> <li>Total knots: \\(n + 1 + 2d\\)</li> <li>Number of basis functions: \\(n + d\\)</li> </ul>"},{"location":"api/basis/#properties","title":"Properties","text":"<ul> <li>Local support: Each basis function is non-zero over at most \\(d+1\\) knot spans</li> <li>Partition of unity: \\(\\sum_i B_{i,d}(x) = 1\\) for all \\(x\\)</li> <li>Non-negativity: \\(B_{i,d}(x) \\geq 0\\) for all \\(i, x\\)</li> <li>Smoothness: \\(B_{i,d}(x)\\) is \\(C^{d-1}\\) continuous</li> </ul>"},{"location":"api/basis/#derivative-computation","title":"Derivative Computation","text":"<p>The \\(k\\)-th derivative of a B-spline is computed using the recursive formula:</p> \\[\\frac{d^k}{dx^k} B_{i,d}(x) = \\frac{d!}{(d-k)!} \\sum_{j=0}^k (-1)^{k-j} \\binom{k}{j} \\frac{B_{i+j,d-k}(x)}{(t_{i+d+1-j} - t_{i+j})^k}\\]"},{"location":"api/basis/#implementation-details","title":"Implementation Details","text":""},{"location":"api/basis/#sparse-matrix-format","title":"Sparse Matrix Format","text":"<ul> <li>All basis matrices are returned as <code>scipy.sparse.csr_matrix</code></li> <li>Efficient storage for large problems</li> <li>Fast matrix-vector operations</li> </ul>"},{"location":"api/basis/#numerical-considerations","title":"Numerical Considerations","text":"<ul> <li>Uses SciPy's <code>BSpline</code> class for robust evaluation</li> <li>Handles edge cases at boundaries</li> <li>Maintains numerical stability for high-degree splines</li> </ul>"},{"location":"api/basis/#performance-tips","title":"Performance Tips","text":"<ol> <li>Reuse knots: Pass the same knot vector to derivative functions</li> <li>Batch evaluation: Evaluate multiple points simultaneously</li> <li>Appropriate degree: Degree 3 (cubic) is usually sufficient</li> <li>Segment selection: More segments increase flexibility but computational cost</li> </ol>"},{"location":"api/core/","title":"Core API","text":"<p>The core module contains the main <code>PSpline</code> class that implements penalized B-spline smoothing.</p>"},{"location":"api/core/#pspline-class","title":"PSpline Class","text":"<p>Univariate penalised B-spline smoother.</p>"},{"location":"api/core/#psplines.core.PSpline.fit","title":"fit","text":"<pre><code>fit(\n    *, xl: float | None = None, xr: float | None = None\n) -&gt; PSpline\n</code></pre> <p>Fit the P-spline model.</p> <p>Parameters:</p> <ul> <li> <code>xl</code>               (<code>float</code>, default:                   <code>None</code> )           \u2013            <p>Left boundary of the domain. Defaults to min(x).</p> </li> <li> <code>xr</code>               (<code>float</code>, default:                   <code>None</code> )           \u2013            <p>Right boundary of the domain. Defaults to max(x).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>PSpline</code>           \u2013            <p>The fitted spline object.</p> </li> </ul> Source code in <code>src/psplines/core.py</code> <pre><code>def fit(self, *, xl: float | None = None, xr: float | None = None) -&gt; PSpline:\n    \"\"\"\n    Fit the P-spline model.\n\n    Parameters\n    ----------\n    xl : float, optional\n        Left boundary of the domain. Defaults to min(x).\n    xr : float, optional\n        Right boundary of the domain. Defaults to max(x).\n\n    Returns\n    -------\n    PSpline\n        The fitted spline object.\n    \"\"\"\n    # Validate and set domain\n    x_array = np.asarray(self.x)\n    x_min = float(np.min(x_array))\n    x_max = float(np.max(x_array))\n\n    if xl is not None:\n        if not np.isfinite(xl):\n            raise ValueError(\"xl must be finite\")\n        if xl &gt; x_min:\n            raise ValueError(f\"xl ({xl}) must be &lt;= min(x) ({x_min})\")\n\n    if xr is not None:\n        if not np.isfinite(xr):\n            raise ValueError(\"xr must be finite\")\n        if xr &lt; x_max:\n            raise ValueError(f\"xr ({xr}) must be &gt;= max(x) ({x_max})\")\n\n    self._xl = xl if xl is not None else x_min\n    self._xr = xr if xr is not None else x_max\n\n    if self._xl &gt;= self._xr:\n        raise ValueError(f\"xl ({self._xl}) must be &lt; xr ({self._xr})\")\n    # basis and penalty\n    self.B, self.knots = b_spline_basis(\n        self.x, self._xl, self._xr, self.nseg, self.degree\n    )\n    nb = self.B.shape[1]\n    D = difference_matrix(nb, self.penalty_order)\n    # sparse cross-products\n    self._BtB = (self.B.T @ self.B).tocsr()\n    self._DtD = (D.T @ D).tocsr()\n    self._Bty = self.B.T @ self.y\n    # constraints\n    self._build_constraints(nb)\n    # solve coefficients\n    P = self.lambda_ * self._DtD\n    self.coef = self._solve_coef(P)\n    # fitted values\n    self.fitted_values = self.B @ self.coef\n    # analytic uncertainty\n    self._update_uncertainty()\n    return self\n</code></pre>"},{"location":"api/core/#psplines.core.PSpline.predict","title":"predict","text":"<pre><code>predict(\n    x_new: ArrayLike,\n    *,\n    derivative_order: int | None = None,\n    return_se: bool = False,\n    se_method: str = \"analytic\",\n    B_boot: int = 5000,\n    seed: int | None = None,\n    n_jobs: int = 1,\n    hdi_prob: float = 0.95,\n) -&gt; NDArray | tuple[NDArray, ...]\n</code></pre> <p>Predict smooth (or derivative) with optional uncertainty.</p> <p>Parameters:</p> <ul> <li> <code>x_new</code>               (<code>array - like</code>)           \u2013            <p>Points at which to evaluate the spline.</p> </li> <li> <code>derivative_order</code>               (<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Order of derivative to compute. None for function values.</p> </li> <li> <code>return_se</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to return standard errors.</p> </li> <li> <code>se_method</code>               (<code>str</code>, default:                   <code>\"analytic\"</code> )           \u2013            <p>Method for computing uncertainty: - 'analytic': delta-method SE -&gt; (fhat, se) - 'bootstrap': parametric bootstrap SEs -&gt; (fhat, se) - 'bayes': posterior mean + HDI credible band -&gt; (mean, lower, upper)</p> </li> <li> <code>B_boot</code>               (<code>int</code>, default:                   <code>5000</code> )           \u2013            <p>Number of bootstrap replicates (for bootstrap method).</p> </li> <li> <code>seed</code>               (<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Random seed (for bootstrap method).</p> </li> <li> <code>n_jobs</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>Number of parallel jobs (for bootstrap method).</p> </li> <li> <code>hdi_prob</code>               (<code>float</code>, default:                   <code>0.95</code> )           \u2013            <p>Probability for HDI credible band (for Bayesian method).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>NDArray or tuple</code>           \u2013            <p>Predictions, optionally with uncertainty estimates.</p> </li> </ul> Source code in <code>src/psplines/core.py</code> <pre><code>def predict(\n    self,\n    x_new: ArrayLike,\n    *,\n    derivative_order: int | None = None,\n    return_se: bool = False,\n    se_method: str = \"analytic\",\n    B_boot: int = 5000,\n    seed: int | None = None,\n    n_jobs: int = 1,\n    hdi_prob: float = 0.95,\n) -&gt; NDArray | tuple[NDArray, ...]:\n    \"\"\"\n    Predict smooth (or derivative) with optional uncertainty.\n\n    Parameters\n    ----------\n    x_new : array-like\n        Points at which to evaluate the spline.\n    derivative_order : int, optional\n        Order of derivative to compute. None for function values.\n    return_se : bool, default False\n        Whether to return standard errors.\n    se_method : str, default \"analytic\"\n        Method for computing uncertainty:\n        - 'analytic': delta-method SE -&gt; (fhat, se)\n        - 'bootstrap': parametric bootstrap SEs -&gt; (fhat, se)\n        - 'bayes': posterior mean + HDI credible band -&gt; (mean, lower, upper)\n    B_boot : int, default 5000\n        Number of bootstrap replicates (for bootstrap method).\n    seed : int, optional\n        Random seed (for bootstrap method).\n    n_jobs : int, default 1\n        Number of parallel jobs (for bootstrap method).\n    hdi_prob : float, default 0.95\n        Probability for HDI credible band (for Bayesian method).\n\n    Returns\n    -------\n    NDArray or tuple\n        Predictions, optionally with uncertainty estimates.\n    \"\"\"\n    if self.coef is None:\n        raise RuntimeError(\"Model not fitted. Call fit() first.\")\n\n    # Validate input\n    try:\n        xq = _as1d(x_new)\n    except (ValueError, TypeError) as e:\n        raise ValueError(f\"Invalid x_new array: {e}\")\n\n    if xq.size == 0:\n        raise ValueError(\"x_new cannot be empty\")\n    if not np.all(np.isfinite(xq)):\n        raise ValueError(\"x_new contains non-finite values (NaN or inf)\")\n\n    # Validate parameters\n    if derivative_order is not None and derivative_order &lt;= 0:\n        raise ValueError(f\"derivative_order must be positive (got {derivative_order})\")\n    if se_method not in (\"analytic\", \"bootstrap\", \"bayes\"):\n        raise ValueError(f\"se_method must be 'analytic', 'bootstrap', or 'bayes' (got '{se_method}')\")\n    if B_boot &lt;= 0:\n        raise ValueError(f\"B_boot must be positive (got {B_boot})\")\n    if n_jobs &lt;= 0:\n        raise ValueError(f\"n_jobs must be positive (got {n_jobs})\")\n    if not (0 &lt; hdi_prob &lt; 1):\n        raise ValueError(f\"hdi_prob must be between 0 and 1 (got {hdi_prob})\")\n\n    # Bayesian credible band\n    if se_method == \"bayes\":\n        if self.trace is None:\n            raise RuntimeError(\"Call bayes_fit() first to sample the posterior.\")\n        if self._spline is None:\n            raise RuntimeError(\"No BSpline stored. Run bayes_fit() first.\")\n\n        # Evaluate stored BSpline (or its k-th derivative)\n        if derivative_order is None:\n            Bq = self._spline(xq)\n        else:\n            Bq = self._spline(xq, nu=derivative_order)\n        # to array for matmul\n        Bq = Bq.toarray() if sp.issparse(Bq) else np.asarray(Bq)\n\n        # posterior alpha draws: shape (n_samples, n_basis)\n        alpha_draws = (\n            self.trace.posterior[\"alpha\"].stack(sample=(\"chain\", \"draw\")).values\n        )\n\n        # If it came transposed (basis \u00d7 samples), swap axes\n        if (\n            alpha_draws.shape[0] == Bq.shape[1]\n            and alpha_draws.shape[1] != Bq.shape[1]\n        ):\n            alpha_draws = alpha_draws.T\n\n        # Check dimensions now match: (n_samples, nb)\n        S, p = alpha_draws.shape\n        if p != Bq.shape[1]:\n            raise ValueError(\n                f\"Mismatch: posterior alpha has length {p}, but basis has {Bq.shape[1]} cols\"\n            )\n\n        # draws of f^(k)(x): (n_samples, n_points)\n        deriv_draws = alpha_draws @ Bq.T\n\n        # summarize\n        mean = deriv_draws.mean(axis=0)\n        lower = np.percentile(deriv_draws, (1 - hdi_prob) / 2 * 100, axis=0)\n        upper = np.percentile(deriv_draws, (1 + hdi_prob) / 2 * 100, axis=0)\n        return mean, lower, upper\n\n    # Bootstrap SEs\n    if se_method == \"bootstrap\" and return_se:\n        return self._bootstrap_predict(xq, derivative_order, B_boot, seed, n_jobs)\n\n    # Analytic or plain prediction\n    if derivative_order is None:\n        assert self._xl is not None and self._xr is not None, \"Domain bounds not set. Call fit() first.\"\n        Bq, _ = b_spline_basis(xq, self._xl, self._xr, self.nseg, self.degree)\n    else:\n        if derivative_order &lt;= 0:\n            raise ValueError(\"derivative_order must be positive\")\n        assert self._xl is not None and self._xr is not None, \"Domain bounds not set. Call fit() first.\"\n        Bq, _ = b_spline_derivative_basis(\n            xq,\n            self._xl,\n            self._xr,\n            self.nseg,\n            self.degree,\n            derivative_order,\n            self.knots,\n        )\n\n    fhat = Bq @ self.coef\n\n    if not return_se:\n        return fhat\n\n    if se_method != \"analytic\":\n        raise ValueError(f\"Unknown se_method: {se_method}\")\n\n    if self.se_coef is None:\n        raise RuntimeError(\"Analytic SEs unavailable. Call fit() with uncertainty computation.\")\n\n    cov_diag = self.se_coef**2\n    B2 = Bq.multiply(Bq) if sp.issparse(Bq) else Bq**2\n    se = np.sqrt(B2 @ cov_diag)\n    return fhat, se\n</code></pre>"},{"location":"api/core/#psplines.core.PSpline.derivative","title":"derivative","text":"<pre><code>derivative(\n    x_new: ArrayLike,\n    *,\n    deriv_order: int = 1,\n    return_se: bool = False,\n    se_method: str = \"analytic\",\n    **kwargs: Any,\n) -&gt; NDArray | tuple[NDArray, ...]\n</code></pre> <p>Compute k-th derivative of the fitted spline.</p> <p>Parameters:</p> <ul> <li> <code>x_new</code>               (<code>array - like</code>)           \u2013            <p>Points at which to evaluate the derivative.</p> </li> <li> <code>deriv_order</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>Order of derivative to compute.</p> </li> <li> <code>return_se</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to return standard errors.</p> </li> <li> <code>se_method</code>               (<code>str</code>, default:                   <code>\"analytic\"</code> )           \u2013            <p>Method for computing standard errors (\"analytic\" or \"bootstrap\").</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional arguments passed to bootstrap method if applicable.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>NDArray or tuple</code>           \u2013            <p>Derivative values, optionally with standard errors.</p> </li> </ul> Source code in <code>src/psplines/core.py</code> <pre><code>def derivative(\n    self,\n    x_new: ArrayLike,\n    *,\n    deriv_order: int = 1,\n    return_se: bool = False,\n    se_method: str = \"analytic\",\n    **kwargs: Any\n) -&gt; NDArray | tuple[NDArray, ...]:\n    \"\"\"\n    Compute k-th derivative of the fitted spline.\n\n    Parameters\n    ----------\n    x_new : array-like\n        Points at which to evaluate the derivative.\n    deriv_order : int, default 1\n        Order of derivative to compute.\n    return_se : bool, default False\n        Whether to return standard errors.\n    se_method : str, default \"analytic\"\n        Method for computing standard errors (\"analytic\" or \"bootstrap\").\n    **kwargs\n        Additional arguments passed to bootstrap method if applicable.\n\n    Returns\n    -------\n    NDArray or tuple\n        Derivative values, optionally with standard errors.\n    \"\"\"\n    if self.coef is None:\n        raise RuntimeError(\"Model not fitted. Call fit() first.\")\n\n    # Validate input\n    try:\n        xq = _as1d(x_new)\n    except (ValueError, TypeError) as e:\n        raise ValueError(f\"Invalid x_new array: {e}\")\n\n    if xq.size == 0:\n        raise ValueError(\"x_new cannot be empty\")\n    if not np.all(np.isfinite(xq)):\n        raise ValueError(\"x_new contains non-finite values (NaN or inf)\")\n    if deriv_order &lt;= 0:\n        raise ValueError(f\"deriv_order must be positive (got {deriv_order})\")\n    if se_method not in (\"analytic\", \"bootstrap\", \"bayes\"):\n        raise ValueError(f\"se_method must be 'analytic', 'bootstrap', or 'bayes' (got '{se_method}')\")\n\n    # Use predict method for bootstrap and Bayesian methods\n    if se_method in (\"bootstrap\", \"bayes\"):\n        return self.predict(\n            x_new,\n            derivative_order=deriv_order,\n            return_se=return_se,\n            se_method=se_method,\n            **kwargs\n        )\n\n    # Direct computation for analytic method (more efficient)\n    assert self._xl is not None and self._xr is not None, \"Domain bounds not set. Call fit() first.\"\n    Bq, _ = b_spline_derivative_basis(\n        xq,\n        self._xl,\n        self._xr,\n        self.nseg,\n        self.degree,\n        deriv_order,\n        self.knots,\n    )\n    fhat = Bq @ self.coef\n\n    if not return_se:\n        return fhat\n\n    if self.se_coef is None:\n        raise RuntimeError(\"Analytic SEs unavailable. Call fit() with uncertainty computation.\")\n\n    cov_diag = self.se_coef**2\n    B2 = Bq.multiply(Bq) if sp.issparse(Bq) else Bq**2\n    se = np.sqrt(B2 @ cov_diag)\n    return fhat, se\n</code></pre>"},{"location":"api/core/#psplines.core.PSpline.bayes_fit","title":"bayes_fit","text":"<pre><code>bayes_fit(\n    a: float = 2.0,\n    b: float = 0.1,\n    c: float = 2.0,\n    d: float = 1.0,\n    draws: int = 2000,\n    tune: int = 2000,\n    chains: int = 4,\n    cores: int = 4,\n    target_accept: float = 0.9,\n    random_seed: int | None = None,\n) -&gt; Any\n</code></pre> Source code in <code>src/psplines/core.py</code> <pre><code>def bayes_fit(\n    self,\n    a: float = 2.0,\n    b: float = 0.1,\n    c: float = 2.0,\n    d: float = 1.0,\n    draws: int = 2000,\n    tune: int = 2000,\n    chains: int = 4,\n    cores: int = 4,\n    target_accept: float = 0.9,\n    random_seed: int | None = None,\n) -&gt; Any:\n    # Prepare basis and penalty\n    x_array = np.asarray(self.x)\n    self._xl = float(np.min(x_array)) if self._xl is None else self._xl\n    self._xr = float(np.max(x_array)) if self._xr is None else self._xr\n    B_sp, self.knots = b_spline_basis(\n        self.x, self._xl, self._xr, self.nseg, self.degree\n    )\n    B = B_sp.toarray() if sp.issparse(B_sp) else B_sp\n    nb = B.shape[1]\n    D = difference_matrix(nb, self.penalty_order).toarray()\n    y = self.y\n    I_nb = np.eye(nb)\n\n    # store BSpline object for predict\n    coeffs = np.eye(nb)\n    self._spline = BSpline(self.knots, coeffs, self.degree, extrapolate=False)\n\n    with pm.Model():\n        lam = pm.Gamma(\"lam\", alpha=a, beta=b, shape=D.shape[0])\n        Q = pm.math.dot(D.T * lam, D)\n        Q_j = Q + I_nb * 1e-6\n        alpha = pm.MvNormal(\n            \"alpha\",\n            mu=pm.math.zeros(Q_j.shape[0]),\n            tau=Q_j,\n            shape=Q_j.shape[0],\n        )\n        sigma = pm.InverseGamma(\"sigma\", alpha=c, beta=d)\n        mu = pm.Deterministic(\"mu\", pm.math.dot(B, alpha))\n        pm.Normal(\"y_obs\", mu=mu, sigma=sigma, observed=y)\n        trace = pm.sample(\n            draws=draws,\n            tune=tune,\n            chains=chains,\n            cores=cores,\n            target_accept=target_accept,\n            random_seed=random_seed,\n        )\n\n    # Store results\n    self.trace = trace\n    self.coef = (\n        trace.posterior[\"alpha\"]\n        .stack(sample=(\"chain\", \"draw\"))\n        .mean(dim=\"sample\")\n        .values\n    )\n    self.fitted_values = (\n        trace.posterior[\"mu\"]\n        .stack(sample=(\"chain\", \"draw\"))\n        .mean(dim=\"sample\")\n        .values\n    )\n    self.lambda_post = (\n        trace.posterior[\"lam\"]\n        .stack(sample=(\"chain\", \"draw\"))\n        .mean(dim=\"sample\")\n        .values\n    )\n    return trace\n</code></pre>"},{"location":"api/core/#psplines.core.PSpline.plot_lam_trace","title":"plot_lam_trace","text":"<pre><code>plot_lam_trace(figsize: tuple[int, int] = (8, 6)) -&gt; None\n</code></pre> <p>Plot trace and marginal posterior for each lambda_j.</p> Source code in <code>src/psplines/core.py</code> <pre><code>def plot_lam_trace(self, figsize: tuple[int, int] = (8, 6)) -&gt; None:\n    \"\"\"\n    Plot trace and marginal posterior for each lambda_j.\n    \"\"\"\n    az.plot_trace(self.trace, var_names=[\"lam\"], figsize=figsize)\n</code></pre>"},{"location":"api/core/#psplines.core.PSpline.plot_alpha_trace","title":"plot_alpha_trace","text":"<pre><code>plot_alpha_trace(figsize: tuple[int, int] = (8, 6)) -&gt; None\n</code></pre> <p>Plot trace and marginal posterior for alpha coefficients.</p> Source code in <code>src/psplines/core.py</code> <pre><code>def plot_alpha_trace(self, figsize: tuple[int, int] = (8, 6)) -&gt; None:\n    \"\"\"\n    Plot trace and marginal posterior for alpha coefficients.\n    \"\"\"\n    az.plot_trace(self.trace, var_names=[\"alpha\"], figsize=figsize)\n</code></pre>"},{"location":"api/core/#psplines.core.PSpline.plot_posterior","title":"plot_posterior","text":"<pre><code>plot_posterior(figsize: tuple[int, int] = (8, 6)) -&gt; None\n</code></pre> <p>Plot posterior</p> Source code in <code>src/psplines/core.py</code> <pre><code>def plot_posterior(self, figsize: tuple[int, int] = (8, 6)) -&gt; None:\n    \"\"\"\n    Plot posterior\n    \"\"\"\n    az.plot_posterior(\n        self.trace,\n        var_names=[\"lam\", \"sigma\"],\n        figsize=figsize,\n        hdi_prob=0.95,\n        point_estimate=\"mean\",\n    )\n</code></pre>"},{"location":"api/core/#usage-examples","title":"Usage Examples","text":""},{"location":"api/core/#basic-usage","title":"Basic Usage","text":"<pre><code>import numpy as np\nfrom psplines import PSpline\n\n# Generate data\nx = np.linspace(0, 1, 50)\ny = np.sin(2 * np.pi * x) + 0.1 * np.random.randn(50)\n\n# Create and fit spline\nspline = PSpline(x, y, nseg=20, lambda_=1.0)\nspline.fit()\n\n# Make predictions\nx_new = np.linspace(0, 1, 100)\ny_pred = spline.predict(x_new)\n</code></pre>"},{"location":"api/core/#with-uncertainty-quantification","title":"With Uncertainty Quantification","text":"<pre><code># Analytical standard errors\ny_pred, se = spline.predict(x_new, return_se=True)\n\n# Bootstrap confidence intervals\ny_pred, se_boot = spline.predict(x_new, return_se=True, \n                                se_method=\"bootstrap\", B_boot=1000)\n</code></pre>"},{"location":"api/core/#derivative-computation","title":"Derivative Computation","text":"<pre><code># First derivative\ndy_dx = spline.derivative(x_new, deriv_order=1)\n\n# Second derivative with uncertainty\nd2y_dx2, se = spline.derivative(x_new, deriv_order=2, return_se=True)\n</code></pre>"},{"location":"api/core/#bayesian-inference","title":"Bayesian Inference","text":"<pre><code># Fit Bayesian model\ntrace = spline.bayes_fit(draws=2000, tune=1000)\n\n# Get posterior credible intervals\nmean, lower, upper = spline.predict(x_new, se_method=\"bayes\")\n</code></pre>"},{"location":"api/core/#model-parameters","title":"Model Parameters","text":""},{"location":"api/core/#basis-function-parameters","title":"Basis Function Parameters","text":"<ul> <li> <p><code>nseg</code>: Number of B-spline segments</p> <ul> <li>Controls the flexibility of the basis</li> <li>More segments = more flexible fit</li> <li>Typical values: 10-50</li> </ul> </li> <li> <p><code>degree</code>: Degree of B-spline basis functions</p> <ul> <li>Controls local smoothness</li> <li>Higher degree = smoother derivatives</li> <li>Typical values: 1-4 (3 is most common)</li> </ul> </li> </ul>"},{"location":"api/core/#penalty-parameters","title":"Penalty Parameters","text":"<ul> <li> <p><code>lambda_</code>: Smoothing parameter</p> <ul> <li>Controls the trade-off between fit and smoothness  </li> <li>Higher values = smoother fits</li> <li>Can be selected automatically</li> </ul> </li> <li> <p><code>penalty_order</code>: Order of the difference penalty</p> <ul> <li>1: Penalizes first differences (rough penalty on slopes)</li> <li>2: Penalizes second differences (rough penalty on curvature)</li> <li>3: Penalizes third differences (rough penalty on jerk)</li> </ul> </li> </ul>"},{"location":"api/core/#constraint-parameters","title":"Constraint Parameters","text":"<ul> <li><code>constraints</code>: Dictionary specifying boundary constraints<ul> <li><code>\"deriv\"</code>: Derivative constraints at boundaries</li> <li>Example: <code>{\"deriv\": {\"order\": 1, \"initial\": 0, \"final\": 0}}</code></li> </ul> </li> </ul>"},{"location":"api/core/#model-attributes","title":"Model Attributes","text":"<p>After fitting, the following attributes are available:</p> <ul> <li><code>coef</code>: B-spline coefficients</li> <li><code>fitted_values</code>: Fitted values at input points</li> <li><code>knots</code>: Knot vector used for B-splines</li> <li><code>ED</code>: Effective degrees of freedom</li> <li><code>sigma2</code>: Residual variance estimate</li> <li><code>se_coef</code>: Standard errors of coefficients</li> <li><code>se_fitted</code>: Standard errors of fitted values</li> </ul>"},{"location":"api/core/#error-handling","title":"Error Handling","text":"<p>The <code>PSpline</code> class includes comprehensive input validation:</p> <ul> <li>Array validation: Checks for proper dimensions, finite values</li> <li>Parameter validation: Ensures valid ranges and types</li> <li>State validation: Verifies model is fitted before prediction</li> <li>Constraint validation: Validates constraint specifications</li> </ul> <p>All errors provide descriptive messages with suggested solutions.</p>"},{"location":"api/optimize/","title":"Optimization API","text":"<p>The optimize module provides functions for automatic selection of the smoothing parameter \u03bb using various criteria.</p>"},{"location":"api/optimize/#functions","title":"Functions","text":"<p>Find Lambda that minimizes GCV = (rss/n) / (1 - edf/n)^2.</p> Source code in <code>src/psplines/optimize.py</code> <pre><code>def cross_validation(\n    pspline: PSpline,\n    lambda_bounds: tuple[float, float] = (1e-6, 1e6),\n) -&gt; tuple[float, float]:\n    \"\"\"\n    Find Lambda that minimizes GCV = (rss/n) / (1 - edf/n)^2.\n    \"\"\"\n\n    def gcv(lam, coef, rss, Dcoef, C):\n        n = pspline.y.size\n        edf = effective_df(\n            pspline.B, difference_matrix(pspline.B.shape[1], pspline.penalty_order), lam\n        )\n        return (rss / n) / (1 - edf / n) ** 2\n\n    return _optimise_lambda(pspline, gcv, lambda_bounds)\n</code></pre> <p>Find Lambda that minimizes AIC = nlog(rss/n) + 2edf.</p> Source code in <code>src/psplines/optimize.py</code> <pre><code>def aic(\n    pspline: PSpline,\n    lambda_bounds: tuple[float, float] = (1e-6, 1e6),\n) -&gt; tuple[float, float]:\n    \"\"\"\n    Find Lambda that minimizes AIC = n*log(rss/n) + 2*edf.\n    \"\"\"\n\n    def crit(lam, coef, rss, Dcoef, C):\n        n = pspline.y.size\n        edf = effective_df(\n            pspline.B, difference_matrix(pspline.B.shape[1], pspline.penalty_order), lam\n        )\n        return n * np.log(rss / n) + 2 * edf\n\n    return _optimise_lambda(pspline, crit, lambda_bounds)\n</code></pre> <p>Pick lambda via maximum curvature of the L-curve. Implements two-stage search (coarse + refine), vectorized curvature, optional smoothing, and edge-case warnings.</p> <p>Parameters:</p> <ul> <li> <code>pspline</code>               (<code>PSpline</code>)           \u2013            <p>Fitted PSpline instance (coefficient solver ready).</p> </li> <li> <code>lambda_bounds</code>               (<code>tuple</code>, default:                   <code>(1e-06, 1000000.0)</code> )           \u2013            <p>(min, max) bounds for initial lambda grid (log-uniform).</p> </li> <li> <code>num_lambda</code>               (<code>int</code>, default:                   <code>81</code> )           \u2013            <p>Number of points in the initial lambda grid.</p> </li> <li> <code>refine</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to perform a second, finer search around the coarse optimum.</p> </li> <li> <code>refine_factor</code>               (<code>float</code>, default:                   <code>10</code> )           \u2013            <p>Factor to widen/narrow bounds for refinement around coarse lambda.</p> </li> <li> <code>refine_points</code>               (<code>int</code>, default:                   <code>81</code> )           \u2013            <p>Number of points in the refined grid.</p> </li> <li> <code>smooth_kappa</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to apply a 3-point moving average to curvature values.</p> </li> </ul> Source code in <code>src/psplines/optimize.py</code> <pre><code>def l_curve(\n    pspline,\n    lambda_bounds=(1e-6, 1e6),\n    num_lambda=81,\n    refine=True,\n    refine_factor=10,\n    refine_points=81,\n    smooth_kappa=True,\n):\n    \"\"\"\n    Pick lambda via maximum curvature of the L-curve.\n    Implements two-stage search (coarse + refine), vectorized curvature,\n    optional smoothing, and edge-case warnings.\n\n    Parameters\n    ----------\n    pspline : PSpline\n        Fitted PSpline instance (coefficient solver ready).\n    lambda_bounds : tuple\n        (min, max) bounds for initial lambda grid (log-uniform).\n    num_lambda : int\n        Number of points in the initial lambda grid.\n    refine : bool\n        Whether to perform a second, finer search around the coarse optimum.\n    refine_factor : float\n        Factor to widen/narrow bounds for refinement around coarse lambda.\n    refine_points : int\n        Number of points in the refined grid.\n    smooth_kappa : bool\n        Whether to apply a 3-point moving average to curvature values.\n    \"\"\"\n    # Coarse grid search\n    log_min, log_max = np.log10(lambda_bounds[0]), np.log10(lambda_bounds[1])\n    grid = np.logspace(log_min, log_max, num_lambda)\n    lr, lp = _sweep_lambda(pspline, grid)\n    valid = np.isfinite(lr) &amp; np.isfinite(lp)\n    x, y, lamv = lp[valid], lr[valid], grid[valid]\n\n    # Vectorized curvature calculation\n    # central differences for dx, dy, ddx, ddy\n    dx = (x[2:] - x[:-2]) * 0.5\n    dy = (y[2:] - y[:-2]) * 0.5\n    ddx = x[2:] - 2 * x[1:-1] + x[:-2]\n    ddy = y[2:] - 2 * y[1:-1] + y[:-2]\n    kappa = np.full_like(x, np.nan)\n    denom = (dx * dx + dy * dy) ** 1.5\n    kappa[1:-1] = np.abs(dx * ddy - dy * ddx) / denom\n\n    # Optional smoothing of curvature\n    if smooth_kappa:\n        kernel = np.ones(3) / 3\n        kappa = np.convolve(kappa, kernel, mode=\"same\")\n\n    # Identify coarse optimum\n    idx = int(np.nanargmax(kappa))\n    # Edge-case warning if optimum near boundary\n    if idx &lt; 2 or idx &gt; len(x) - 3:\n        warnings.warn(\n            \"L-curve optimum at boundary of grid; consider expanding lambda_bounds\",\n            UserWarning,\n        )\n    lam_corner = lamv[idx]\n    kappa_corner = kappa[idx]\n\n    # Optional refinement around coarse optimum\n    if refine:\n        lower = lam_corner / refine_factor\n        upper = lam_corner * refine_factor\n        log_l, log_u = np.log10(lower), np.log10(upper)\n        grid2 = np.logspace(log_l, log_u, refine_points)\n        lr2, lp2 = _sweep_lambda(pspline, grid2)\n        valid2 = np.isfinite(lr2) &amp; np.isfinite(lp2)\n        x2, y2, lamv2 = lp2[valid2], lr2[valid2], grid2[valid2]\n\n        dx2 = (x2[2:] - x2[:-2]) * 0.5\n        dy2 = (y2[2:] - y2[:-2]) * 0.5\n        ddx2 = x2[2:] - 2 * x2[1:-1] + x2[:-2]\n        ddy2 = y2[2:] - 2 * y2[1:-1] + y2[:-2]\n        kappa2 = np.full_like(x2, np.nan)\n        denom2 = (dx2 * dx2 + dy2 * dy2) ** 1.5\n        kappa2[1:-1] = np.abs(dx2 * ddy2 - dy2 * ddx2) / denom2\n        if smooth_kappa:\n            kappa2 = np.convolve(kappa2, kernel, mode=\"same\")\n\n        idx2 = int(np.nanargmax(kappa2))\n        if idx2 &lt; 2 or idx2 &gt; len(x2) - 3:\n            warnings.warn(\n                \"Refined L-curve optimum at boundary; expand refine_factor or refine_points\",\n                UserWarning,\n            )\n        lam_corner = lamv2[idx2]\n        kappa_corner = kappa2[idx2]\n\n    return _finish(pspline, lam_corner, kappa_corner)\n</code></pre> <p>Pick Lambda via minimum distance on V\u2011curve.</p> Source code in <code>src/psplines/optimize.py</code> <pre><code>def v_curve(\n    pspline: PSpline,\n    lambda_bounds: tuple[float, float] = (1e-6, 1e6),\n    num_lambda: int = 81,\n) -&gt; tuple[float, float]:\n    \"\"\"\n    Pick Lambda via minimum distance on V\u2011curve.\n    \"\"\"\n    grid = np.logspace(\n        np.log10(lambda_bounds[0]), np.log10(lambda_bounds[1]), num_lambda\n    )\n    lr, lp = _sweep_lambda(pspline, grid)\n    valid = np.isfinite(lr) &amp; np.isfinite(lp)\n    if valid.sum() &lt; 2:\n        raise RuntimeError(\"Not enough V\u2011curve points\")\n    dr = np.diff(lr[valid])\n    dp = np.diff(lp[valid])\n    dist = np.hypot(dr, dp)\n    mid = np.sqrt(grid[valid][:-1] * grid[valid][1:])\n    idx = int(np.argmin(dist))\n    return _finish(pspline, mid[idx], dist[idx])\n</code></pre>"},{"location":"api/optimize/#usage-examples","title":"Usage Examples","text":""},{"location":"api/optimize/#cross-validation-recommended","title":"Cross-Validation (Recommended)","text":"<pre><code>import numpy as np\nfrom psplines import PSpline\nfrom psplines.optimize import cross_validation\n\n# Create spline\nx = np.linspace(0, 1, 100)\ny = np.sin(2 * np.pi * x) + 0.1 * np.random.randn(100)\nspline = PSpline(x, y, nseg=20)\n\n# Find optimal lambda using cross-validation\nbest_lambda, cv_score = cross_validation(spline)\nprint(f\"Optimal \u03bb: {best_lambda:.6f}, CV score: {cv_score:.6f}\")\n\n# Use optimal lambda\nspline.lambda_ = best_lambda\nspline.fit()\n</code></pre>"},{"location":"api/optimize/#aic-based-selection","title":"AIC-Based Selection","text":"<pre><code>from psplines.optimize import aic\n\n# Find optimal lambda using AIC\nbest_lambda, aic_score = aic(spline)\nprint(f\"Optimal \u03bb: {best_lambda:.6f}, AIC: {aic_score:.6f}\")\n</code></pre>"},{"location":"api/optimize/#l-curve-method","title":"L-Curve Method","text":"<pre><code>from psplines.optimize import l_curve\n\n# Find optimal lambda using L-curve\nbest_lambda, curvature = l_curve(spline)\nprint(f\"Optimal \u03bb: {best_lambda:.6f}, Curvature: {curvature:.6f}\")\n</code></pre>"},{"location":"api/optimize/#comparing-methods","title":"Comparing Methods","text":"<pre><code>import matplotlib.pyplot as plt\n\n# Compare different methods\nmethods = {\n    'CV': cross_validation,\n    'AIC': aic,\n    'L-curve': l_curve,\n    'V-curve': v_curve\n}\n\nresults = {}\nfor name, method in methods.items():\n    try:\n        lambda_opt, score = method(spline)\n        results[name] = lambda_opt\n        print(f\"{name}: \u03bb = {lambda_opt:.6f}\")\n    except Exception as e:\n        print(f\"{name} failed: {e}\")\n\n# Plot comparison\nif results:\n    methods_list = list(results.keys())\n    lambdas = list(results.values())\n\n    plt.figure(figsize=(8, 5))\n    plt.bar(methods_list, lambdas)\n    plt.yscale('log')\n    plt.ylabel('Optimal \u03bb')\n    plt.title('Comparison of Parameter Selection Methods')\n    plt.show()\n</code></pre>"},{"location":"api/optimize/#mathematical-background","title":"Mathematical Background","text":""},{"location":"api/optimize/#cross-validation","title":"Cross-Validation","text":"<p>Generalized Cross-Validation (GCV) minimizes: \\(\\(\\text{GCV}(\\lambda) = \\frac{n \\|y - S_\\lambda y\\|^2}{[n - \\text{tr}(S_\\lambda)]^2}\\)\\)</p> <p>where \\(S_\\lambda = B(B^TB + \\lambda D^TD)^{-1}B^T\\) is the smoothing matrix.</p> <p>Advantages: - Well-established statistical foundation - Good performance across various problems - Automatic selection without user input</p> <p>Disadvantages: - Can be computationally expensive - May oversmooth in some cases</p>"},{"location":"api/optimize/#akaike-information-criterion-aic","title":"Akaike Information Criterion (AIC)","text":"<p>AIC balances fit quality and model complexity: \\(\\(\\text{AIC}(\\lambda) = n \\log(\\hat{\\sigma}^2) + 2 \\cdot \\text{ED}(\\lambda)\\)\\)</p> <p>where: - \\(\\hat{\\sigma}^2 = \\|y - S_\\lambda y\\|^2 / n\\) is the residual variance - \\(\\text{ED}(\\lambda) = \\text{tr}(S_\\lambda)\\) is the effective degrees of freedom</p> <p>Advantages: - Information-theoretic foundation - Fast computation - Good for model comparison</p> <p>Disadvantages: - May not work well for all noise levels - Less robust than cross-validation</p>"},{"location":"api/optimize/#l-curve-method_1","title":"L-Curve Method","text":"<p>The L-curve plots \\(\\log(\\|D\\alpha_\\lambda\\|^2)\\) vs \\(\\log(\\|y - B\\alpha_\\lambda\\|^2)\\) and finds the point of maximum curvature.</p> <p>Curvature is computed as: \\(\\(\\kappa(\\lambda) = \\frac{2(\\rho' \\eta'' - \\rho'' \\eta')}{(\\rho'^2 + \\eta'^2)^{3/2}}\\)\\)</p> <p>where \\(\\rho(\\lambda) = \\log(\\|y - B\\alpha_\\lambda\\|^2)\\) and \\(\\eta(\\lambda) = \\log(\\|D\\alpha_\\lambda\\|^2)\\).</p> <p>Advantages: - Intuitive geometric interpretation - No statistical assumptions - Good for ill-posed problems</p> <p>Disadvantages: - Can be sensitive to noise - May not have clear corner</p>"},{"location":"api/optimize/#v-curve-method","title":"V-Curve Method","text":"<p>Similar to L-curve but uses different scaling and looks for valley shape.</p>"},{"location":"api/optimize/#implementation-details","title":"Implementation Details","text":""},{"location":"api/optimize/#optimization-strategy","title":"Optimization Strategy","text":"<p>All methods use: 1. Logarithmic search: Test \u03bb values on logarithmic grid 2. Golden section search: Refine around the optimal region 3. Sparse linear algebra: Efficient computation of smoothing matrices</p>"},{"location":"api/optimize/#default-parameter-ranges","title":"Default Parameter Ranges","text":"<ul> <li>\u03bb range: \\(10^{-6}\\) to \\(10^6\\) (12 orders of magnitude)</li> <li>Grid points: 50 logarithmically spaced values</li> <li>Refinement: Golden section search with tolerance \\(10^{-6}\\)</li> </ul>"},{"location":"api/optimize/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Cross-validation: \\(O(n^3)\\) for dense problems, \\(O(n^2)\\) for sparse</li> <li>AIC: \\(O(n^2)\\) computation per \u03bb value</li> <li>L-curve: \\(O(n^2)\\) plus curvature computation</li> <li>V-curve: Similar to L-curve</li> </ul>"},{"location":"api/optimize/#numerical-stability","title":"Numerical Stability","text":"<ul> <li>Uses sparse Cholesky decomposition when possible</li> <li>Handles ill-conditioned matrices gracefully</li> <li>Monitors condition numbers and issues warnings</li> </ul>"},{"location":"api/optimize/#method-selection-guidelines","title":"Method Selection Guidelines","text":""},{"location":"api/optimize/#recommended-approach","title":"Recommended Approach","text":"<ol> <li>Start with cross-validation: Most robust for general use</li> <li>Try AIC: If CV is too slow or gives unreasonable results</li> <li>Use L-curve: For regularization-heavy applications</li> <li>Compare methods: Check consistency across approaches</li> </ol>"},{"location":"api/optimize/#problem-specific-recommendations","title":"Problem-Specific Recommendations","text":"<ul> <li>Small datasets (n &lt; 100): Cross-validation or AIC</li> <li>Large datasets (n &gt; 1000): AIC or L-curve for speed</li> <li>High noise: Cross-validation (more robust)</li> <li>Low noise: Any method should work well</li> <li>Sparse data: L-curve or V-curve</li> <li>Time series: Cross-validation with temporal structure</li> </ul>"},{"location":"api/optimize/#troubleshooting","title":"Troubleshooting","text":"<p>If optimization fails: 1. Check input data for issues (NaN, infinite values) 2. Try different nseg values 3. Reduce the \u03bb search range 4. Use a different optimization method 5. Manually specify \u03bb based on domain knowledge</p>"},{"location":"api/penalty/","title":"Penalty Matrices API","text":"<p>The penalty module provides functions for constructing difference penalty matrices used in P-spline smoothing.</p>"},{"location":"api/penalty/#functions","title":"Functions","text":"<p>Create a sparse difference matrix of shape (n-order)\u00d7n.</p> <p>Parameters:</p> <ul> <li> <code>n</code>               (<code>int</code>)           \u2013            <p>Length of the coefficient vector \u03b1.</p> </li> <li> <code>order</code>               (<code>int</code>, default:                   <code>2</code> )           \u2013            <p>Order d of the finite difference (must be &gt;= 0).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>D</code> (              <code>csr_matrix</code> )          \u2013            <p>Sparse (n-order)\u00d7n matrix implementing d-th order differences. If order=0, returns identity; if order&gt;=n, returns an empty matrix.</p> </li> </ul> Source code in <code>src/psplines/penalty.py</code> <pre><code>def difference_matrix(n: int, order: int = 2) -&gt; csr_matrix:\n    \"\"\"\n    Create a sparse difference matrix of shape (n-order)\u00d7n.\n\n    Parameters\n    ----------\n    n : int\n        Length of the coefficient vector \u03b1.\n    order : int\n        Order d of the finite difference (must be &gt;= 0).\n\n    Returns\n    -------\n    D : csr_matrix\n        Sparse (n-order)\u00d7n matrix implementing d-th order differences.\n        If order=0, returns identity; if order&gt;=n, returns an empty matrix.\n    \"\"\"\n    if order &lt; 0:\n        raise ValueError(\"order must be non-negative\")\n    if order == 0:\n        # zero-order = identity operator\n        return diags([np.ones(n)], [0], shape=(n, n), format=\"csr\")\n    if order &gt;= n:\n        # no valid differences\n        return csr_matrix((0, n))\n\n    # number of rows\n    m = n - order\n    # offsets for diagonals: 0,1,...,order\n    offsets = np.arange(order + 1)\n    # build each diagonal of length m\n    data = [((-1) ** (order - k)) * comb(order, k) * np.ones(m) for k in offsets]\n    D = diags(data, offsets, shape=(m, n), format=\"csr\")\n    return D\n</code></pre>"},{"location":"api/penalty/#usage-examples","title":"Usage Examples","text":""},{"location":"api/penalty/#first-order-differences","title":"First-Order Differences","text":"<pre><code>import numpy as np\nfrom psplines.penalty import difference_matrix\n\n# First-order difference matrix\nD1 = difference_matrix(n=5, order=1)\nprint(D1.toarray())\n# [[-1  1  0  0  0]\n#  [ 0 -1  1  0  0]\n#  [ 0  0 -1  1  0]\n#  [ 0  0  0 -1  1]]\n</code></pre>"},{"location":"api/penalty/#second-order-differences","title":"Second-Order Differences","text":"<pre><code># Second-order difference matrix  \nD2 = difference_matrix(n=5, order=2)\nprint(D2.toarray())\n# [[ 1 -2  1  0  0]\n#  [ 0  1 -2  1  0]\n#  [ 0  0  1 -2  1]]\n</code></pre>"},{"location":"api/penalty/#higher-order-differences","title":"Higher-Order Differences","text":"<pre><code># Third-order difference matrix\nD3 = difference_matrix(n=6, order=3)\nprint(D3.toarray())\n# [[-1  3 -3  1  0  0]\n#  [ 0 -1  3 -3  1  0]\n#  [ 0  0 -1  3 -3  1]]\n</code></pre>"},{"location":"api/penalty/#mathematical-background","title":"Mathematical Background","text":""},{"location":"api/penalty/#difference-operators","title":"Difference Operators","text":"<p>The \\(p\\)-th order difference operator \\(\\Delta^p\\) is defined recursively:</p> <ul> <li>\\(\\Delta^0 \\alpha_i = \\alpha_i\\) (identity)</li> <li>\\(\\Delta^1 \\alpha_i = \\alpha_{i+1} - \\alpha_i\\) (first difference)</li> <li>\\(\\Delta^p \\alpha_i = \\Delta^{p-1} \\alpha_{i+1} - \\Delta^{p-1} \\alpha_i\\)</li> </ul>"},{"location":"api/penalty/#matrix-representation","title":"Matrix Representation","text":"<p>The difference matrix \\(D_p\\) has entries such that \\((D_p \\alpha)_i = \\Delta^p \\alpha_i\\).</p> <p>For first-order differences (\\(p=1\\)): \\(\\(D_1 = \\begin{pmatrix} -1 &amp; 1 &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; -1 &amp; 1 &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; 0 &amp; \\cdots &amp; -1 &amp; 1 \\end{pmatrix}\\)\\)</p> <p>For second-order differences (\\(p=2\\)): \\(\\(D_2 = \\begin{pmatrix} 1 &amp; -2 &amp; 1 &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; 1 &amp; -2 &amp; 1 &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; -2 &amp; 1 \\end{pmatrix}\\)\\)</p>"},{"location":"api/penalty/#properties","title":"Properties","text":"<ul> <li>Dimension: For \\(n\\) coefficients, \\(D_p\\) has size \\((n-p) \\times n\\)</li> <li>Rank: The rank of \\(D_p\\) is \\(n-p\\) (for typical cases)</li> <li>Sparsity: Each row has at most \\(p+1\\) non-zero entries</li> <li>Banded structure: All non-zeros lie within \\(p+1\\) diagonals</li> </ul>"},{"location":"api/penalty/#penalty-interpretation","title":"Penalty Interpretation","text":"<p>The penalty term \\(\\lambda \\|D_p \\alpha\\|^2\\) penalizes:</p> <ul> <li>\\(p=1\\): Large first differences (rough slopes)</li> <li>\\(p=2\\): Large second differences (rough curvature)  </li> <li>\\(p=3\\): Large third differences (rough rate of curvature change)</li> </ul>"},{"location":"api/penalty/#implementation-details","title":"Implementation Details","text":""},{"location":"api/penalty/#sparse-storage","title":"Sparse Storage","text":"<ul> <li>Returns <code>scipy.sparse.csr_matrix</code> for efficient storage</li> <li>Memory usage: \\(O((n-p) \\times (p+1))\\) instead of \\(O((n-p) \\times n)\\)</li> <li>Fast matrix-vector multiplication</li> </ul>"},{"location":"api/penalty/#numerical-properties","title":"Numerical Properties","text":"<ul> <li>Condition number: Increases with penalty order</li> <li>Null space: \\(D_p\\) has a null space of dimension \\(p\\)</li> <li>Regularization: The penalty \\(\\lambda \\|D_p \\alpha\\|^2\\) regularizes the fit</li> </ul>"},{"location":"api/penalty/#construction-algorithm","title":"Construction Algorithm","text":"<p>The implementation uses efficient sparse matrix construction:</p> <ol> <li>Compute binomial coefficients for the difference operator</li> <li>Build row indices, column indices, and data arrays</li> <li>Construct sparse matrix in COO format</li> <li>Convert to CSR format for efficient operations</li> </ol>"},{"location":"api/penalty/#usage-in-p-splines","title":"Usage in P-Splines","text":"<p>In the P-spline objective function: \\(\\(\\min_\\alpha \\|y - B\\alpha\\|^2 + \\lambda \\|D_p \\alpha\\|^2\\)\\)</p> <p>The penalty matrix appears as \\(P = \\lambda D_p^T D_p\\), leading to the linear system: \\(\\((B^T B + \\lambda D_p^T D_p) \\alpha = B^T y\\)\\)</p>"},{"location":"api/penalty/#penalty-order-selection","title":"Penalty Order Selection","text":"<ul> <li>Order 1: Good for piecewise linear trends</li> <li>Order 2: Most common choice, penalizes curvature</li> <li>Order 3: For very smooth curves, penalizes jerk</li> <li>Higher orders: Rarely needed, may cause numerical issues</li> </ul>"},{"location":"api/utils/","title":"Utilities API","text":"<p>The utils module provides utility functions for plotting and visualization of P-spline results.</p>"},{"location":"api/utils/#functions","title":"Functions","text":"<p>Plot data and P-spline fit, subsampling for large datasets (inspired by Figure 2.9, Page 29).</p> <p>Parameters: - pspline: PSpline object - title: Plot title - subsample: Number of points to plot (default: 1000)</p> Source code in <code>src/psplines/utils.py</code> <pre><code>def plot_fit(pspline: PSpline, title: str = \"P-spline Fit\", subsample: int = 1000) -&gt; None:\n    \"\"\"\n    Plot data and P-spline fit, subsampling for large datasets (inspired by Figure 2.9, Page 29).\n\n    Parameters:\n    - pspline: PSpline object\n    - title: Plot title\n    - subsample: Number of points to plot (default: 1000)\n    \"\"\"\n    n = len(pspline.x)\n    if n &gt; subsample:\n        idx = np.random.choice(n, subsample, replace=False)\n        idx = np.sort(idx)\n        x_plot = pspline.x[idx]\n        y_plot = pspline.y[idx]\n        fit_plot = pspline.fitted_values[idx]\n    else:\n        idx = np.arange(n)\n        x_plot = pspline.x\n        y_plot = pspline.y\n        fit_plot = pspline.fitted_values\n\n    plt.scatter(x_plot, y_plot, c=\"grey\", s=1, label=\"Data\")\n    plt.plot(x_plot, fit_plot, c=\"blue\", lw=2, label=\"Fit\")\n    plt.xlabel(\"x\")\n    plt.ylabel(\"y\")\n    plt.title(title)\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n</code></pre>"},{"location":"api/utils/#usage-examples","title":"Usage Examples","text":""},{"location":"api/utils/#basic-plotting","title":"Basic Plotting","text":"<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom psplines import PSpline\nfrom psplines.utils import plot_fit\n\n# Create and fit spline\nx = np.linspace(0, 2*np.pi, 50)\ny = np.sin(x) + 0.1 * np.random.randn(50)\nspline = PSpline(x, y, nseg=15, lambda_=1.0)\nspline.fit()\n\n# Create basic plot\nfig, ax = plt.subplots(figsize=(10, 6))\nplot_fit(spline, ax=ax)\nplt.show()\n</code></pre>"},{"location":"api/utils/#customized-plotting","title":"Customized Plotting","text":"<pre><code># Plot with uncertainty and custom styling\nx_new = np.linspace(0, 2*np.pi, 200)\nfig, ax = plt.subplots(figsize=(12, 8))\n\nplot_fit(spline, x_new=x_new, show_se=True, ax=ax,\n         data_kws={'alpha': 0.6, 'color': 'darkblue'},\n         fit_kws={'color': 'red', 'linewidth': 2},\n         se_kws={'alpha': 0.2, 'color': 'red'})\n\nax.set_title('P-Spline Fit with Uncertainty')\nax.set_xlabel('x')\nax.set_ylabel('y')\nplt.show()\n</code></pre>"},{"location":"api/utils/#multiple-subplots","title":"Multiple Subplots","text":"<pre><code># Create comprehensive diagnostic plots\nfig, axes = plt.subplots(2, 2, figsize=(15, 10))\n\n# Main fit plot\nplot_fit(spline, ax=axes[0, 0], show_se=True)\naxes[0, 0].set_title('Data and Fit')\n\n# Residuals\nresiduals = spline.y - spline.fitted_values\naxes[0, 1].scatter(spline.fitted_values, residuals, alpha=0.6)\naxes[0, 1].axhline(0, color='red', linestyle='--')\naxes[0, 1].set_title('Residuals vs Fitted')\naxes[0, 1].set_xlabel('Fitted Values')\naxes[0, 1].set_ylabel('Residuals')\n\n# First derivative\ndy_dx = spline.derivative(x_new, deriv_order=1)\naxes[1, 0].plot(x_new, dy_dx, 'g-', linewidth=2)\naxes[1, 0].set_title('First Derivative')\naxes[1, 0].set_xlabel('x')\naxes[1, 0].set_ylabel('dy/dx')\n\n# Second derivative\nd2y_dx2 = spline.derivative(x_new, deriv_order=2)\naxes[1, 1].plot(x_new, d2y_dx2, 'm-', linewidth=2)\naxes[1, 1].set_title('Second Derivative')\naxes[1, 1].set_xlabel('x')\naxes[1, 1].set_ylabel('d\u00b2y/dx\u00b2')\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"api/utils/#mathematical-visualization-utilities","title":"Mathematical Visualization Utilities","text":"<p>While not part of the core API, here are some useful functions for understanding P-splines:</p>"},{"location":"api/utils/#basis-function-visualization","title":"Basis Function Visualization","text":"<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom psplines.basis import b_spline_basis\n\ndef plot_basis_functions(xl=0, xr=1, nseg=5, degree=3):\n    \"\"\"Plot individual B-spline basis functions.\"\"\"\n    x = np.linspace(xl, xr, 200)\n    B, knots = b_spline_basis(x, xl, xr, nseg, degree)\n\n    plt.figure(figsize=(12, 6))\n    for i in range(B.shape[1]):\n        plt.plot(x, B[:, i].toarray().flatten(), \n                 alpha=0.7, label=f'B_{i}')\n\n    # Mark knots\n    interior_knots = knots[degree:-degree]\n    plt.vlines(interior_knots, 0, 1, colors='red', \n               linestyles='dashed', alpha=0.5)\n\n    plt.title(f'B-spline Basis Functions (degree={degree}, nseg={nseg})')\n    plt.xlabel('x')\n    plt.ylabel('Basis Function Value')\n    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n    plt.tight_layout()\n    plt.show()\n\n# Example usage\nplot_basis_functions(nseg=5, degree=3)\n</code></pre>"},{"location":"api/utils/#penalty-matrix-visualization","title":"Penalty Matrix Visualization","text":"<pre><code>import matplotlib.pyplot as plt\nfrom psplines.penalty import difference_matrix\n\ndef plot_penalty_matrix(n=10, order=2):\n    \"\"\"Visualize the penalty matrix structure.\"\"\"\n    D = difference_matrix(n, order)\n\n    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\n    # Penalty matrix D\n    axes[0].spy(D, markersize=8)\n    axes[0].set_title(f'Difference Matrix D_{order} ({D.shape[0]}\u00d7{D.shape[1]})')\n    axes[0].set_xlabel('Column')\n    axes[0].set_ylabel('Row')\n\n    # Penalty matrix D^T D\n    DtD = D.T @ D\n    im = axes[1].imshow(DtD.toarray(), cmap='Blues')\n    axes[1].set_title(f'Penalty Matrix D_{order}^T D_{order} ({DtD.shape[0]}\u00d7{DtD.shape[1]})')\n    axes[1].set_xlabel('Column')\n    axes[1].set_ylabel('Row')\n    plt.colorbar(im, ax=axes[1])\n\n    plt.tight_layout()\n    plt.show()\n\n# Example usage\nplot_penalty_matrix(n=10, order=2)\n</code></pre>"},{"location":"api/utils/#smoothing-parameter-effect-visualization","title":"Smoothing Parameter Effect Visualization","text":"<pre><code>def plot_lambda_effect(spline, lambdas=None):\n    \"\"\"Show effect of different smoothing parameters.\"\"\"\n    if lambdas is None:\n        lambdas = np.logspace(-2, 2, 5)\n\n    x_plot = np.linspace(spline.x.min(), spline.x.max(), 200)\n\n    plt.figure(figsize=(12, 8))\n    colors = plt.cm.viridis(np.linspace(0, 1, len(lambdas)))\n\n    for i, lam in enumerate(lambdas):\n        # Create temporary spline with this lambda\n        temp_spline = PSpline(spline.x, spline.y, \n                             nseg=spline.nseg, lambda_=lam)\n        temp_spline.fit()\n        y_fit = temp_spline.predict(x_plot)\n\n        plt.plot(x_plot, y_fit, color=colors[i], \n                 linewidth=2, label=f'\u03bb = {lam:.3f}')\n\n    plt.scatter(spline.x, spline.y, alpha=0.6, \n                color='black', s=30, label='Data')\n    plt.xlabel('x')\n    plt.ylabel('y')\n    plt.title('Effect of Smoothing Parameter \u03bb')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    plt.show()\n\n# Example usage (assuming you have a fitted spline)\n# plot_lambda_effect(spline)\n</code></pre>"},{"location":"api/utils/#plotting-best-practices","title":"Plotting Best Practices","text":""},{"location":"api/utils/#figure-styling","title":"Figure Styling","text":"<pre><code># Use consistent styling across plots\nplt.rcParams.update({\n    'font.size': 12,\n    'axes.labelsize': 12,\n    'axes.titlesize': 14,\n    'legend.fontsize': 10,\n    'figure.dpi': 100,\n    'savefig.dpi': 300,\n    'savefig.bbox': 'tight'\n})\n</code></pre>"},{"location":"api/utils/#color-schemes","title":"Color Schemes","text":"<ul> <li>Data points: Use muted colors with transparency</li> <li>Fit line: Use bright, contrasting colors</li> <li>Uncertainty bands: Use same color as fit with low alpha</li> <li>Derivatives: Use distinct colors (green, magenta, etc.)</li> </ul>"},{"location":"api/utils/#layout-recommendations","title":"Layout Recommendations","text":"<ul> <li>Single plot: 10\u00d76 inches for papers, 12\u00d78 for presentations</li> <li>Subplot grids: 15\u00d710 inches for 2\u00d72 grids</li> <li>Always include axis labels and titles</li> <li>Use <code>tight_layout()</code> to avoid overlapping elements</li> <li>Save as PNG (300 DPI) for publications, PDF for vector graphics</li> </ul>"},{"location":"examples/gallery/","title":"Examples Gallery","text":"<p>This gallery showcases practical applications of PSplines across different domains and use cases.</p>"},{"location":"examples/gallery/#overview","title":"Overview","text":"<p>The examples demonstrate: - Basic Usage: Core functionality and common patterns - Parameter Selection: Automated optimization techniques - Uncertainty Quantification: Confidence intervals and bootstrap methods - Real-World Applications: Time series, scientific data, and practical scenarios</p> <p>All example code is available in the <code>/examples/</code> directory of the repository.</p>"},{"location":"examples/gallery/#quick-start-examples","title":"\ud83d\ude80 Quick Start Examples","text":""},{"location":"examples/gallery/#example-1-basic-smoothing","title":"Example 1: Basic Smoothing","text":"<p>File: <code>01_basic_usage.py</code></p> <p>A comprehensive introduction to PSplines covering data generation, fitting, prediction, and visualization.</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom psplines import PSpline\n\n# Generate noisy data\nnp.random.seed(42)\nx = np.linspace(0, 2*np.pi, 50)\ny = np.sin(x) + 0.1 * np.random.randn(50)\n\n# Create and fit P-spline\nspline = PSpline(x, y, nseg=15, lambda_=1.0)\nspline.fit()\n\n# Make predictions\nx_new = np.linspace(0, 2*np.pi, 200)\ny_pred = spline.predict(x_new)\n\n# Plot results\nplt.scatter(x, y, alpha=0.6, label='Data')\nplt.plot(x_new, y_pred, 'r-', linewidth=2, label='P-spline')\nplt.legend()\n</code></pre> <p>Key Features Demonstrated: - Basic P-spline creation and fitting - Parameter specification (<code>nseg</code>, <code>lambda_</code>) - Prediction on new data points - Visualization techniques</p> <p>Run this example: <pre><code>python examples/01_basic_usage.py\n</code></pre></p>"},{"location":"examples/gallery/#example-2-parameter-optimization","title":"Example 2: Parameter Optimization","text":"<p>File: <code>02_parameter_selection.py</code></p> <p>Demonstrates various methods for automatic parameter selection including cross-validation, AIC, and L-curve methods.</p> <pre><code>from psplines.optimize import cross_validation, aic_selection, l_curve\n\n# Automatic lambda selection using cross-validation\nspline = PSpline(x, y, nseg=20)\noptimal_lambda, cv_score = cross_validation(spline)\nspline.lambda_ = optimal_lambda\nspline.fit()\n\nprint(f\"Optimal \u03bb: {optimal_lambda:.6f}\")\nprint(f\"Effective DoF: {spline.ED:.2f}\")\n</code></pre> <p>Key Features Demonstrated: - Cross-validation optimization - AIC-based parameter selection - L-curve method - Comparison of different selection criteria - Performance vs accuracy trade-offs</p> <p>Run this example: <pre><code>python examples/02_parameter_selection.py\n</code></pre></p>"},{"location":"examples/gallery/#example-3-uncertainty-quantification","title":"Example 3: Uncertainty Quantification","text":"<p>File: <code>03_uncertainty_methods.py</code></p> <p>Comprehensive demonstration of uncertainty quantification methods including analytical standard errors, bootstrap methods, and Bayesian inference.</p> <pre><code># Get predictions with uncertainty estimates\ny_pred, se = spline.predict(x_new, return_se=True, se_method='analytic')\n\n# Create confidence bands\nconfidence_level = 0.95\nz_score = 1.96\nlower_band = y_pred - z_score * se\nupper_band = y_pred + z_score * se\n\n# Visualize with confidence intervals\nplt.fill_between(x_new, lower_band, upper_band, alpha=0.3, label='95% CI')\n</code></pre> <p>Key Features Demonstrated: - Analytical standard errors - Bootstrap uncertainty quantification - Bayesian inference (when PyMC available) - Confidence vs prediction intervals - Method comparison and validation</p> <p>Run this example: <pre><code>python examples/03_uncertainty_methods.py\n</code></pre></p>"},{"location":"examples/gallery/#example-4-real-world-application","title":"Example 4: Real-World Application","text":"<p>File: <code>04_real_world_application.py</code></p> <p>A complete workflow for analyzing real-world time series data with trend extraction, seasonal components, and forecasting.</p> <p>Key Features Demonstrated: - Data preprocessing and exploration - Multi-component analysis (trend + seasonal) - Model diagnostics and validation - Practical forecasting workflow - Performance optimization for larger datasets</p> <p>Run this example: <pre><code>python examples/04_real_world_application.py\n</code></pre></p>"},{"location":"examples/gallery/#domain-specific-applications","title":"\ud83d\udcca Domain-Specific Applications","text":""},{"location":"examples/gallery/#scientific-data-analysis","title":"Scientific Data Analysis","text":""},{"location":"examples/gallery/#signal-processing","title":"Signal Processing","text":"<pre><code># Smooth noisy measurements while preserving signal characteristics\ndef smooth_signal(t, signal, noise_level='auto'):\n    \"\"\"\n    Smooth noisy signal data using P-splines.\n\n    Parameters:\n    - t: time points\n    - signal: noisy signal values  \n    - noise_level: 'auto' for automatic detection or numeric value\n    \"\"\"\n    spline = PSpline(t, signal, nseg=min(len(t)//4, 50))\n\n    if noise_level == 'auto':\n        optimal_lambda, _ = cross_validation(spline)\n        spline.lambda_ = optimal_lambda\n    else:\n        # Use noise level to guide smoothing\n        spline.lambda_ = 1.0 / noise_level**2\n\n    spline.fit()\n    return spline\n\n# Example usage\nt = np.linspace(0, 10, 1000)\ntrue_signal = np.sin(t) + 0.5*np.sin(3*t)\nnoisy_signal = true_signal + 0.1*np.random.randn(1000)\n\nspline = smooth_signal(t, noisy_signal)\nsmooth_signal_values = spline.predict(t)\n</code></pre>"},{"location":"examples/gallery/#experimental-data-fitting","title":"Experimental Data Fitting","text":"<pre><code># Fit experimental data with derivative constraints\ndef fit_experimental_data(x_data, y_data, derivative_at_zero=None):\n    \"\"\"\n    Fit experimental data with optional derivative constraints.\n    \"\"\"\n    spline = PSpline(x_data, y_data, nseg=25)\n\n    # Optimize smoothing parameter\n    optimal_lambda, _ = cross_validation(spline, cv_method='kfold', k_folds=10)\n    spline.lambda_ = optimal_lambda\n    spline.fit()\n\n    # Check derivative constraint if provided\n    if derivative_at_zero is not None:\n        computed_derivative = spline.derivative(np.array([0]), deriv_order=1)[0]\n        print(f\"Expected derivative at x=0: {derivative_at_zero}\")\n        print(f\"Computed derivative at x=0: {computed_derivative:.4f}\")\n        print(f\"Difference: {abs(computed_derivative - derivative_at_zero):.4f}\")\n\n    return spline\n\n# Example with enzyme kinetics data\nconcentrations = np.array([0, 0.5, 1.0, 2.0, 5.0, 10.0, 20.0])\nreaction_rates = np.array([0, 2.1, 3.8, 6.2, 9.1, 10.8, 11.9])\nreaction_rates += 0.2 * np.random.randn(len(reaction_rates))  # Add noise\n\nspline = fit_experimental_data(concentrations, reaction_rates, derivative_at_zero=0)\n</code></pre>"},{"location":"examples/gallery/#financial-time-series","title":"Financial Time Series","text":""},{"location":"examples/gallery/#stock-price-analysis","title":"Stock Price Analysis","text":"<pre><code>def analyze_stock_prices(dates, prices, return_components=False):\n    \"\"\"\n    Analyze stock price data using P-splines for trend extraction.\n\n    Returns trend component and optionally volatility estimates.\n    \"\"\"\n    # Convert dates to numeric values for fitting\n    x_numeric = np.arange(len(dates))\n\n    # Log prices for multiplicative models\n    log_prices = np.log(prices)\n\n    # Fit trend with appropriate smoothing\n    trend_spline = PSpline(x_numeric, log_prices, nseg=min(len(dates)//10, 100))\n    optimal_lambda, _ = cross_validation(trend_spline)\n    trend_spline.lambda_ = optimal_lambda\n    trend_spline.fit()\n\n    # Extract trend\n    log_trend = trend_spline.predict(x_numeric)\n    trend = np.exp(log_trend)\n\n    # Compute residuals (detrended returns)\n    residuals = log_prices - log_trend\n\n    if return_components:\n        # Estimate local volatility using absolute residuals\n        abs_residuals = np.abs(residuals)\n        vol_spline = PSpline(x_numeric, abs_residuals, nseg=min(len(dates)//5, 50))\n        vol_lambda, _ = cross_validation(vol_spline)\n        vol_spline.lambda_ = vol_lambda\n        vol_spline.fit()\n\n        volatility = vol_spline.predict(x_numeric)\n\n        return {\n            'trend': trend,\n            'residuals': residuals,\n            'volatility': volatility,\n            'trend_spline': trend_spline,\n            'vol_spline': vol_spline\n        }\n\n    return trend, residuals\n\n# Example usage\ndates = pd.date_range('2020-01-01', '2023-12-31', freq='D')\n# Simulate stock prices (in practice, load real data)\nlog_returns = 0.0008 + 0.02 * np.random.randn(len(dates))\nlog_prices = np.cumsum(log_returns)\nprices = 100 * np.exp(log_prices)\n\nresults = analyze_stock_prices(dates, prices, return_components=True)\n</code></pre>"},{"location":"examples/gallery/#biomedical-applications","title":"Biomedical Applications","text":""},{"location":"examples/gallery/#growth-curve-analysis","title":"Growth Curve Analysis","text":"<pre><code>def analyze_growth_curves(time_points, measurements, subject_ids=None):\n    \"\"\"\n    Analyze biological growth curves with P-splines.\n\n    Handles individual and population-level analysis.\n    \"\"\"\n    results = {}\n\n    if subject_ids is None:\n        # Single growth curve\n        spline = PSpline(time_points, measurements, nseg=20)\n        optimal_lambda, _ = cross_validation(spline)\n        spline.lambda_ = optimal_lambda\n        spline.fit()\n\n        # Compute growth rate (first derivative)\n        growth_rate = spline.derivative(time_points, deriv_order=1)\n\n        results['spline'] = spline\n        results['growth_rate'] = growth_rate\n\n    else:\n        # Multiple subjects\n        unique_subjects = np.unique(subject_ids)\n        individual_results = {}\n\n        for subject in unique_subjects:\n            mask = subject_ids == subject\n            t_subj = time_points[mask]\n            y_subj = measurements[mask]\n\n            if len(t_subj) &gt; 5:  # Minimum points for fitting\n                spline = PSpline(t_subj, y_subj, nseg=min(15, len(t_subj)//3))\n                optimal_lambda, _ = cross_validation(spline, n_lambda=20)\n                spline.lambda_ = optimal_lambda\n                spline.fit()\n\n                individual_results[subject] = {\n                    'spline': spline,\n                    'time': t_subj,\n                    'measurements': y_subj\n                }\n\n        results['individual'] = individual_results\n\n        # Population average\n        if len(individual_results) &gt; 1:\n            # Create common time grid\n            t_common = np.linspace(time_points.min(), time_points.max(), 100)\n\n            # Evaluate each individual curve on common grid\n            individual_curves = []\n            for subj_data in individual_results.values():\n                try:\n                    curve = subj_data['spline'].predict(t_common)\n                    individual_curves.append(curve)\n                except:\n                    continue\n\n            if individual_curves:\n                individual_curves = np.array(individual_curves)\n\n                # Fit spline to mean curve\n                mean_curve = np.mean(individual_curves, axis=0)\n                population_spline = PSpline(t_common, mean_curve, nseg=25)\n                pop_lambda, _ = cross_validation(population_spline)\n                population_spline.lambda_ = pop_lambda\n                population_spline.fit()\n\n                results['population'] = {\n                    'spline': population_spline,\n                    'mean_curve': mean_curve,\n                    'individual_curves': individual_curves,\n                    'time_grid': t_common\n                }\n\n    return results\n\n# Example: Bacterial growth data\ntime_hours = np.array([0, 2, 4, 6, 8, 12, 16, 20, 24])\noptical_density = np.array([0.1, 0.15, 0.25, 0.45, 0.8, 1.2, 1.4, 1.45, 1.5])\noptical_density += 0.05 * np.random.randn(len(optical_density))  # Measurement noise\n\ngrowth_analysis = analyze_growth_curves(time_hours, optical_density)\n</code></pre>"},{"location":"examples/gallery/#advanced-techniques","title":"\ud83d\udee0 Advanced Techniques","text":""},{"location":"examples/gallery/#memory-efficient-processing","title":"Memory-Efficient Processing","text":"<pre><code>def process_large_dataset(x, y, max_points=5000):\n    \"\"\"\n    Memory-efficient P-spline fitting for large datasets.\n    \"\"\"\n    n = len(x)\n\n    if n &lt;= max_points:\n        # Standard approach\n        spline = PSpline(x, y, nseg=min(50, n//10))\n        optimal_lambda, _ = cross_validation(spline)\n        spline.lambda_ = optimal_lambda\n        spline.fit()\n        return spline\n\n    else:\n        # Subsample for parameter estimation\n        subsample_idx = np.random.choice(n, max_points, replace=False)\n        x_sub = x[subsample_idx]\n        y_sub = y[subsample_idx]\n\n        # Find optimal parameters on subsample\n        spline_sub = PSpline(x_sub, y_sub, nseg=40)\n        optimal_lambda, _ = cross_validation(spline_sub)\n\n        # Apply to full dataset with found parameters\n        spline_full = PSpline(x, y, nseg=min(60, n//50), lambda_=optimal_lambda)\n        spline_full.fit()\n\n        return spline_full\n</code></pre>"},{"location":"examples/gallery/#parallel-processing","title":"Parallel Processing","text":"<pre><code>def parallel_bootstrap_uncertainty(spline, x_eval, n_boot=1000, n_jobs=-1):\n    \"\"\"\n    Compute bootstrap uncertainty estimates in parallel.\n    \"\"\"\n    try:\n        from joblib import Parallel, delayed\n    except ImportError:\n        print(\"joblib not available, using sequential processing\")\n        n_jobs = 1\n\n    def bootstrap_sample(i):\n        # Generate bootstrap sample\n        n = spline.n\n        bootstrap_y = (spline.predict(spline.x) + \n                      np.sqrt(spline.sigma2) * np.random.randn(n))\n\n        # Fit bootstrap spline\n        boot_spline = PSpline(spline.x, bootstrap_y, \n                             nseg=spline.nseg, lambda_=spline.lambda_)\n        boot_spline.fit()\n\n        # Predict at evaluation points\n        return boot_spline.predict(x_eval)\n\n    if n_jobs == 1:\n        # Sequential\n        bootstrap_predictions = [bootstrap_sample(i) for i in range(n_boot)]\n    else:\n        # Parallel\n        bootstrap_predictions = Parallel(n_jobs=n_jobs)(\n            delayed(bootstrap_sample)(i) for i in range(n_boot)\n        )\n\n    bootstrap_predictions = np.array(bootstrap_predictions)\n\n    # Compute statistics\n    mean_pred = np.mean(bootstrap_predictions, axis=0)\n    std_pred = np.std(bootstrap_predictions, axis=0)\n\n    # Confidence intervals\n    lower_ci = np.percentile(bootstrap_predictions, 2.5, axis=0)\n    upper_ci = np.percentile(bootstrap_predictions, 97.5, axis=0)\n\n    return {\n        'mean': mean_pred,\n        'std': std_pred,\n        'lower_ci': lower_ci,\n        'upper_ci': upper_ci,\n        'samples': bootstrap_predictions\n    }\n</code></pre>"},{"location":"examples/gallery/#model-diagnostics","title":"\ud83d\udd0d Model Diagnostics","text":""},{"location":"examples/gallery/#residual-analysis","title":"Residual Analysis","text":"<pre><code>def comprehensive_diagnostics(spline, x_data, y_data):\n    \"\"\"\n    Perform comprehensive diagnostic analysis of P-spline fit.\n    \"\"\"\n    # Basic predictions and residuals\n    y_pred = spline.predict(x_data)\n    residuals = y_data - y_pred\n\n    # Standardized residuals\n    residual_std = np.std(residuals)\n    standardized_residuals = residuals / residual_std\n\n    # Diagnostic statistics\n    diagnostics = {\n        'mse': np.mean(residuals**2),\n        'mae': np.mean(np.abs(residuals)),\n        'r_squared': 1 - np.var(residuals) / np.var(y_data),\n        'effective_dof': spline.ED,\n        'aic': spline.n * np.log(np.mean(residuals**2)) + 2 * spline.ED,\n        'residual_autocorr': np.corrcoef(residuals[:-1], residuals[1:])[0, 1]\n    }\n\n    # Normality test (Shapiro-Wilk)\n    try:\n        from scipy.stats import shapiro\n        _, p_value_normality = shapiro(standardized_residuals)\n        diagnostics['normality_p'] = p_value_normality\n    except:\n        pass\n\n    # Outlier detection (using IQR method)\n    Q1, Q3 = np.percentile(standardized_residuals, [25, 75])\n    IQR = Q3 - Q1\n    outlier_threshold = 1.5 * IQR\n    outliers = np.abs(standardized_residuals) &gt; (Q3 + outlier_threshold)\n    diagnostics['n_outliers'] = np.sum(outliers)\n    diagnostics['outlier_fraction'] = np.sum(outliers) / len(residuals)\n\n    return diagnostics, residuals, standardized_residuals\n\n# Example usage in model validation\ndef validate_model(spline, x_data, y_data, plot=True):\n    \"\"\"\n    Validate P-spline model with diagnostic plots and statistics.\n    \"\"\"\n    diagnostics, residuals, std_residuals = comprehensive_diagnostics(\n        spline, x_data, y_data\n    )\n\n    print(\"=== Model Diagnostics ===\")\n    print(f\"R\u00b2 = {diagnostics['r_squared']:.4f}\")\n    print(f\"MSE = {diagnostics['mse']:.6f}\")\n    print(f\"MAE = {diagnostics['mae']:.6f}\")\n    print(f\"Effective DoF = {diagnostics['effective_dof']:.2f}\")\n    print(f\"AIC = {diagnostics['aic']:.2f}\")\n    print(f\"Outliers: {diagnostics['n_outliers']} ({diagnostics['outlier_fraction']:.1%})\")\n    print(f\"Residual autocorrelation = {diagnostics['residual_autocorr']:.4f}\")\n\n    if 'normality_p' in diagnostics:\n        print(f\"Normality test p-value = {diagnostics['normality_p']:.4f}\")\n        if diagnostics['normality_p'] &lt; 0.05:\n            print(\"\u26a0\ufe0f  Warning: Residuals may not be normally distributed\")\n\n    if plot:\n        fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n\n        # Residuals vs fitted\n        y_pred = spline.predict(x_data)\n        axes[0, 0].scatter(y_pred, residuals, alpha=0.6)\n        axes[0, 0].axhline(y=0, color='red', linestyle='--')\n        axes[0, 0].set_xlabel('Fitted Values')\n        axes[0, 0].set_ylabel('Residuals')\n        axes[0, 0].set_title('Residuals vs Fitted')\n        axes[0, 0].grid(True, alpha=0.3)\n\n        # QQ plot for normality\n        axes[0, 1].hist(std_residuals, bins=20, density=True, alpha=0.7)\n        x_norm = np.linspace(std_residuals.min(), std_residuals.max(), 100)\n        y_norm = (1/np.sqrt(2*np.pi)) * np.exp(-0.5 * x_norm**2)\n        axes[0, 1].plot(x_norm, y_norm, 'r-', linewidth=2, label='Standard Normal')\n        axes[0, 1].set_xlabel('Standardized Residuals')\n        axes[0, 1].set_ylabel('Density')\n        axes[0, 1].set_title('Residual Distribution')\n        axes[0, 1].legend()\n        axes[0, 1].grid(True, alpha=0.3)\n\n        # Residuals vs order (for autocorrelation)\n        axes[1, 0].plot(residuals, 'o-', alpha=0.6)\n        axes[1, 0].axhline(y=0, color='red', linestyle='--')\n        axes[1, 0].set_xlabel('Observation Order')\n        axes[1, 0].set_ylabel('Residuals')\n        axes[1, 0].set_title('Residuals vs Order')\n        axes[1, 0].grid(True, alpha=0.3)\n\n        # Scale-location plot\n        sqrt_abs_residuals = np.sqrt(np.abs(std_residuals))\n        axes[1, 1].scatter(y_pred, sqrt_abs_residuals, alpha=0.6)\n        axes[1, 1].set_xlabel('Fitted Values')\n        axes[1, 1].set_ylabel('\u221a|Standardized Residuals|')\n        axes[1, 1].set_title('Scale-Location Plot')\n        axes[1, 1].grid(True, alpha=0.3)\n\n        plt.tight_layout()\n        plt.show()\n\n    return diagnostics\n</code></pre>"},{"location":"examples/gallery/#running-the-examples","title":"\ud83d\udcda Running the Examples","text":""},{"location":"examples/gallery/#prerequisites","title":"Prerequisites","text":"<p>Make sure you have PSplines installed with all dependencies: <pre><code>pip install psplines[full]\n</code></pre></p> <p>Or for development: <pre><code>git clone https://github.com/graysonbellamy/psplines.git\ncd psplines\nuv sync --dev\n</code></pre></p>"},{"location":"examples/gallery/#running-individual-examples","title":"Running Individual Examples","text":"<pre><code># Basic usage\npython examples/01_basic_usage.py\n\n# Parameter selection comparison\npython examples/02_parameter_selection.py\n\n# Uncertainty quantification methods\npython examples/03_uncertainty_methods.py\n\n# Real-world application workflow\npython examples/04_real_world_application.py\n</code></pre>"},{"location":"examples/gallery/#interactive-exploration","title":"Interactive Exploration","text":"<p>For interactive exploration, use Jupyter notebooks: <pre><code>jupyter notebook examples/\n</code></pre></p>"},{"location":"examples/gallery/#best-practices","title":"\ud83c\udfaf Best Practices","text":"<p>Based on these examples, here are key best practices:</p>"},{"location":"examples/gallery/#1-parameter-selection","title":"1. Parameter Selection","text":"<ul> <li>Default: Use cross-validation for robust parameter selection</li> <li>Fast prototyping: Start with <code>lambda_=1.0</code> and <code>nseg=20</code></li> <li>Large datasets: Use AIC or reduce <code>n_lambda</code> in CV for speed</li> <li>Critical applications: Compare multiple methods (CV, AIC, L-curve)</li> </ul>"},{"location":"examples/gallery/#2-model-validation","title":"2. Model Validation","text":"<ul> <li>Always check residual plots for patterns</li> <li>Use diagnostic statistics (R\u00b2, AIC, effective DoF)</li> <li>Test on held-out data when possible</li> <li>Consider bootstrap validation for small datasets</li> </ul>"},{"location":"examples/gallery/#3-computational-efficiency","title":"3. Computational Efficiency","text":"<ul> <li>Memory: Use fewer segments for very large datasets</li> <li>Speed: Enable parallel processing (<code>n_jobs=-1</code>) for bootstrap</li> <li>Storage: Consider subsampling for datasets &gt; 10,000 points</li> </ul>"},{"location":"examples/gallery/#4-domain-specific-considerations","title":"4. Domain-Specific Considerations","text":"<ul> <li>Time series: Check for autocorrelation in residuals</li> <li>Scientific data: Validate derivatives match physical expectations</li> <li>Financial data: Use log-prices for multiplicative models</li> <li>Biomedical: Consider growth rate constraints and biological limits</li> </ul>"},{"location":"examples/gallery/#additional-resources","title":"\ud83d\udcd6 Additional Resources","text":"<ul> <li>API Reference: Complete function documentation</li> <li>Tutorials: Step-by-step guides</li> <li>Theory: Mathematical foundations</li> <li>GitHub Issues: Report bugs or request features</li> </ul> <p>All examples are provided under the same license as the PSplines package and include both synthetic and real-world applications to help you get started quickly.</p>"},{"location":"theory/mathematical-background/","title":"Mathematical Background","text":"<p>This section provides the mathematical foundation for P-splines, covering the theory, algorithms, and computational aspects.</p>"},{"location":"theory/mathematical-background/#overview","title":"Overview","text":"<p>P-splines (Penalized B-splines) combine the flexibility of B-spline basis functions with difference penalties to create a powerful smoothing framework. The method was introduced by Eilers and Marx (1996) and has become a cornerstone of modern statistical smoothing.</p>"},{"location":"theory/mathematical-background/#the-p-spline-model","title":"The P-Spline Model","text":""},{"location":"theory/mathematical-background/#basic-formulation","title":"Basic Formulation","text":"<p>Given data points \\((x_i, y_i)\\) for \\(i = 1, \\ldots, n\\), P-splines fit a smooth function \\(f(x)\\) by solving:</p> \\[\\min_\\alpha \\|y - B\\alpha\\|^2 + \\lambda \\|D_p \\alpha\\|^2\\] <p>where: - \\(y = (y_1, \\ldots, y_n)^T\\) is the response vector - \\(B\\) is the \\(n \\times m\\) B-spline basis matrix - \\(\\alpha = (\\alpha_1, \\ldots, \\alpha_m)^T\\) are the B-spline coefficients - \\(D_p\\) is the \\((m-p) \\times m\\) \\(p\\)-th order difference matrix - \\(\\lambda &gt; 0\\) is the smoothing parameter</p> <p>The smooth function is then given by: \\(\\(f(x) = \\sum_{j=1}^m \\alpha_j B_j(x)\\)\\)</p>"},{"location":"theory/mathematical-background/#matrix-form-solution","title":"Matrix Form Solution","text":"<p>The solution to the P-spline optimization problem is: \\(\\(\\hat{\\alpha} = (B^T B + \\lambda D_p^T D_p)^{-1} B^T y\\)\\)</p> <p>And the fitted values are: \\(\\(\\hat{y} = B\\hat{\\alpha} = S_\\lambda y\\)\\)</p> <p>where \\(S_\\lambda = B(B^T B + \\lambda D_p^T D_p)^{-1} B^T\\) is the smoothing matrix.</p>"},{"location":"theory/mathematical-background/#b-spline-basis-functions","title":"B-Spline Basis Functions","text":""},{"location":"theory/mathematical-background/#definition","title":"Definition","text":"<p>B-splines of degree \\(d\\) are defined recursively using the Cox-de Boor formula:</p> <p>Degree 0 (indicator functions): \\(\\(B_{i,0}(x) = \\begin{cases}  1 &amp; \\text{if } t_i \\leq x &lt; t_{i+1} \\\\ 0 &amp; \\text{otherwise} \\end{cases}\\)\\)</p> <p>Higher degrees (\\(d \\geq 1\\)): \\(\\(B_{i,d}(x) = \\frac{x - t_i}{t_{i+d} - t_i} B_{i,d-1}(x) + \\frac{t_{i+d+1} - x}{t_{i+d+1} - t_{i+1}} B_{i+1,d-1}(x)\\)\\)</p> <p>where \\(\\{t_i\\}\\) is the knot sequence.</p>"},{"location":"theory/mathematical-background/#knot-vector-construction","title":"Knot Vector Construction","text":"<p>For equally-spaced knots on interval \\([a,b]\\) with \\(K\\) segments:</p> <ol> <li>Interior knots: \\(\\{a, a + \\frac{b-a}{K}, a + 2\\frac{b-a}{K}, \\ldots, b\\}\\) (\\(K+1\\) knots)</li> <li>Extended knots: Add \\(d\\) knots at each boundary:</li> <li>Left: \\(\\{a-d\\frac{b-a}{K}, \\ldots, a-\\frac{b-a}{K}\\}\\)</li> <li>Right: \\(\\{b+\\frac{b-a}{K}, \\ldots, b+d\\frac{b-a}{K}\\}\\)</li> <li>Total knots: \\(K + 1 + 2d\\)</li> <li>Basis functions: \\(K + d\\)</li> </ol>"},{"location":"theory/mathematical-background/#properties","title":"Properties","text":"<p>Local Support: Each \\(B_{i,d}(x)\\) is non-zero only on \\([t_i, t_{i+d+1})\\).</p> <p>Partition of Unity: \\(\\sum_{i} B_{i,d}(x) = 1\\) for all \\(x\\) in the interior.</p> <p>Non-negativity: \\(B_{i,d}(x) \\geq 0\\) for all \\(i,x\\).</p> <p>Smoothness: \\(B_{i,d} \\in C^{d-1}\\), i.e., \\((d-1)\\) times continuously differentiable.</p>"},{"location":"theory/mathematical-background/#derivatives","title":"Derivatives","text":"<p>The \\(k\\)-th derivative of a B-spline satisfies: \\(\\(\\frac{d^k}{dx^k} B_{i,d}(x) = \\frac{d!}{(d-k)!} \\sum_{j=0}^k (-1)^{k-j} \\binom{k}{j} \\frac{B_{i+j,d-k}(x)}{(t_{i+d+1-j} - t_{i+j})^k}\\)\\)</p> <p>This allows efficient computation of derivative basis matrices.</p>"},{"location":"theory/mathematical-background/#difference-penalties","title":"Difference Penalties","text":""},{"location":"theory/mathematical-background/#difference-operators","title":"Difference Operators","text":"<p>The \\(p\\)-th order forward difference operator is defined recursively: - \\(\\Delta^0 \\alpha_i = \\alpha_i\\) - \\(\\Delta^1 \\alpha_i = \\alpha_{i+1} - \\alpha_i\\) - \\(\\Delta^p \\alpha_i = \\Delta^{p-1} \\alpha_{i+1} - \\Delta^{p-1} \\alpha_i\\)</p>"},{"location":"theory/mathematical-background/#difference-matrices","title":"Difference Matrices","text":"<p>The difference matrix \\(D_p\\) has entries such that \\((D_p \\alpha)_i = \\Delta^p \\alpha_i\\).</p> <p>First-order differences (\\(p=1\\)): \\(\\(D_1 = \\begin{pmatrix} -1 &amp; 1 &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; -1 &amp; 1 &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; 0 &amp; \\cdots &amp; -1 &amp; 1 \\end{pmatrix}\\)\\)</p> <p>Second-order differences (\\(p=2\\)): \\(\\(D_2 = \\begin{pmatrix} 1 &amp; -2 &amp; 1 &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; 1 &amp; -2 &amp; 1 &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; -2 &amp; 1 \\end{pmatrix}\\)\\)</p> <p>General form: For \\(p\\)-th order differences, row \\(i\\) of \\(D_p\\) contains the binomial coefficients: \\(\\(D_p[i, i+j] = (-1)^{p-j} \\binom{p}{j}, \\quad j = 0, 1, \\ldots, p\\)\\)</p>"},{"location":"theory/mathematical-background/#penalty-interpretation","title":"Penalty Interpretation","text":"<p>The penalty \\(\\|D_p \\alpha\\|^2\\) controls different aspects of smoothness:</p> <ul> <li>\\(p=1\\): \\(\\sum_{i=1}^{m-1} (\\alpha_{i+1} - \\alpha_i)^2\\) penalizes large differences</li> <li>\\(p=2\\): \\(\\sum_{i=1}^{m-2} (\\alpha_{i+2} - 2\\alpha_{i+1} + \\alpha_i)^2\\) penalizes large second differences</li> <li>\\(p=3\\): Penalizes large third differences (changes in curvature)</li> </ul>"},{"location":"theory/mathematical-background/#statistical-properties","title":"Statistical Properties","text":""},{"location":"theory/mathematical-background/#degrees-of-freedom","title":"Degrees of Freedom","text":"<p>The effective degrees of freedom is: \\(\\(\\text{df}(\\lambda) = \\text{tr}(S_\\lambda) = \\text{tr}[B(B^T B + \\lambda D_p^T D_p)^{-1} B^T]\\)\\)</p> <p>This measures the complexity of the fitted model.</p>"},{"location":"theory/mathematical-background/#bias-variance-decomposition","title":"Bias-Variance Decomposition","text":"<p>The mean squared error can be decomposed as: \\(\\(\\text{MSE} = \\text{Bias}^2 + \\text{Variance} + \\text{Noise}\\)\\)</p> <p>where: - Bias increases with \\(\\lambda\\) (more smoothing) - Variance decreases with \\(\\lambda\\) (less flexibility) - Noise is irreducible error</p>"},{"location":"theory/mathematical-background/#uncertainty-quantification","title":"Uncertainty Quantification","text":""},{"location":"theory/mathematical-background/#analytical-standard-errors","title":"Analytical Standard Errors","text":"<p>Under the assumption \\(y \\sim N(B\\alpha^*, \\sigma^2 I)\\), the covariance matrix of \\(\\hat{\\alpha}\\) is: \\(\\(\\text{Cov}(\\hat{\\alpha}) = \\sigma^2 (B^T B + \\lambda D_p^T D_p)^{-1} B^T B (B^T B + \\lambda D_p^T D_p)^{-1}\\)\\)</p> <p>For predictions \\(f(x_0) = b(x_0)^T \\hat{\\alpha}\\) where \\(b(x_0)\\) is the basis vector at \\(x_0\\): \\(\\(\\text{Var}(f(x_0)) = \\sigma^2 b(x_0)^T (B^T B + \\lambda D_p^T D_p)^{-1} b(x_0)\\)\\)</p>"},{"location":"theory/mathematical-background/#bootstrap-methods","title":"Bootstrap Methods","text":"<p>Parametric bootstrap generates replicates: \\(\\(y^{(b)} = \\hat{y} + \\epsilon^{(b)}, \\quad \\epsilon^{(b)} \\sim N(0, \\hat{\\sigma}^2 I)\\)\\)</p> <p>Each replicate gives \\(\\hat{\\alpha}^{(b)}\\) and \\(\\hat{f}^{(b)}(x_0)\\), allowing empirical variance estimation.</p>"},{"location":"theory/mathematical-background/#computational-aspects","title":"Computational Aspects","text":""},{"location":"theory/mathematical-background/#sparse-matrix-exploitation","title":"Sparse Matrix Exploitation","text":"<ul> <li>Basis matrix \\(B\\): Typically has \\((d+1)\\) non-zeros per row</li> <li>Penalty matrix \\(D_p^T D_p\\): Banded with bandwidth \\((2p+1)\\)</li> <li>System matrix: \\(B^T B + \\lambda D_p^T D_p\\) is sparse and symmetric</li> </ul>"},{"location":"theory/mathematical-background/#numerical-solution","title":"Numerical Solution","text":"<p>The linear system \\((B^T B + \\lambda D_p^T D_p) \\alpha = B^T y\\) is solved using: 1. Cholesky decomposition for small-medium problems 2. Sparse direct solvers for large sparse problems 3. Iterative methods for very large problems</p>"},{"location":"theory/mathematical-background/#complexity-analysis","title":"Complexity Analysis","text":"<ul> <li>Matrix construction: \\(O(nm)\\) for basis, \\(O(m^2)\\) for penalties</li> <li>System solution: \\(O(m^3)\\) dense, \\(O(m^{3/2})\\) sparse (typically)</li> <li>Total complexity: \\(O(nm + m^2)\\) to \\(O(nm + m^3)\\) depending on sparsity</li> </ul>"},{"location":"theory/mathematical-background/#parameter-selection-theory","title":"Parameter Selection Theory","text":""},{"location":"theory/mathematical-background/#generalized-cross-validation-gcv","title":"Generalized Cross-Validation (GCV)","text":"<p>GCV minimizes: \\(\\(\\text{GCV}(\\lambda) = \\frac{n \\|y - S_\\lambda y\\|^2}{(n - \\text{tr}(S_\\lambda))^2}\\)\\)</p> <p>This approximates leave-one-out cross-validation efficiently.</p>"},{"location":"theory/mathematical-background/#akaike-information-criterion-aic","title":"Akaike Information Criterion (AIC)","text":"<p>AIC balances fit and complexity: \\(\\(\\text{AIC}(\\lambda) = n \\log\\left(\\frac{\\|y - S_\\lambda y\\|^2}{n}\\right) + 2 \\cdot \\text{tr}(S_\\lambda)\\)\\)</p>"},{"location":"theory/mathematical-background/#l-curve-method","title":"L-Curve Method","text":"<p>Plots \\(\\log(\\|D_p \\hat{\\alpha}\\|^2)\\) vs \\(\\log(\\|y - B\\hat{\\alpha}\\|^2)\\) and selects the point of maximum curvature: \\(\\(\\kappa(\\lambda) = \\frac{2(\\rho' \\eta'' - \\rho'' \\eta')}{(\\rho'^2 + \\eta'^2)^{3/2}}\\)\\)</p> <p>where \\(\\rho(\\lambda) = \\log(\\|y - B\\hat{\\alpha}\\|^2)\\) and \\(\\eta(\\lambda) = \\log(\\|D_p \\hat{\\alpha}\\|^2)\\).</p>"},{"location":"theory/mathematical-background/#bayesian-p-splines","title":"Bayesian P-Splines","text":""},{"location":"theory/mathematical-background/#model-specification","title":"Model Specification","text":"<p>In the Bayesian framework: \\(\\(y \\mid \\alpha, \\sigma^2 \\sim N(B\\alpha, \\sigma^2 I)\\)\\) \\(\\(\\alpha \\mid \\tau \\sim N(0, (\\lambda D_p^T D_p)^{-1})\\)\\) \\(\\(\\lambda \\sim \\text{Gamma}(a, b)\\)\\) \\(\\(\\sigma^2 \\sim \\text{InverseGamma}(c, d)\\)\\)</p>"},{"location":"theory/mathematical-background/#posterior-distribution","title":"Posterior Distribution","text":"<p>The posterior for \\(\\alpha\\) is: \\(\\(\\alpha \\mid y, \\lambda, \\sigma^2 \\sim N(\\mu_\\alpha, \\Sigma_\\alpha)\\)\\)</p> <p>where: \\(\\(\\Sigma_\\alpha = \\sigma^2 (B^T B + \\lambda D_p^T D_p)^{-1}\\)\\) \\(\\(\\mu_\\alpha = \\Sigma_\\alpha B^T y / \\sigma^2\\)\\)</p>"},{"location":"theory/mathematical-background/#markov-chain-monte-carlo","title":"Markov Chain Monte Carlo","text":"<p>MCMC sampling alternates between: 1. Update \\(\\alpha\\): Multivariate normal conditional 2. Update \\(\\lambda\\): Gamma conditional (conjugate prior) 3. Update \\(\\sigma^2\\): Inverse-Gamma conditional (conjugate prior)</p> <p>This provides full posterior distributions for uncertainty quantification.</p>"},{"location":"theory/mathematical-background/#extensions-and-variants","title":"Extensions and Variants","text":""},{"location":"theory/mathematical-background/#multidimensional-p-splines","title":"Multidimensional P-Splines","text":"<p>For functions \\(f(x_1, x_2)\\), use tensor product bases: \\(\\(f(x_1, x_2) = \\sum_{i,j} \\alpha_{ij} B_i(x_1) B_j(x_2)\\)\\)</p> <p>with penalties on both dimensions.</p>"},{"location":"theory/mathematical-background/#non-gaussian-responses","title":"Non-Gaussian Responses","text":"<p>For exponential family responses: \\(\\(\\min_\\alpha -\\ell(\\mu) + \\lambda \\|D_p \\alpha\\|^2\\)\\)</p> <p>where \\(\\mu = g^{-1}(B\\alpha)\\) and \\(g\\) is the link function.</p>"},{"location":"theory/mathematical-background/#varying-penalties","title":"Varying Penalties","text":"<p>Allow spatially varying penalties: \\(\\(\\lambda \\|W D_p \\alpha\\|^2\\)\\)</p> <p>where \\(W\\) is a diagonal weight matrix.</p>"},{"location":"theory/mathematical-background/#references","title":"References","text":"<ol> <li> <p>Eilers, P. H. C., &amp; Marx, B. D. (1996). Flexible smoothing with B-splines and penalties. Statistical Science, 11(2), 89-121.</p> </li> <li> <p>Eilers, P. H. C., &amp; Marx, B. D. (2021). Practical Smoothing: The Joys of P-splines. Cambridge University Press.</p> </li> <li> <p>Ruppert, D., Wand, M. P., &amp; Carroll, R. J. (2003). Semiparametric Regression. Cambridge University Press.</p> </li> <li> <p>Wood, S. N. (2017). Generalized Additive Models: An Introduction with R. Chapman and Hall/CRC.</p> </li> </ol>"},{"location":"tutorials/advanced-features/","title":"Advanced Features Tutorial","text":"<p>This tutorial covers advanced PSplines features including constraints, different penalty orders, specialized basis configurations, and advanced computational techniques.</p>"},{"location":"tutorials/advanced-features/#introduction","title":"Introduction","text":"<p>Beyond basic smoothing, PSplines offers sophisticated features for specialized applications:</p> <ul> <li>Boundary constraints: Control derivatives at endpoints</li> <li>Interior constraints: Force specific values or derivatives</li> <li>Variable penalty orders: Different smoothness assumptions</li> <li>Sparse data handling: Techniques for irregular or sparse datasets</li> <li>Large dataset optimization: Memory-efficient approaches</li> <li>Custom basis configurations: Fine-tuning knot placement and degrees</li> </ul>"},{"location":"tutorials/advanced-features/#setup","title":"Setup","text":"<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom psplines import PSpline, BSplineBasis, build_penalty_matrix\nfrom psplines.optimize import cross_validation, l_curve\nimport scipy.sparse as sp\nfrom scipy.interpolate import UnivariateSpline\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set random seed\nnp.random.seed(42)\n</code></pre>"},{"location":"tutorials/advanced-features/#boundary-constraints","title":"Boundary Constraints","text":"<p>Control the behavior of your spline at the boundaries.</p>"},{"location":"tutorials/advanced-features/#derivative-constraints","title":"Derivative Constraints","text":"<pre><code># Generate sample data with known boundary behavior\nn = 60\nx = np.linspace(0, 2*np.pi, n)\n# Function with known derivatives at boundaries\ntrue_function = x**2 * np.sin(x)\n# f'(0) = 0, f'(2\u03c0) = (2\u03c0)^2 * cos(2\u03c0) + 2(2\u03c0) * sin(2\u03c0) = (2\u03c0)^2\ntrue_deriv_0 = 0.0\ntrue_deriv_end = (2*np.pi)**2\n\ny = true_function + 0.1 * np.random.randn(n)\n\n# Evaluation points\nx_eval = np.linspace(0, 2*np.pi, 200)\ntrue_eval = x_eval**2 * np.sin(x_eval)\n\n# Unconstrained fit\nspline_unconstrained = PSpline(x, y, nseg=25)\noptimal_lambda, _ = cross_validation(spline_unconstrained)\nspline_unconstrained.lambda_ = optimal_lambda\nspline_unconstrained.fit()\n\n# Constrained fit with derivative constraints\n# Note: This is a conceptual example - full constraint implementation \n# would require extending the PSpline class\ndef fit_with_constraints(x_data, y_data, nseg, lambda_val, constraints=None):\n    \"\"\"\n    Conceptual framework for constrained P-spline fitting.\n    In practice, this would modify the linear system to include constraints.\n    \"\"\"\n    # Create basic spline\n    spline = PSpline(x_data, y_data, nseg=nseg, lambda_=lambda_val)\n    spline.fit()\n\n    # For demonstration, we'll show how constraints would affect the solution\n    # In a full implementation, this would modify the normal equations:\n    # (B^T B + \u03bb P^T P + C^T C) \u03b1 = B^T y + C^T d\n    # where C is the constraint matrix and d are the constraint values\n\n    if constraints is not None:\n        print(f\"Would apply constraints: {constraints}\")\n        # This would implement the constraint logic\n\n    return spline\n\n# Plot unconstrained vs conceptual constrained approach\nplt.figure(figsize=(14, 10))\n\nplt.subplot(2, 2, 1)\nplt.scatter(x, y, alpha=0.6, s=30, label='Data')\nplt.plot(x_eval, true_eval, 'g--', linewidth=2, label='True function')\ny_pred_uncon = spline_unconstrained.predict(x_eval)\nplt.plot(x_eval, y_pred_uncon, 'r-', linewidth=2, label='Unconstrained P-spline')\nplt.title('Unconstrained Fit')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\n# Show derivatives at boundaries\ndy_dx_uncon = spline_unconstrained.derivative(x_eval, deriv_order=1)\nactual_deriv_0 = spline_unconstrained.derivative(np.array([0]), deriv_order=1)[0]\nactual_deriv_end = spline_unconstrained.derivative(np.array([2*np.pi]), deriv_order=1)[0]\n\nplt.subplot(2, 2, 2)\nplt.plot(x_eval, dy_dx_uncon, 'r-', linewidth=2, label=\"Unconstrained f'\")\ntrue_deriv_eval = 2*x_eval*np.sin(x_eval) + x_eval**2*np.cos(x_eval)\nplt.plot(x_eval, true_deriv_eval, 'g--', linewidth=2, label=\"True f'\")\nplt.axhline(y=true_deriv_0, color='blue', linestyle=':', label=f\"f'(0) = {true_deriv_0}\")\nplt.axhline(y=true_deriv_end, color='orange', linestyle=':', label=f\"f'(2\u03c0) = {true_deriv_end:.1f}\")\nplt.title('First Derivatives')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\nprint(f\"=== Boundary Derivative Analysis ===\")\nprint(f\"True f'(0): {true_deriv_0:.4f}\")\nprint(f\"Unconstrained f'(0): {actual_deriv_0:.4f}\")\nprint(f\"True f'(2\u03c0): {true_deriv_end:.4f}\")\nprint(f\"Unconstrained f'(2\u03c0): {actual_deriv_end:.4f}\")\n\n# Demonstrate constraint framework concept\nplt.subplot(2, 1, 2)\nplt.scatter(x, y, alpha=0.6, s=30, label='Data')\nplt.plot(x_eval, true_eval, 'g--', linewidth=2, label='True function')\nplt.plot(x_eval, y_pred_uncon, 'r-', linewidth=2, label='Unconstrained')\n\n# Manually adjust for demonstration (this would be automatic with constraints)\n# This is just for visualization - not a real constraint implementation\nspline_demo = PSpline(x, y, nseg=25, lambda_=optimal_lambda * 2)  # More smoothing\nspline_demo.fit()\ny_pred_demo = spline_demo.predict(x_eval)\nplt.plot(x_eval, y_pred_demo, 'b-', linewidth=2, alpha=0.7, \n         label='Higher smoothing (demo)')\n\nplt.title('Comparison of Smoothing Approaches')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"tutorials/advanced-features/#value-constraints","title":"Value Constraints","text":"<pre><code># Demonstrate monotonicity constraints conceptually\ndef demonstrate_monotonicity():\n    \"\"\"\n    Show how monotonicity constraints would work conceptually.\n    \"\"\"\n    # Generate monotonic data with noise\n    x_mono = np.linspace(0, 5, 50)\n    y_true_mono = np.log(x_mono + 1) + 0.5 * x_mono\n    y_mono = y_true_mono + 0.2 * np.random.randn(50)\n\n    # Unconstrained fit\n    spline_mono = PSpline(x_mono, y_mono, nseg=20)\n    opt_lambda, _ = cross_validation(spline_mono)\n    spline_mono.lambda_ = opt_lambda\n    spline_mono.fit()\n\n    # Evaluation\n    x_eval_mono = np.linspace(0, 5, 100)\n    y_pred_mono = spline_mono.predict(x_eval_mono)\n    dy_dx_mono = spline_mono.derivative(x_eval_mono, deriv_order=1)\n\n    # Check monotonicity\n    is_monotonic = np.all(dy_dx_mono &gt;= 0)\n    violations = np.sum(dy_dx_mono &lt; 0)\n\n    plt.figure(figsize=(12, 8))\n\n    plt.subplot(2, 1, 1)\n    plt.scatter(x_mono, y_mono, alpha=0.6, s=30, label='Data')\n    plt.plot(x_eval_mono, y_pred_mono, 'r-', linewidth=2, label='P-spline fit')\n    plt.plot(x_eval_mono, np.log(x_eval_mono + 1) + 0.5 * x_eval_mono, 'g--', \n             linewidth=2, label='True monotonic function')\n    plt.title(f'Monotonic Data Fit (Violations: {violations}/{len(dy_dx_mono)})')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n\n    plt.subplot(2, 1, 2)\n    plt.plot(x_eval_mono, dy_dx_mono, 'r-', linewidth=2, label=\"f'(x)\")\n    plt.axhline(y=0, color='black', linestyle='--', alpha=0.7, label='y=0')\n    negative_regions = dy_dx_mono &lt; 0\n    if np.any(negative_regions):\n        plt.fill_between(x_eval_mono, 0, dy_dx_mono, where=negative_regions, \n                        alpha=0.3, color='red', label='Violations')\n    plt.title('First Derivative (Should be \u2265 0 for monotonicity)')\n    plt.xlabel('x')\n    plt.ylabel(\"f'(x)\")\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n\n    plt.tight_layout()\n    plt.show()\n\n    print(f\"Monotonicity check: {'PASSED' if is_monotonic else 'FAILED'}\")\n    print(f\"Derivative violations: {violations}/{len(dy_dx_mono)}\")\n    print(f\"Minimum derivative: {np.min(dy_dx_mono):.4f}\")\n\n    return spline_mono\n\nspline_mono = demonstrate_monotonicity()\n</code></pre>"},{"location":"tutorials/advanced-features/#different-penalty-orders","title":"Different Penalty Orders","text":"<p>Explore how different penalty orders affect the smoothness characteristics.</p> <pre><code># Generate data with different smoothness characteristics\nx_pen = np.linspace(0, 4*np.pi, 80)\ny_smooth = np.sin(x_pen) * np.exp(-x_pen/8)  # Smooth function\ny_wiggly = y_smooth + 0.1 * np.sin(15*x_pen)  # Added high-frequency component\ny_data = y_wiggly + 0.05 * np.random.randn(80)\n\nx_eval_pen = np.linspace(0, 4*np.pi, 200)\ny_smooth_eval = np.sin(x_eval_pen) * np.exp(-x_eval_pen/8)\n\n# Compare different penalty orders\npenalty_orders = [1, 2, 3, 4]\ncolors = ['red', 'blue', 'green', 'purple']\n\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\naxes = axes.ravel()\n\nfor i, penalty_order in enumerate(penalty_orders):\n    # Create spline with specific penalty order\n    spline_pen = PSpline(x_pen, y_data, nseg=25, penalty_order=penalty_order)\n\n    # Find optimal lambda\n    opt_lambda, _ = cross_validation(spline_pen, n_lambda=30)\n    spline_pen.lambda_ = opt_lambda\n    spline_pen.fit()\n\n    # Evaluate\n    y_pred_pen = spline_pen.predict(x_eval_pen)\n\n    # Plot\n    axes[i].scatter(x_pen, y_data, alpha=0.6, s=20, color='gray', label='Noisy data')\n    axes[i].plot(x_eval_pen, y_smooth_eval, 'g--', linewidth=2, alpha=0.7, \n                 label='Smooth truth')\n    axes[i].plot(x_eval_pen, y_pred_pen, color=colors[i], linewidth=2, \n                 label=f'P{penalty_order} penalty')\n    axes[i].set_title(f'Penalty Order {penalty_order} (DoF = {spline_pen.ED:.1f})')\n    axes[i].legend()\n    axes[i].grid(True, alpha=0.3)\n\n    # Analyze smoothness\n    derivatives = []\n    for deriv_order in range(1, min(penalty_order + 2, 4)):\n        deriv = spline_pen.derivative(x_eval_pen, deriv_order=deriv_order)\n        roughness = np.sqrt(np.mean(np.diff(deriv)**2))\n        derivatives.append(roughness)\n\n    print(f\"Penalty Order {penalty_order}: \u03bb={opt_lambda:.4f}, DoF={spline_pen.ED:.1f}\")\n\nplt.tight_layout()\nplt.show()\n\n# Detailed comparison of penalty matrices\nprint(\"\\n=== Penalty Matrix Properties ===\")\nfor penalty_order in penalty_orders:\n    # Build penalty matrix\n    nb = 30  # Example number of basis functions\n    P = build_penalty_matrix(nb, penalty_order)\n\n    # Analyze properties\n    rank = np.linalg.matrix_rank(P.toarray())\n    null_space_dim = nb - rank\n\n    print(f\"Order {penalty_order}: rank={rank}, null space dim={null_space_dim}\")\n\n    # Show what the penalty penalizes\n    if penalty_order == 1:\n        print(\"  Penalizes: Large first differences (rough slopes)\")\n    elif penalty_order == 2:\n        print(\"  Penalizes: Large second differences (rough curvature)\")\n    elif penalty_order == 3:\n        print(\"  Penalizes: Large third differences (rough jerk)\")\n    else:\n        print(f\"  Penalizes: Large {penalty_order}-th differences\")\n</code></pre>"},{"location":"tutorials/advanced-features/#custom-basis-configuration","title":"Custom Basis Configuration","text":"<p>Fine-tune the B-spline basis for specialized applications.</p> <pre><code># Demonstrate custom basis configurations\ndef create_custom_basis_demo():\n    \"\"\"\n    Show how different basis configurations affect the fit.\n    \"\"\"\n    # Generate data with varying complexity\n    x_basis = np.linspace(0, 10, 100)\n    # Complex function with different behaviors in different regions\n    y_true_basis = (np.sin(x_basis) * (x_basis &lt; 3) + \n                   0.2 * x_basis**2 * ((x_basis &gt;= 3) &amp; (x_basis &lt; 7)) + \n                   np.sin(2*x_basis) * (x_basis &gt;= 7))\n    y_basis = y_true_basis + 0.1 * np.random.randn(100)\n\n    x_eval_basis = np.linspace(0, 10, 200)\n    y_true_eval = (np.sin(x_eval_basis) * (x_eval_basis &lt; 3) + \n                  0.2 * x_eval_basis**2 * ((x_eval_basis &gt;= 3) &amp; (x_eval_basis &lt; 7)) + \n                  np.sin(2*x_eval_basis) * (x_eval_basis &gt;= 7))\n\n    # Different basis configurations\n    configurations = [\n        (\"Uniform knots, degree 3\", {\"nseg\": 25, \"degree\": 3}),\n        (\"Dense knots, degree 3\", {\"nseg\": 40, \"degree\": 3}),\n        (\"Uniform knots, degree 1\", {\"nseg\": 25, \"degree\": 1}),\n        (\"Uniform knots, degree 5\", {\"nseg\": 25, \"degree\": 5})\n    ]\n\n    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n    axes = axes.ravel()\n\n    for i, (config_name, config_params) in enumerate(configurations):\n        # Create spline with custom configuration\n        spline_config = PSpline(x_basis, y_basis, **config_params)\n\n        # Optimize and fit\n        opt_lambda, _ = cross_validation(spline_config, n_lambda=20)\n        spline_config.lambda_ = opt_lambda\n        spline_config.fit()\n\n        # Evaluate\n        y_pred_config = spline_config.predict(x_eval_basis)\n\n        # Plot\n        axes[i].scatter(x_basis, y_basis, alpha=0.5, s=15, color='gray')\n        axes[i].plot(x_eval_basis, y_true_eval, 'g--', linewidth=2, label='True')\n        axes[i].plot(x_eval_basis, y_pred_config, 'r-', linewidth=2, label='P-spline')\n        axes[i].set_title(f'{config_name}\\nDoF = {spline_config.ED:.1f}')\n        axes[i].legend()\n        axes[i].grid(True, alpha=0.3)\n\n        # Calculate fit quality\n        mse = np.mean((y_basis - spline_config.predict(x_basis))**2)\n        print(f\"{config_name}: MSE = {mse:.4f}, DoF = {spline_config.ED:.1f}\")\n\n    plt.tight_layout()\n    plt.show()\n\ncreate_custom_basis_demo()\n</code></pre>"},{"location":"tutorials/advanced-features/#non-uniform-knot-placement","title":"Non-uniform Knot Placement","text":"<pre><code># Demonstrate adaptive knot placement\ndef adaptive_knot_demo():\n    \"\"\"\n    Show how non-uniform knot placement can improve fits for irregular data.\n    \"\"\"\n    # Generate data with varying complexity\n    x_adapt = np.concatenate([\n        np.linspace(0, 2, 20),      # Sparse region\n        np.linspace(2, 4, 60),      # Dense region (complex behavior)\n        np.linspace(4, 6, 20)       # Sparse region\n    ])\n\n    # Function with different complexity in different regions\n    y_true_adapt = np.where(\n        (x_adapt &gt;= 2) &amp; (x_adapt &lt;= 4),\n        np.sin(10 * x_adapt) * 0.5,  # High frequency in middle\n        0.1 * x_adapt                 # Linear elsewhere\n    )\n    y_adapt = y_true_adapt + 0.1 * np.random.randn(len(x_adapt))\n\n    x_eval_adapt = np.linspace(0, 6, 200)\n    y_true_eval_adapt = np.where(\n        (x_eval_adapt &gt;= 2) &amp; (x_eval_adapt &lt;= 4),\n        np.sin(10 * x_eval_adapt) * 0.5,\n        0.1 * x_eval_adapt\n    )\n\n    # Standard uniform knot placement\n    spline_uniform = PSpline(x_adapt, y_adapt, nseg=25)\n    opt_lambda_unif, _ = cross_validation(spline_uniform)\n    spline_uniform.lambda_ = opt_lambda_unif\n    spline_uniform.fit()\n\n    # Conceptual adaptive placement (in practice, this would require \n    # custom knot vector generation)\n    # For demonstration, we use more segments (which approximates denser knots)\n    spline_dense = PSpline(x_adapt, y_adapt, nseg=40)\n    opt_lambda_dense, _ = cross_validation(spline_dense)\n    spline_dense.lambda_ = opt_lambda_dense * 2  # More smoothing to compensate\n    spline_dense.fit()\n\n    # Compare results\n    plt.figure(figsize=(14, 6))\n\n    plt.subplot(1, 2, 1)\n    plt.scatter(x_adapt, y_adapt, alpha=0.6, s=20, c=x_adapt, cmap='viridis', \n                label='Data (colored by x)')\n    plt.colorbar(label='x position')\n    plt.plot(x_eval_adapt, y_true_eval_adapt, 'g--', linewidth=2, label='True function')\n    y_pred_uniform = spline_uniform.predict(x_eval_adapt)\n    plt.plot(x_eval_adapt, y_pred_uniform, 'r-', linewidth=2, \n             label=f'Uniform knots (DoF={spline_uniform.ED:.1f})')\n    plt.title('Uniform Knot Spacing')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n\n    plt.subplot(1, 2, 2)\n    plt.scatter(x_adapt, y_adapt, alpha=0.6, s=20, c=x_adapt, cmap='viridis')\n    plt.plot(x_eval_adapt, y_true_eval_adapt, 'g--', linewidth=2, label='True function')\n    y_pred_dense = spline_dense.predict(x_eval_adapt)\n    plt.plot(x_eval_adapt, y_pred_dense, 'b-', linewidth=2, \n             label=f'More segments (DoF={spline_dense.ED:.1f})')\n    plt.title('Denser Basis (Adaptive Approximation)')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n\n    plt.tight_layout()\n    plt.show()\n\n    # Calculate regional fit quality\n    regions = [\n        (0, 2, \"Sparse region 1\"),\n        (2, 4, \"Complex region\"),\n        (4, 6, \"Sparse region 2\")\n    ]\n\n    print(\"=== Regional Fit Quality ===\")\n    for start, end, region_name in regions:\n        mask = (x_adapt &gt;= start) &amp; (x_adapt &lt;= end)\n        if np.any(mask):\n            x_region = x_adapt[mask]\n            y_region = y_adapt[mask]\n\n            pred_uniform = spline_uniform.predict(x_region)\n            pred_dense = spline_dense.predict(x_region)\n\n            mse_uniform = np.mean((y_region - pred_uniform)**2)\n            mse_dense = np.mean((y_region - pred_dense)**2)\n\n            print(f\"{region_name}: Uniform MSE = {mse_uniform:.4f}, \"\n                  f\"Dense MSE = {mse_dense:.4f}\")\n\nadaptive_knot_demo()\n</code></pre>"},{"location":"tutorials/advanced-features/#large-dataset-optimization","title":"Large Dataset Optimization","text":"<p>Techniques for handling large datasets efficiently.</p> <pre><code># Demonstrate large dataset techniques\ndef large_dataset_demo():\n    \"\"\"\n    Show optimization techniques for large datasets.\n    \"\"\"\n    # Generate large dataset\n    n_large = 2000\n    print(f\"Generating large dataset with {n_large} points...\")\n\n    x_large = np.sort(np.random.uniform(0, 10, n_large))\n    y_true_large = np.sin(x_large) + 0.1 * x_large + 0.5 * np.sin(5*x_large)\n    y_large = y_true_large + 0.15 * np.random.randn(n_large)\n\n    # Standard approach (for comparison)\n    print(\"Standard approach...\")\n    import time\n\n    start_time = time.time()\n    spline_standard = PSpline(x_large, y_large, nseg=50)\n    opt_lambda, _ = cross_validation(spline_standard, n_lambda=20)  # Fewer lambda values\n    spline_standard.lambda_ = opt_lambda\n    spline_standard.fit()\n    standard_time = time.time() - start_time\n\n    # Memory-efficient approach with fewer segments\n    print(\"Memory-efficient approach...\")\n    start_time = time.time()\n    spline_efficient = PSpline(x_large, y_large, nseg=30)  # Fewer segments\n    opt_lambda_eff, _ = cross_validation(spline_efficient, n_lambda=15)\n    spline_efficient.lambda_ = opt_lambda_eff\n    spline_efficient.fit()\n    efficient_time = time.time() - start_time\n\n    # Subsampling approach for very large datasets\n    print(\"Subsampling approach...\")\n    subsample_size = 500\n    subsample_indices = np.random.choice(n_large, subsample_size, replace=False)\n    x_sub = x_large[subsample_indices]\n    y_sub = y_large[subsample_indices]\n\n    start_time = time.time()\n    spline_sub = PSpline(x_sub, y_sub, nseg=25)\n    opt_lambda_sub, _ = cross_validation(spline_sub)\n    spline_sub.lambda_ = opt_lambda_sub\n    spline_sub.fit()\n    subsample_time = time.time() - start_time\n\n    # Evaluate all approaches\n    x_eval_large = np.linspace(0, 10, 300)\n    y_true_eval_large = (np.sin(x_eval_large) + 0.1 * x_eval_large + \n                        0.5 * np.sin(5*x_eval_large))\n\n    y_pred_standard = spline_standard.predict(x_eval_large)\n    y_pred_efficient = spline_efficient.predict(x_eval_large)\n    y_pred_sub = spline_sub.predict(x_eval_large)\n\n    # Plot comparison (subsample for visualization)\n    vis_indices = np.random.choice(n_large, 200, replace=False)\n    x_vis = x_large[vis_indices]\n    y_vis = y_large[vis_indices]\n\n    plt.figure(figsize=(15, 10))\n\n    plt.subplot(2, 2, 1)\n    plt.scatter(x_vis, y_vis, alpha=0.4, s=10, color='gray')\n    plt.plot(x_eval_large, y_true_eval_large, 'g--', linewidth=2, label='True')\n    plt.plot(x_eval_large, y_pred_standard, 'r-', linewidth=2, \n             label=f'Standard (50 seg, DoF={spline_standard.ED:.1f})')\n    plt.title(f'Standard Approach ({standard_time:.2f}s)')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n\n    plt.subplot(2, 2, 2)\n    plt.scatter(x_vis, y_vis, alpha=0.4, s=10, color='gray')\n    plt.plot(x_eval_large, y_true_eval_large, 'g--', linewidth=2, label='True')\n    plt.plot(x_eval_large, y_pred_efficient, 'b-', linewidth=2, \n             label=f'Efficient (30 seg, DoF={spline_efficient.ED:.1f})')\n    plt.title(f'Memory Efficient ({efficient_time:.2f}s)')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n\n    plt.subplot(2, 2, 3)\n    plt.scatter(x_sub, y_sub, alpha=0.6, s=15, color='orange', label='Subsample')\n    plt.plot(x_eval_large, y_true_eval_large, 'g--', linewidth=2, label='True')\n    plt.plot(x_eval_large, y_pred_sub, 'purple', linewidth=2, \n             label=f'Subsampled (DoF={spline_sub.ED:.1f})')\n    plt.title(f'Subsampling ({subsample_time:.2f}s, n={subsample_size})')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n\n    # Error comparison\n    plt.subplot(2, 2, 4)\n    errors_standard = np.abs(y_pred_standard - y_true_eval_large)\n    errors_efficient = np.abs(y_pred_efficient - y_true_eval_large)\n    errors_sub = np.abs(y_pred_sub - y_true_eval_large)\n\n    plt.plot(x_eval_large, errors_standard, 'r-', alpha=0.7, label='Standard')\n    plt.plot(x_eval_large, errors_efficient, 'b-', alpha=0.7, label='Efficient')\n    plt.plot(x_eval_large, errors_sub, 'purple', alpha=0.7, label='Subsampled')\n    plt.xlabel('x')\n    plt.ylabel('|Error|')\n    plt.title('Absolute Errors')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n\n    plt.tight_layout()\n    plt.show()\n\n    # Performance summary\n    print(\"\\n=== Large Dataset Performance Summary ===\")\n    print(f\"Dataset size: {n_large} points\")\n    print(f\"Standard (50 seg):   {standard_time:.2f}s, MSE = {np.mean(errors_standard**2):.4f}\")\n    print(f\"Efficient (30 seg):  {efficient_time:.2f}s, MSE = {np.mean(errors_efficient**2):.4f}\")\n    print(f\"Subsampled ({subsample_size} pts): {subsample_time:.2f}s, MSE = {np.mean(errors_sub**2):.4f}\")\n    print(f\"Speed improvement (efficient): {standard_time/efficient_time:.1f}x\")\n    print(f\"Speed improvement (subsample): {standard_time/subsample_time:.1f}x\")\n\nlarge_dataset_demo()\n</code></pre>"},{"location":"tutorials/advanced-features/#specialized-smoothing-applications","title":"Specialized Smoothing Applications","text":""},{"location":"tutorials/advanced-features/#periodic-data","title":"Periodic Data","text":"<pre><code># Handle periodic data\ndef periodic_spline_demo():\n    \"\"\"\n    Demonstrate handling of periodic data.\n    \"\"\"\n    # Generate periodic data\n    n_period = 60\n    x_period = np.linspace(0, 2*np.pi, n_period)\n    y_true_period = np.sin(2*x_period) + 0.5*np.cos(5*x_period)\n    y_period = y_true_period + 0.2 * np.random.randn(n_period)\n\n    # Standard spline (non-periodic)\n    spline_nonperiodic = PSpline(x_period, y_period, nseg=20)\n    opt_lambda_np, _ = cross_validation(spline_nonperiodic)\n    spline_nonperiodic.lambda_ = opt_lambda_np\n    spline_nonperiodic.fit()\n\n    # For true periodic splines, we'd need to modify the basis construction\n    # Here we demonstrate the concept by ensuring boundary continuity\n    x_extended = np.concatenate([x_period, x_period + 2*np.pi])\n    y_extended = np.concatenate([y_period, y_period])  # Replicate data\n\n    spline_extended = PSpline(x_extended, y_extended, nseg=25)\n    opt_lambda_ext, _ = cross_validation(spline_extended, n_lambda=15)\n    spline_extended.lambda_ = opt_lambda_ext\n    spline_extended.fit()\n\n    # Evaluate\n    x_eval_period = np.linspace(0, 4*np.pi, 300)  # Two periods\n    y_true_eval_period = np.sin(2*x_eval_period) + 0.5*np.cos(5*x_eval_period)\n\n    y_pred_np = spline_nonperiodic.predict(x_eval_period[x_eval_period &lt;= 2*np.pi])\n    y_pred_ext = spline_extended.predict(x_eval_period)\n\n    plt.figure(figsize=(14, 8))\n\n    plt.subplot(2, 1, 1)\n    plt.scatter(x_period, y_period, alpha=0.7, s=40, color='red', label='Original data')\n    x_plot_short = x_eval_period[x_eval_period &lt;= 2*np.pi]\n    plt.plot(x_plot_short, y_true_eval_period[:len(x_plot_short)], 'g--', \n             linewidth=2, label='True function')\n    plt.plot(x_plot_short, y_pred_np, 'r-', linewidth=2, label='Standard P-spline')\n    plt.axvline(x=0, color='black', linestyle=':', alpha=0.5)\n    plt.axvline(x=2*np.pi, color='black', linestyle=':', alpha=0.5)\n    plt.title('Standard (Non-Periodic) P-Spline')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n\n    plt.subplot(2, 1, 2)\n    plt.scatter(x_period, y_period, alpha=0.7, s=40, color='red', label='Original data')\n    plt.scatter(x_period + 2*np.pi, y_period, alpha=0.7, s=40, color='blue', \n                label='Replicated data')\n    plt.plot(x_eval_period, y_true_eval_period, 'g--', linewidth=2, label='True function')\n    plt.plot(x_eval_period, y_pred_ext, 'b-', linewidth=2, label='Extended data P-spline')\n    plt.axvline(x=0, color='black', linestyle=':', alpha=0.5)\n    plt.axvline(x=2*np.pi, color='black', linestyle=':', alpha=0.5)\n    plt.axvline(x=4*np.pi, color='black', linestyle=':', alpha=0.5)\n    plt.title('Pseudo-Periodic P-Spline (Extended Data)')\n    plt.xlabel('x')\n    plt.ylabel('y')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n\n    plt.tight_layout()\n    plt.show()\n\n    # Check boundary continuity\n    boundary_left = spline_nonperiodic.predict(np.array([0]))[0]\n    boundary_right = spline_nonperiodic.predict(np.array([2*np.pi]))[0]\n\n    boundary_left_ext = spline_extended.predict(np.array([0]))[0]\n    boundary_right_ext = spline_extended.predict(np.array([2*np.pi]))[0]\n\n    print(\"=== Boundary Analysis ===\")\n    print(f\"Standard spline:\")\n    print(f\"  f(0) = {boundary_left:.4f}\")\n    print(f\"  f(2\u03c0) = {boundary_right:.4f}\")\n    print(f\"  Difference = {abs(boundary_right - boundary_left):.4f}\")\n    print(f\"Extended spline:\")\n    print(f\"  f(0) = {boundary_left_ext:.4f}\")\n    print(f\"  f(2\u03c0) = {boundary_right_ext:.4f}\")\n    print(f\"  Difference = {abs(boundary_right_ext - boundary_left_ext):.4f}\")\n\nperiodic_spline_demo()\n</code></pre>"},{"location":"tutorials/advanced-features/#multi-scale-data","title":"Multi-Scale Data","text":"<pre><code># Handle data with multiple scales\ndef multiscale_demo():\n    \"\"\"\n    Demonstrate techniques for multi-scale data.\n    \"\"\"\n    # Generate multi-scale data\n    x_multi = np.linspace(0, 10, 120)\n\n    # Multiple components at different scales\n    trend = 0.1 * x_multi**2  # Slow trend\n    seasonal = np.sin(2*np.pi*x_multi/2)  # Seasonal component\n    high_freq = 0.2 * np.sin(20*x_multi)  # High frequency noise\n\n    y_true_multi = trend + seasonal + high_freq\n    y_multi = y_true_multi + 0.1 * np.random.randn(120)\n\n    # Different smoothing approaches\n    lambdas = [0.01, 1.0, 100.0]  # Under-smooth, balanced, over-smooth\n\n    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n    # Original data and components\n    axes[0, 0].plot(x_multi, trend, 'g-', linewidth=2, label='Trend')\n    axes[0, 0].plot(x_multi, seasonal, 'b-', linewidth=2, label='Seasonal')\n    axes[0, 0].plot(x_multi, high_freq, 'r-', alpha=0.7, label='High freq')\n    axes[0, 0].plot(x_multi, y_true_multi, 'k-', linewidth=2, label='Total')\n    axes[0, 0].scatter(x_multi, y_multi, alpha=0.4, s=10, color='gray')\n    axes[0, 0].set_title('Data Components')\n    axes[0, 0].legend()\n    axes[0, 0].grid(True, alpha=0.3)\n\n    # Different smoothing levels\n    for i, lam in enumerate(lambdas):\n        ax = axes[0, 1] if i == 0 else axes[1, i-1]\n\n        spline_scale = PSpline(x_multi, y_multi, nseg=30, lambda_=lam)\n        spline_scale.fit()\n\n        x_eval_multi = np.linspace(0, 10, 200)\n        y_pred_multi = spline_scale.predict(x_eval_multi)\n\n        trend_eval = 0.1 * x_eval_multi**2\n        seasonal_eval = np.sin(2*np.pi*x_eval_multi/2)\n        high_freq_eval = 0.2 * np.sin(20*x_eval_multi)\n        y_true_eval = trend_eval + seasonal_eval + high_freq_eval\n\n        ax.scatter(x_multi, y_multi, alpha=0.4, s=10, color='gray')\n        ax.plot(x_eval_multi, y_true_eval, 'g--', linewidth=2, alpha=0.7, label='True')\n        ax.plot(x_eval_multi, y_pred_multi, 'r-', linewidth=2, \n                label=f'P-spline (\u03bb={lam})')\n\n        if lam == 0.01:\n            title_suffix = \"Under-smoothed\"\n        elif lam == 1.0:\n            title_suffix = \"Balanced\"\n        else:\n            title_suffix = \"Over-smoothed\"\n\n        ax.set_title(f'{title_suffix} (\u03bb={lam}, DoF={spline_scale.ED:.1f})')\n        ax.legend()\n        ax.grid(True, alpha=0.3)\n\n    plt.tight_layout()\n    plt.show()\n\n    # Frequency analysis\n    print(\"=== Multi-Scale Analysis ===\")\n    print(\"Recommendation: Use cross-validation or multiple smoothing levels\")\n    print(\"For trend extraction: Use high \u03bb (over-smooth)\")\n    print(\"For feature detection: Use low \u03bb (under-smooth)\")\n    print(\"For general use: Use CV-optimal \u03bb\")\n\nmultiscale_demo()\n</code></pre>"},{"location":"tutorials/advanced-features/#summary","title":"Summary","text":"<p>This tutorial covered advanced PSplines features:</p>"},{"location":"tutorials/advanced-features/#key-advanced-features","title":"Key Advanced Features","text":"<ol> <li>Constraints: Boundary and monotonicity constraints (conceptual framework)</li> <li>Penalty Orders: Different smoothness assumptions (1st, 2nd, 3rd order)</li> <li>Custom Basis: Non-uniform knots, different degrees</li> <li>Large Datasets: Memory-efficient techniques, subsampling</li> <li>Specialized Applications: Periodic data, multi-scale analysis</li> </ol>"},{"location":"tutorials/advanced-features/#practical-guidelines","title":"Practical Guidelines","text":"<ul> <li>Penalty Order: Use 2nd order (default) for most applications</li> <li>Large Data: Reduce segments or subsample if memory/speed is critical</li> <li>Periodic Data: Extend data or use specialized periodic basis</li> <li>Multi-Scale: Consider multiple smoothing levels or wavelets</li> <li>Custom Basis: Higher degree for smooth functions, lower for piecewise behavior</li> </ul>"},{"location":"tutorials/advanced-features/#advanced-techniques-summary","title":"Advanced Techniques Summary","text":"Technique When to Use Computational Cost Complexity Higher penalty order Very smooth data Similar Low Custom knots Irregular complexity Similar Medium Subsampling Very large datasets Much lower Low Constraints Known behavior Higher High Extended basis Periodic data Similar Medium"},{"location":"tutorials/advanced-features/#next-steps","title":"Next Steps","text":"<ul> <li>Parameter Selection: Optimize smoothing parameters</li> <li>Uncertainty Methods: Quantify prediction uncertainty  </li> <li>Basic Usage: Review fundamental concepts</li> <li>Examples Gallery: Real-world applications</li> </ul>"},{"location":"tutorials/basic-usage/","title":"Basic Usage Tutorial","text":"<p>This tutorial provides a comprehensive introduction to using PSplines for data smoothing and analysis.</p>"},{"location":"tutorials/basic-usage/#introduction","title":"Introduction","text":"<p>P-splines (Penalized B-splines) are a flexible method for smoothing noisy data. This tutorial will guide you through the fundamental concepts and practical usage.</p>"},{"location":"tutorials/basic-usage/#setting-up-your-environment","title":"Setting Up Your Environment","text":"<p>First, ensure you have PSplines installed with all dependencies:</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom psplines import PSpline\nfrom psplines.optimize import cross_validation, aic_selection, l_curve\n\n# Set random seed for reproducible results\nnp.random.seed(42)\n</code></pre>"},{"location":"tutorials/basic-usage/#creating-sample-data","title":"Creating Sample Data","text":"<p>Let's generate some noisy data to work with throughout this tutorial:</p> <pre><code># Generate synthetic data with noise\nn_points = 100\nx = np.linspace(0, 4*np.pi, n_points)\ntrue_function = np.sin(x) * np.exp(-x/8)\nnoise_level = 0.1\ny = true_function + noise_level * np.random.randn(n_points)\n\n# Plot the data\nplt.figure(figsize=(10, 6))\nplt.scatter(x, y, alpha=0.6, s=30, label='Noisy observations')\nplt.plot(x, true_function, 'g--', linewidth=2, label='True function')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.legend()\nplt.title('Sample Data: Noisy Sine Wave with Exponential Decay')\nplt.grid(True, alpha=0.3)\nplt.show()\n</code></pre>"},{"location":"tutorials/basic-usage/#basic-p-spline-fitting","title":"Basic P-Spline Fitting","text":""},{"location":"tutorials/basic-usage/#creating-and-fitting-a-p-spline","title":"Creating and Fitting a P-Spline","text":"<p>The simplest way to use PSplines is to create a <code>PSpline</code> object and fit it:</p> <pre><code># Create a P-spline with 20 segments\nspline = PSpline(x, y, nseg=20, lambda_=1.0)\n\n# Fit the spline\nspline.fit()\n\n# Create evaluation points for smooth curve\nx_eval = np.linspace(x.min(), x.max(), 200)\ny_pred = spline.predict(x_eval)\n\n# Plot results\nplt.figure(figsize=(10, 6))\nplt.scatter(x, y, alpha=0.6, s=30, label='Data')\nplt.plot(x_eval, y_pred, 'r-', linewidth=2, label='P-spline fit')\nplt.plot(x, true_function, 'g--', linewidth=2, alpha=0.7, label='True function')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.legend()\nplt.title('Basic P-Spline Fit')\nplt.grid(True, alpha=0.3)\nplt.show()\n</code></pre>"},{"location":"tutorials/basic-usage/#understanding-key-parameters","title":"Understanding Key Parameters","text":""},{"location":"tutorials/basic-usage/#number-of-segments-nseg","title":"Number of Segments (<code>nseg</code>)","text":"<p>The number of segments controls the flexibility of the spline basis:</p> <pre><code>fig, axes = plt.subplots(2, 2, figsize=(12, 10))\naxes = axes.ravel()\n\nnseg_values = [5, 15, 30, 50]\n\nfor i, nseg in enumerate(nseg_values):\n    spline = PSpline(x, y, nseg=nseg, lambda_=1.0)\n    spline.fit()\n    y_pred = spline.predict(x_eval)\n\n    axes[i].scatter(x, y, alpha=0.6, s=20)\n    axes[i].plot(x_eval, y_pred, 'r-', linewidth=2)\n    axes[i].plot(x, true_function, 'g--', alpha=0.7)\n    axes[i].set_title(f'nseg = {nseg}, DoF = {spline.ED:.1f}')\n    axes[i].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.suptitle('Effect of Number of Segments', y=1.02, fontsize=14)\nplt.show()\n</code></pre>"},{"location":"tutorials/basic-usage/#smoothing-parameter-lambda_","title":"Smoothing Parameter (<code>lambda_</code>)","text":"<p>The smoothing parameter controls the trade-off between fit and smoothness:</p> <pre><code>fig, axes = plt.subplots(2, 2, figsize=(12, 10))\naxes = axes.ravel()\n\nlambda_values = [0.001, 0.1, 10.0, 1000.0]\n\nfor i, lam in enumerate(lambda_values):\n    spline = PSpline(x, y, nseg=20, lambda_=lam)\n    spline.fit()\n    y_pred = spline.predict(x_eval)\n\n    axes[i].scatter(x, y, alpha=0.6, s=20)\n    axes[i].plot(x_eval, y_pred, 'r-', linewidth=2)\n    axes[i].plot(x, true_function, 'g--', alpha=0.7)\n    axes[i].set_title(f'\u03bb = {lam}, DoF = {spline.ED:.1f}')\n    axes[i].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.suptitle('Effect of Smoothing Parameter', y=1.02, fontsize=14)\nplt.show()\n</code></pre>"},{"location":"tutorials/basic-usage/#automatic-parameter-selection","title":"Automatic Parameter Selection","text":"<p>Instead of manually choosing parameters, you can use automatic selection methods:</p>"},{"location":"tutorials/basic-usage/#cross-validation","title":"Cross-Validation","text":"<pre><code># Create spline without specifying lambda\nspline = PSpline(x, y, nseg=20)\n\n# Use cross-validation to find optimal lambda\noptimal_lambda, cv_score = cross_validation(spline, lambda_min=1e-5, lambda_max=1e3)\nprint(f\"Optimal \u03bb: {optimal_lambda:.6f}\")\nprint(f\"CV score: {cv_score:.6f}\")\n\n# Fit with optimal parameter\nspline.lambda_ = optimal_lambda\nspline.fit()\n\ny_pred = spline.predict(x_eval)\n\nplt.figure(figsize=(10, 6))\nplt.scatter(x, y, alpha=0.6, s=30, label='Data')\nplt.plot(x_eval, y_pred, 'r-', linewidth=2, label=f'CV optimal (\u03bb={optimal_lambda:.4f})')\nplt.plot(x, true_function, 'g--', linewidth=2, alpha=0.7, label='True function')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.legend()\nplt.title(f'Cross-Validation Optimized P-Spline (DoF = {spline.ED:.1f})')\nplt.grid(True, alpha=0.3)\nplt.show()\n</code></pre>"},{"location":"tutorials/basic-usage/#aic-selection","title":"AIC Selection","text":"<pre><code># AIC-based selection\nspline_aic = PSpline(x, y, nseg=20)\noptimal_lambda_aic, aic_score = aic_selection(spline_aic, lambda_min=1e-5, lambda_max=1e3)\nprint(f\"AIC optimal \u03bb: {optimal_lambda_aic:.6f}\")\nprint(f\"AIC score: {aic_score:.6f}\")\n\nspline_aic.lambda_ = optimal_lambda_aic\nspline_aic.fit()\n</code></pre>"},{"location":"tutorials/basic-usage/#comparing-selection-methods","title":"Comparing Selection Methods","text":"<pre><code># Compare different selection methods\nmethods = ['Manual', 'Cross-Validation', 'AIC']\nlambdas = [1.0, optimal_lambda, optimal_lambda_aic]\ncolors = ['blue', 'red', 'orange']\n\nplt.figure(figsize=(12, 8))\n\nfor method, lam, color in zip(methods, lambdas, colors):\n    spline_compare = PSpline(x, y, nseg=20, lambda_=lam)\n    spline_compare.fit()\n    y_pred_compare = spline_compare.predict(x_eval)\n\n    plt.plot(x_eval, y_pred_compare, color=color, linewidth=2, \n             label=f'{method} (\u03bb={lam:.4f}, DoF={spline_compare.ED:.1f})')\n\nplt.scatter(x, y, alpha=0.6, s=30, color='gray', label='Data')\nplt.plot(x, true_function, 'g--', linewidth=2, alpha=0.7, label='True function')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.legend()\nplt.title('Comparison of Parameter Selection Methods')\nplt.grid(True, alpha=0.3)\nplt.show()\n</code></pre>"},{"location":"tutorials/basic-usage/#working-with-derivatives","title":"Working with Derivatives","text":"<p>P-splines can efficiently compute derivatives of the fitted function:</p> <pre><code># Fit spline with optimal parameters\nspline = PSpline(x, y, nseg=20, lambda_=optimal_lambda)\nspline.fit()\n\n# Compute function and derivatives\ny_pred = spline.predict(x_eval)\ndy_dx = spline.derivative(x_eval, deriv_order=1)\nd2y_dx2 = spline.derivative(x_eval, deriv_order=2)\n\n# True derivatives for comparison\ntrue_dy_dx = np.cos(x_eval) * np.exp(-x_eval/8) - (1/8) * np.sin(x_eval) * np.exp(-x_eval/8)\ntrue_d2y_dx2 = (-np.sin(x_eval) - (1/4)*np.cos(x_eval) + (1/64)*np.sin(x_eval)) * np.exp(-x_eval/8)\n\n# Plot function and derivatives\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n# Function\naxes[0].scatter(x, y, alpha=0.6, s=20, label='Data')\naxes[0].plot(x_eval, y_pred, 'r-', linewidth=2, label='P-spline')\naxes[0].plot(x_eval, true_function, 'g--', alpha=0.7, label='True')\naxes[0].set_title('Function')\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\n# First derivative\naxes[1].plot(x_eval, dy_dx, 'r-', linewidth=2, label=\"P-spline f'\")\naxes[1].plot(x_eval, true_dy_dx, 'g--', alpha=0.7, label=\"True f'\")\naxes[1].set_title('First Derivative')\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\n\n# Second derivative\naxes[2].plot(x_eval, d2y_dx2, 'r-', linewidth=2, label='P-spline f\"')\naxes[2].plot(x_eval, true_d2y_dx2, 'g--', alpha=0.7, label='True f\"')\naxes[2].set_title('Second Derivative')\naxes[2].legend()\naxes[2].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"tutorials/basic-usage/#accessing-model-information","title":"Accessing Model Information","text":"<p>After fitting, you can access various model properties:</p> <pre><code>print(\"=== Model Information ===\")\nprint(f\"Number of data points: {spline.n}\")\nprint(f\"Number of basis functions: {spline.nb}\")\nprint(f\"Effective degrees of freedom: {spline.ED:.2f}\")\nprint(f\"Residual variance (\u03c3\u00b2): {spline.sigma2:.6f}\")\nprint(f\"Residual standard deviation (\u03c3): {np.sqrt(spline.sigma2):.6f}\")\nprint(f\"Smoothing parameter (\u03bb): {spline.lambda_}\")\nprint(f\"B-spline degree: {spline.degree}\")\nprint(f\"Penalty order: {spline.penalty_order}\")\n\n# Model diagnostics\nresiduals = y - spline.predict(x)\nprint(f\"\\n=== Diagnostics ===\")\nprint(f\"Mean squared error: {np.mean(residuals**2):.6f}\")\nprint(f\"Mean absolute error: {np.mean(np.abs(residuals)):.6f}\")\nprint(f\"R-squared (approx): {1 - np.var(residuals)/np.var(y):.4f}\")\n</code></pre>"},{"location":"tutorials/basic-usage/#handling-different-data-scenarios","title":"Handling Different Data Scenarios","text":""},{"location":"tutorials/basic-usage/#non-uniform-data-spacing","title":"Non-uniform Data Spacing","text":"<p>P-splines work well with non-uniformly spaced data:</p> <pre><code># Create non-uniform data\nn_points = 80\nx_nonuniform = np.sort(np.random.uniform(0, 4*np.pi, n_points))\ny_nonuniform = (np.sin(x_nonuniform) * np.exp(-x_nonuniform/8) + \n                0.1 * np.random.randn(n_points))\n\n# Fit P-spline\nspline_nonuniform = PSpline(x_nonuniform, y_nonuniform, nseg=25)\noptimal_lambda_nu, _ = cross_validation(spline_nonuniform)\nspline_nonuniform.lambda_ = optimal_lambda_nu\nspline_nonuniform.fit()\n\n# Evaluate and plot\nx_eval_nu = np.linspace(x_nonuniform.min(), x_nonuniform.max(), 200)\ny_pred_nu = spline_nonuniform.predict(x_eval_nu)\n\nplt.figure(figsize=(10, 6))\nplt.scatter(x_nonuniform, y_nonuniform, alpha=0.6, s=30, label='Non-uniform data')\nplt.plot(x_eval_nu, y_pred_nu, 'r-', linewidth=2, label='P-spline fit')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.legend()\nplt.title('P-Spline with Non-Uniform Data Spacing')\nplt.grid(True, alpha=0.3)\nplt.show()\n</code></pre>"},{"location":"tutorials/basic-usage/#sparse-data","title":"Sparse Data","text":"<p>P-splines can handle sparse data by using fewer segments:</p> <pre><code># Create sparse data\nx_sparse = x[::5]  # Take every 5th point\ny_sparse = y[::5]\n\nprint(f\"Sparse data: {len(x_sparse)} points (originally {len(x)} points)\")\n\n# Fit with fewer segments for sparse data\nspline_sparse = PSpline(x_sparse, y_sparse, nseg=8)\noptimal_lambda_sparse, _ = cross_validation(spline_sparse)\nspline_sparse.lambda_ = optimal_lambda_sparse\nspline_sparse.fit()\n\ny_pred_sparse = spline_sparse.predict(x_eval)\n\nplt.figure(figsize=(10, 6))\nplt.scatter(x_sparse, y_sparse, alpha=0.8, s=50, label=f'Sparse data (n={len(x_sparse)})')\nplt.plot(x_eval, y_pred_sparse, 'r-', linewidth=2, label='P-spline fit')\nplt.plot(x, true_function, 'g--', alpha=0.7, label='True function')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.legend()\nplt.title('P-Spline with Sparse Data')\nplt.grid(True, alpha=0.3)\nplt.show()\n</code></pre>"},{"location":"tutorials/basic-usage/#common-pitfalls-and-solutions","title":"Common Pitfalls and Solutions","text":""},{"location":"tutorials/basic-usage/#over-smoothing","title":"Over-smoothing","text":"<p>If your fit looks too smooth and misses important features:</p> <pre><code># Demonstrate over-smoothing\nspline_oversmooth = PSpline(x, y, nseg=20, lambda_=1000.0)\nspline_oversmooth.fit()\ny_oversmooth = spline_oversmooth.predict(x_eval)\n\n# Solution: reduce lambda or use automatic selection\nspline_corrected = PSpline(x, y, nseg=20, lambda_=0.1)\nspline_corrected.fit()\ny_corrected = spline_corrected.predict(x_eval)\n\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nplt.scatter(x, y, alpha=0.6, s=30)\nplt.plot(x_eval, y_oversmooth, 'r-', linewidth=2, label=f'Over-smooth (\u03bb=1000, DoF={spline_oversmooth.ED:.1f})')\nplt.plot(x, true_function, 'g--', alpha=0.7, label='True')\nplt.title('Problem: Over-smoothing')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\nplt.subplot(1, 2, 2)\nplt.scatter(x, y, alpha=0.6, s=30)\nplt.plot(x_eval, y_corrected, 'r-', linewidth=2, label=f'Corrected (\u03bb=0.1, DoF={spline_corrected.ED:.1f})')\nplt.plot(x, true_function, 'g--', alpha=0.7, label='True')\nplt.title('Solution: Reduced \u03bb')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"tutorials/basic-usage/#under-smoothing-overfitting","title":"Under-smoothing (Overfitting)","text":"<p>If your fit follows the noise too closely:</p> <pre><code># Demonstrate under-smoothing\nspline_undersmooth = PSpline(x, y, nseg=40, lambda_=0.001)\nspline_undersmooth.fit()\ny_undersmooth = spline_undersmooth.predict(x_eval)\n\n# Solution: increase lambda or reduce nseg\nspline_corrected2 = PSpline(x, y, nseg=20, lambda_=10.0)\nspline_corrected2.fit()\ny_corrected2 = spline_corrected2.predict(x_eval)\n\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nplt.scatter(x, y, alpha=0.6, s=30)\nplt.plot(x_eval, y_undersmooth, 'r-', linewidth=2, label=f'Under-smooth (\u03bb=0.001, DoF={spline_undersmooth.ED:.1f})')\nplt.plot(x, true_function, 'g--', alpha=0.7, label='True')\nplt.title('Problem: Under-smoothing')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\nplt.subplot(1, 2, 2)\nplt.scatter(x, y, alpha=0.6, s=30)\nplt.plot(x_eval, y_corrected2, 'r-', linewidth=2, label=f'Corrected (\u03bb=10, DoF={spline_corrected2.ED:.1f})')\nplt.plot(x, true_function, 'g--', alpha=0.7, label='True')\nplt.title('Solution: Increased \u03bb')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"tutorials/basic-usage/#summary","title":"Summary","text":"<p>In this tutorial, you learned:</p> <ol> <li>Basic P-spline fitting with the <code>PSpline</code> class</li> <li>Key parameters: <code>nseg</code> (flexibility) and <code>lambda_</code> (smoothing)</li> <li>Automatic parameter selection using cross-validation and AIC</li> <li>Derivative computation for fitted functions</li> <li>Model diagnostics and information extraction</li> <li>Handling different data scenarios: non-uniform, sparse data</li> <li>Common pitfalls and their solutions</li> </ol>"},{"location":"tutorials/basic-usage/#next-steps","title":"Next Steps","text":"<ul> <li>Parameter Selection: Deep dive into optimization methods</li> <li>Uncertainty Methods: Learn about confidence intervals and bootstrap</li> <li>Advanced Features: Constraints, Bayesian inference, and more</li> <li>Examples Gallery: Real-world applications</li> </ul>"},{"location":"tutorials/parameter-selection/","title":"Parameter Selection Tutorial","text":"<p>This tutorial covers the various methods available in PSplines for automatic parameter selection, helping you choose optimal smoothing parameters for your data.</p>"},{"location":"tutorials/parameter-selection/#introduction","title":"Introduction","text":"<p>One of the most critical aspects of P-spline fitting is selecting the appropriate smoothing parameter \u03bb (lambda). This parameter controls the bias-variance trade-off:</p> <ul> <li>Small \u03bb: Less smoothing, lower bias, higher variance</li> <li>Large \u03bb: More smoothing, higher bias, lower variance</li> </ul> <p>This tutorial demonstrates all available methods for automatic \u03bb selection.</p>"},{"location":"tutorials/parameter-selection/#setup-and-sample-data","title":"Setup and Sample Data","text":"<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom psplines import PSpline\nfrom psplines.optimize import cross_validation, aic_selection, l_curve\n\n# Generate sample data\nnp.random.seed(42)\nn = 100\nx = np.linspace(0, 2*np.pi, n)\ntrue_function = np.sin(2*x) * np.exp(-x/3)\nnoise_std = 0.15\ny = true_function + noise_std * np.random.randn(n)\n\n# Evaluation points\nx_eval = np.linspace(0, 2*np.pi, 200)\ntrue_eval = np.sin(2*x_eval) * np.exp(-x_eval/3)\n\n# Plot the data\nplt.figure(figsize=(10, 6))\nplt.scatter(x, y, alpha=0.6, s=30, label='Noisy data')\nplt.plot(x_eval, true_eval, 'g--', linewidth=2, label='True function')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.legend()\nplt.title('Sample Data for Parameter Selection')\nplt.grid(True, alpha=0.3)\nplt.show()\n</code></pre>"},{"location":"tutorials/parameter-selection/#cross-validation-recommended","title":"Cross-Validation (Recommended)","text":"<p>Cross-validation is generally the most reliable method for parameter selection.</p>"},{"location":"tutorials/parameter-selection/#basic-cross-validation","title":"Basic Cross-Validation","text":"<pre><code># Create spline object\nspline = PSpline(x, y, nseg=25)\n\n# Perform cross-validation\noptimal_lambda, cv_score = cross_validation(\n    spline, \n    lambda_min=1e-6, \n    lambda_max=1e2,\n    n_lambda=50,\n    cv_method='kfold',\n    k_folds=5\n)\n\nprint(f\"Optimal \u03bb (CV): {optimal_lambda:.6f}\")\nprint(f\"CV score: {cv_score:.6f}\")\n\n# Fit with optimal parameter\nspline.lambda_ = optimal_lambda\nspline.fit()\n\n# Evaluate\ny_pred_cv = spline.predict(x_eval)\n\nplt.figure(figsize=(10, 6))\nplt.scatter(x, y, alpha=0.6, s=30, label='Data')\nplt.plot(x_eval, y_pred_cv, 'r-', linewidth=2, \n         label=f'CV optimal (\u03bb={optimal_lambda:.4f}, DoF={spline.ED:.1f})')\nplt.plot(x_eval, true_eval, 'g--', linewidth=2, alpha=0.7, label='True function')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.legend()\nplt.title('Cross-Validation Parameter Selection')\nplt.grid(True, alpha=0.3)\nplt.show()\n</code></pre>"},{"location":"tutorials/parameter-selection/#understanding-the-cv-curve","title":"Understanding the CV Curve","text":"<pre><code># Generate lambda values for CV curve\nlambda_values = np.logspace(-6, 2, 50)\ncv_scores = []\ndof_values = []\n\nfor lam in lambda_values:\n    spline_temp = PSpline(x, y, nseg=25, lambda_=lam)\n    spline_temp.fit()\n\n    # Compute CV score manually for demonstration\n    from sklearn.model_selection import KFold\n    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n    cv_errors = []\n\n    for train_idx, test_idx in kf.split(x):\n        x_train, x_test = x[train_idx], x[test_idx]\n        y_train, y_test = y[train_idx], y[test_idx]\n\n        spline_cv = PSpline(x_train, y_train, nseg=25, lambda_=lam)\n        spline_cv.fit()\n        y_pred_test = spline_cv.predict(x_test)\n        cv_errors.append(np.mean((y_test - y_pred_test)**2))\n\n    cv_scores.append(np.mean(cv_errors))\n    dof_values.append(spline_temp.ED)\n\n# Find minimum\nmin_idx = np.argmin(cv_scores)\noptimal_lambda_manual = lambda_values[min_idx]\n\n# Plot CV curve\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 10))\n\n# CV score vs lambda\nax1.semilogx(lambda_values, cv_scores, 'b-', linewidth=2, label='CV score')\nax1.axvline(optimal_lambda_manual, color='r', linestyle='--', \n            label=f'Minimum (\u03bb={optimal_lambda_manual:.4f})')\nax1.set_xlabel('\u03bb (smoothing parameter)')\nax1.set_ylabel('Cross-Validation Score')\nax1.set_title('Cross-Validation Curve')\nax1.legend()\nax1.grid(True, alpha=0.3)\n\n# DoF vs lambda\nax2.semilogx(lambda_values, dof_values, 'g-', linewidth=2, label='Degrees of Freedom')\nax2.axvline(optimal_lambda_manual, color='r', linestyle='--')\nax2.set_xlabel('\u03bb (smoothing parameter)')\nax2.set_ylabel('Effective Degrees of Freedom')\nax2.set_title('Model Complexity vs Smoothing Parameter')\nax2.legend()\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"Manual CV optimal \u03bb: {optimal_lambda_manual:.6f}\")\nprint(f\"Built-in CV optimal \u03bb: {optimal_lambda:.6f}\")\n</code></pre>"},{"location":"tutorials/parameter-selection/#aic-selection","title":"AIC Selection","text":"<p>The Akaike Information Criterion balances model fit and complexity.</p> <pre><code># AIC-based selection\nspline_aic = PSpline(x, y, nseg=25)\noptimal_lambda_aic, aic_score = aic_selection(\n    spline_aic,\n    lambda_min=1e-6,\n    lambda_max=1e2,\n    n_lambda=50\n)\n\nprint(f\"Optimal \u03bb (AIC): {optimal_lambda_aic:.6f}\")\nprint(f\"AIC score: {aic_score:.6f}\")\n\n# Fit and evaluate\nspline_aic.lambda_ = optimal_lambda_aic\nspline_aic.fit()\ny_pred_aic = spline_aic.predict(x_eval)\n\n# Plot AIC curve\nlambda_values_aic = np.logspace(-6, 2, 50)\naic_scores = []\n\nfor lam in lambda_values_aic:\n    spline_temp = PSpline(x, y, nseg=25, lambda_=lam)\n    spline_temp.fit()\n\n    # Compute AIC\n    n = len(y)\n    residuals = y - spline_temp.predict(x)\n    mse = np.mean(residuals**2)\n    aic = n * np.log(mse) + 2 * spline_temp.ED\n    aic_scores.append(aic)\n\nmin_aic_idx = np.argmin(aic_scores)\noptimal_lambda_aic_manual = lambda_values_aic[min_aic_idx]\n\nplt.figure(figsize=(10, 6))\nplt.semilogx(lambda_values_aic, aic_scores, 'b-', linewidth=2, label='AIC score')\nplt.axvline(optimal_lambda_aic_manual, color='r', linestyle='--',\n            label=f'Minimum (\u03bb={optimal_lambda_aic_manual:.4f})')\nplt.xlabel('\u03bb (smoothing parameter)')\nplt.ylabel('AIC Score')\nplt.title('AIC Selection Curve')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n</code></pre>"},{"location":"tutorials/parameter-selection/#l-curve-method","title":"L-Curve Method","text":"<p>The L-curve method finds the corner in the trade-off between fit and smoothness.</p> <pre><code># L-curve selection\nspline_lcurve = PSpline(x, y, nseg=25)\noptimal_lambda_lcurve, curvature_info = l_curve(\n    spline_lcurve,\n    lambda_min=1e-6,\n    lambda_max=1e2,\n    n_lambda=50\n)\n\nprint(f\"Optimal \u03bb (L-curve): {optimal_lambda_lcurve:.6f}\")\n\n# Fit and evaluate\nspline_lcurve.lambda_ = optimal_lambda_lcurve\nspline_lcurve.fit()\ny_pred_lcurve = spline_lcurve.predict(x_eval)\n\n# Create L-curve plot\nlambda_values_lc = np.logspace(-6, 2, 50)\nresidual_norms = []\npenalty_norms = []\n\nfor lam in lambda_values_lc:\n    spline_temp = PSpline(x, y, nseg=25, lambda_=lam)\n    spline_temp.fit()\n\n    residuals = y - spline_temp.predict(x)\n    residual_norm = np.linalg.norm(residuals)\n\n    # Compute penalty norm\n    penalty_norm = np.linalg.norm(spline_temp.penalty_matrix @ spline_temp.alpha)\n\n    residual_norms.append(residual_norm)\n    penalty_norms.append(penalty_norm)\n\n# Plot L-curve\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nplt.loglog(residual_norms, penalty_norms, 'b-', linewidth=2, marker='o', markersize=3)\noptimal_idx = np.argmin(np.abs(lambda_values_lc - optimal_lambda_lcurve))\nplt.loglog(residual_norms[optimal_idx], penalty_norms[optimal_idx], \n           'ro', markersize=10, label=f'L-curve corner (\u03bb={optimal_lambda_lcurve:.4f})')\nplt.xlabel('||Residuals||')\nplt.ylabel('||Penalty||')\nplt.title('L-Curve')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\n# Plot curvature\ncurvatures = []\nfor i, lam in enumerate(lambda_values_lc):\n    if i &gt; 0 and i &lt; len(lambda_values_lc) - 1:\n        # Simple curvature approximation\n        x1, y1 = np.log(residual_norms[i-1]), np.log(penalty_norms[i-1])\n        x2, y2 = np.log(residual_norms[i]), np.log(penalty_norms[i])\n        x3, y3 = np.log(residual_norms[i+1]), np.log(penalty_norms[i+1])\n\n        # Curvature formula for parametric curve\n        dx1, dy1 = x2 - x1, y2 - y1\n        dx2, dy2 = x3 - x2, y3 - y2\n\n        curvature = abs(dx1*dy2 - dy1*dx2) / (dx1**2 + dy1**2)**1.5 if (dx1**2 + dy1**2) &gt; 0 else 0\n        curvatures.append(curvature)\n    else:\n        curvatures.append(0)\n\nplt.subplot(1, 2, 2)\nplt.semilogx(lambda_values_lc, curvatures, 'g-', linewidth=2, label='Curvature')\nplt.axvline(optimal_lambda_lcurve, color='r', linestyle='--', label=f'Max curvature')\nplt.xlabel('\u03bb (smoothing parameter)')\nplt.ylabel('Curvature')\nplt.title('L-Curve Curvature')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"tutorials/parameter-selection/#generalized-cross-validation-gcv","title":"Generalized Cross-Validation (GCV)","text":"<p>GCV is an efficient approximation to leave-one-out cross-validation.</p> <pre><code>def gcv_score(spline):\n    \"\"\"Compute GCV score for fitted spline.\"\"\"\n    n = spline.n\n    residuals = spline.y - spline.predict(spline.x)\n    rss = np.sum(residuals**2)\n\n    # GCV formula\n    gcv = (n * rss) / (n - spline.ED)**2\n    return gcv\n\n# GCV selection\nlambda_values_gcv = np.logspace(-6, 2, 50)\ngcv_scores = []\n\nfor lam in lambda_values_gcv:\n    spline_temp = PSpline(x, y, nseg=25, lambda_=lam)\n    spline_temp.fit()\n    gcv_scores.append(gcv_score(spline_temp))\n\n# Find optimal\nmin_gcv_idx = np.argmin(gcv_scores)\noptimal_lambda_gcv = lambda_values_gcv[min_gcv_idx]\n\nprint(f\"Optimal \u03bb (GCV): {optimal_lambda_gcv:.6f}\")\n\n# Fit with optimal parameter\nspline_gcv = PSpline(x, y, nseg=25, lambda_=optimal_lambda_gcv)\nspline_gcv.fit()\ny_pred_gcv = spline_gcv.predict(x_eval)\n\n# Plot GCV curve\nplt.figure(figsize=(10, 6))\nplt.semilogx(lambda_values_gcv, gcv_scores, 'b-', linewidth=2, label='GCV score')\nplt.axvline(optimal_lambda_gcv, color='r', linestyle='--',\n            label=f'Minimum (\u03bb={optimal_lambda_gcv:.4f})')\nplt.xlabel('\u03bb (smoothing parameter)')\nplt.ylabel('GCV Score')\nplt.title('Generalized Cross-Validation Curve')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n</code></pre>"},{"location":"tutorials/parameter-selection/#comparing-all-methods","title":"Comparing All Methods","text":"<p>Let's compare all parameter selection methods:</p> <pre><code># Collect all methods and their optimal lambdas\nmethods = {\n    'Cross-Validation': optimal_lambda,\n    'AIC': optimal_lambda_aic,\n    'L-Curve': optimal_lambda_lcurve,\n    'GCV': optimal_lambda_gcv\n}\n\n# Fit splines with each method\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\naxes = axes.ravel()\n\ncolors = ['red', 'blue', 'green', 'orange']\npredictions = {}\n\nfor i, (method, lam) in enumerate(methods.items()):\n    spline_method = PSpline(x, y, nseg=25, lambda_=lam)\n    spline_method.fit()\n    y_pred_method = spline_method.predict(x_eval)\n    predictions[method] = y_pred_method\n\n    # Calculate metrics\n    residuals = y - spline_method.predict(x)\n    mse = np.mean(residuals**2)\n\n    axes[i].scatter(x, y, alpha=0.6, s=20, color='gray')\n    axes[i].plot(x_eval, y_pred_method, color=colors[i], linewidth=2, \n                label=f'{method}')\n    axes[i].plot(x_eval, true_eval, 'g--', alpha=0.7, linewidth=1.5, label='True')\n    axes[i].set_title(f'{method}\\n\u03bb={lam:.4f}, DoF={spline_method.ED:.1f}, MSE={mse:.4f}')\n    axes[i].legend()\n    axes[i].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Summary comparison\nprint(\"\\n=== Parameter Selection Comparison ===\")\nprint(f\"{'Method':&lt;20} {'Lambda':&lt;12} {'DoF':&lt;8} {'MSE':&lt;12}\")\nprint(\"-\" * 55)\n\nfor method, lam in methods.items():\n    spline_temp = PSpline(x, y, nseg=25, lambda_=lam)\n    spline_temp.fit()\n    residuals = y - spline_temp.predict(x)\n    mse = np.mean(residuals**2)\n    print(f\"{method:&lt;20} {lam:&lt;12.6f} {spline_temp.ED:&lt;8.2f} {mse:&lt;12.6f}\")\n</code></pre>"},{"location":"tutorials/parameter-selection/#method-specific-parameters-and-options","title":"Method-Specific Parameters and Options","text":""},{"location":"tutorials/parameter-selection/#advanced-cross-validation-options","title":"Advanced Cross-Validation Options","text":"<pre><code># Different CV methods\ncv_methods = ['kfold', 'loo']  # k-fold and leave-one-out\n\nfor cv_method in cv_methods:\n    if cv_method == 'kfold':\n        opt_lambda, score = cross_validation(\n            PSpline(x, y, nseg=25),\n            cv_method='kfold',\n            k_folds=10,  # More folds for better estimate\n            lambda_min=1e-6,\n            lambda_max=1e2,\n            n_lambda=30\n        )\n    else:  # Leave-one-out\n        opt_lambda, score = cross_validation(\n            PSpline(x, y, nseg=25),\n            cv_method='loo',\n            lambda_min=1e-6,\n            lambda_max=1e2,\n            n_lambda=30\n        )\n\n    print(f\"{cv_method.upper()} CV: \u03bb = {opt_lambda:.6f}, score = {score:.6f}\")\n</code></pre>"},{"location":"tutorials/parameter-selection/#grid-search-vs-optimization","title":"Grid Search vs. Optimization","text":"<pre><code># Grid search (exhaustive)\nlambda_grid = np.logspace(-4, 1, 100)  # Fine grid\nspline_grid = PSpline(x, y, nseg=25)\n\nbest_lambda_grid = None\nbest_score = float('inf')\n\nfor lam in lambda_grid:\n    spline_grid.lambda_ = lam\n    spline_grid.fit()\n\n    # Use GCV as criterion\n    score = gcv_score(spline_grid)\n\n    if score &lt; best_score:\n        best_score = score\n        best_lambda_grid = lam\n\nprint(f\"Grid search optimal \u03bb: {best_lambda_grid:.6f}\")\n\n# Compare with optimization-based approach\nfrom scipy.optimize import minimize_scalar\n\ndef gcv_objective(log_lambda):\n    \"\"\"Objective function for optimization.\"\"\"\n    lam = 10**log_lambda\n    spline_temp = PSpline(x, y, nseg=25, lambda_=lam)\n    spline_temp.fit()\n    return gcv_score(spline_temp)\n\n# Optimize\nresult = minimize_scalar(gcv_objective, bounds=(-6, 2), method='bounded')\noptimal_lambda_opt = 10**result.x\n\nprint(f\"Optimization-based optimal \u03bb: {optimal_lambda_opt:.6f}\")\n</code></pre>"},{"location":"tutorials/parameter-selection/#choosing-the-right-method","title":"Choosing the Right Method","text":""},{"location":"tutorials/parameter-selection/#decision-guidelines","title":"Decision Guidelines","text":"<pre><code>def recommend_method(n_points, noise_level='unknown'):\n    \"\"\"\n    Recommend parameter selection method based on data characteristics.\n    \"\"\"\n    recommendations = []\n\n    if n_points &lt; 50:\n        recommendations.append(\"\u2022 Use AIC or L-curve (CV may be unreliable with small samples)\")\n    elif n_points &lt; 200:\n        recommendations.append(\"\u2022 Cross-validation (5-fold) or AIC are good choices\")\n    else:\n        recommendations.append(\"\u2022 Cross-validation (10-fold) or GCV for efficiency\")\n\n    if noise_level == 'high':\n        recommendations.append(\"\u2022 Consider L-curve method - more robust to high noise\")\n    elif noise_level == 'low':\n        recommendations.append(\"\u2022 AIC works well with low-noise data\")\n\n    return recommendations\n\n# Example usage\nn = len(x)\nprint(f\"Data size: {n} points\")\nprint(\"Recommendations:\")\nfor rec in recommend_method(n, 'medium'):\n    print(rec)\n</code></pre>"},{"location":"tutorials/parameter-selection/#performance-comparison","title":"Performance Comparison","text":"<pre><code>import time\n\n# Time different methods\nmethods_to_time = [\n    ('Cross-Validation', lambda s: cross_validation(s, n_lambda=20)),\n    ('AIC', lambda s: aic_selection(s, n_lambda=20)),\n    ('L-Curve', lambda s: l_curve(s, n_lambda=20))\n]\n\nprint(\"=== Performance Comparison ===\")\nprint(f\"{'Method':&lt;20} {'Time (s)':&lt;10} {'Lambda':&lt;12}\")\nprint(\"-\" * 45)\n\nfor method_name, method_func in methods_to_time:\n    spline_test = PSpline(x, y, nseg=25)\n\n    start_time = time.time()\n    opt_lambda, _ = method_func(spline_test)\n    end_time = time.time()\n\n    elapsed = end_time - start_time\n    print(f\"{method_name:&lt;20} {elapsed:&lt;10.4f} {opt_lambda:&lt;12.6f}\")\n</code></pre>"},{"location":"tutorials/parameter-selection/#advanced-tips","title":"Advanced Tips","text":""},{"location":"tutorials/parameter-selection/#multiple-criteria-consensus","title":"Multiple Criteria Consensus","text":"<pre><code># Use multiple methods and take consensus\nall_lambdas = [\n    optimal_lambda,      # CV\n    optimal_lambda_aic,  # AIC\n    optimal_lambda_gcv,  # GCV\n    optimal_lambda_lcurve # L-curve\n]\n\n# Geometric mean as consensus (works well for log-scale parameters)\nconsensus_lambda = np.exp(np.mean(np.log(all_lambdas)))\nprint(f\"Consensus \u03bb (geometric mean): {consensus_lambda:.6f}\")\n\n# Fit with consensus parameter\nspline_consensus = PSpline(x, y, nseg=25, lambda_=consensus_lambda)\nspline_consensus.fit()\ny_pred_consensus = spline_consensus.predict(x_eval)\n\n# Compare consensus with individual methods\nplt.figure(figsize=(12, 8))\nplt.scatter(x, y, alpha=0.6, s=30, color='gray', label='Data')\n\nmethods_plot = [\n    ('CV', optimal_lambda, 'red'),\n    ('AIC', optimal_lambda_aic, 'blue'),\n    ('GCV', optimal_lambda_gcv, 'green'),\n    ('Consensus', consensus_lambda, 'purple')\n]\n\nfor method, lam, color in methods_plot:\n    spline_temp = PSpline(x, y, nseg=25, lambda_=lam)\n    spline_temp.fit()\n    y_pred_temp = spline_temp.predict(x_eval)\n    plt.plot(x_eval, y_pred_temp, color=color, linewidth=2, alpha=0.8,\n             label=f'{method} (\u03bb={lam:.4f})')\n\nplt.plot(x_eval, true_eval, 'g--', linewidth=2, alpha=0.7, label='True function')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.legend()\nplt.title('Consensus Parameter Selection')\nplt.grid(True, alpha=0.3)\nplt.show()\n</code></pre>"},{"location":"tutorials/parameter-selection/#summary","title":"Summary","text":"<p>In this tutorial, you learned:</p> <ol> <li>Cross-validation: Most reliable, especially k-fold CV</li> <li>AIC selection: Good balance of accuracy and efficiency</li> <li>L-curve method: Robust to noise, geometric interpretation</li> <li>GCV: Efficient approximation to leave-one-out CV</li> <li>Method selection: Guidelines based on data characteristics</li> <li>Advanced techniques: Grid search, optimization, consensus methods</li> </ol>"},{"location":"tutorials/parameter-selection/#recommendations","title":"Recommendations:","text":"<ul> <li>General use: 5-10 fold cross-validation</li> <li>Small datasets: AIC or L-curve</li> <li>Large datasets: GCV or AIC for efficiency</li> <li>High noise: L-curve method</li> <li>Critical applications: Use consensus of multiple methods</li> </ul>"},{"location":"tutorials/parameter-selection/#next-steps","title":"Next Steps","text":"<ul> <li>Uncertainty Methods: Learn about confidence intervals</li> <li>Advanced Features: Constraints and specialized techniques</li> <li>Basic Usage: Review fundamental concepts</li> </ul>"},{"location":"tutorials/uncertainty-methods/","title":"Uncertainty Quantification Tutorial","text":"<p>This tutorial covers the different methods available in PSplines for quantifying uncertainty in your fitted curves, including analytical standard errors, bootstrap methods, and Bayesian approaches.</p>"},{"location":"tutorials/uncertainty-methods/#introduction","title":"Introduction","text":"<p>Uncertainty quantification is crucial for: - Understanding the reliability of your smooth fits - Making statistical inferences from your results - Communicating confidence in predictions - Identifying regions where more data might be needed</p> <p>PSplines offers three main approaches to uncertainty quantification: 1. Analytical standard errors (fast, approximate) 2. Bootstrap methods (slower, empirical) 3. Bayesian inference (comprehensive, requires PyMC)</p>"},{"location":"tutorials/uncertainty-methods/#setup-and-sample-data","title":"Setup and Sample Data","text":"<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom psplines import PSpline\nfrom psplines.optimize import cross_validation\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Generate sample data with known uncertainty\nnp.random.seed(42)\nn = 80\nx = np.sort(np.random.uniform(0, 3*np.pi, n))\ntrue_function = np.sin(x) * np.exp(-x/5) + 0.2 * np.cos(3*x)\nnoise_std = 0.12\ny = true_function + noise_std * np.random.randn(n)\n\n# Evaluation points\nx_eval = np.linspace(0, 3*np.pi, 200)\ntrue_eval = np.sin(x_eval) * np.exp(-x_eval/5) + 0.2 * np.cos(3*x_eval)\n\n# Plot data\nplt.figure(figsize=(12, 6))\nplt.scatter(x, y, alpha=0.7, s=40, label='Noisy observations')\nplt.plot(x_eval, true_eval, 'g--', linewidth=2, label='True function')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.legend()\nplt.title('Sample Data for Uncertainty Analysis')\nplt.grid(True, alpha=0.3)\nplt.show()\n\n# Fit optimal P-spline\nspline = PSpline(x, y, nseg=30)\noptimal_lambda, _ = cross_validation(spline)\nspline.lambda_ = optimal_lambda\nspline.fit()\n\nprint(f\"Optimal \u03bb: {optimal_lambda:.6f}\")\nprint(f\"Effective DoF: {spline.ED:.2f}\")\nprint(f\"Estimated \u03c3\u00b2: {spline.sigma2:.6f}\")\n</code></pre>"},{"location":"tutorials/uncertainty-methods/#method-1-analytical-standard-errors","title":"Method 1: Analytical Standard Errors","text":"<p>The fastest method uses analytical formulas based on the covariance matrix.</p>"},{"location":"tutorials/uncertainty-methods/#basic-usage","title":"Basic Usage","text":"<pre><code># Get predictions with analytical standard errors\ny_pred_analytical, se_analytical = spline.predict(x_eval, return_se=True, se_method='analytic')\n\n# Create confidence bands\nconfidence_level = 0.95\nz_score = 1.96  # For 95% confidence\nlower_band = y_pred_analytical - z_score * se_analytical\nupper_band = y_pred_analytical + z_score * se_analytical\n\n# Plot results\nplt.figure(figsize=(12, 8))\nplt.scatter(x, y, alpha=0.7, s=40, color='gray', label='Data')\nplt.plot(x_eval, true_eval, 'g--', linewidth=2, label='True function')\nplt.plot(x_eval, y_pred_analytical, 'r-', linewidth=2, label='P-spline fit')\nplt.fill_between(x_eval, lower_band, upper_band, alpha=0.3, color='red', \n                 label=f'{int(confidence_level*100)}% Confidence Band')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.legend()\nplt.title('Analytical Standard Errors')\nplt.grid(True, alpha=0.3)\nplt.show()\n\n# Compute coverage statistics\n# Points where true function lies within confidence band\nin_band = (true_eval &gt;= lower_band) &amp; (true_eval &lt;= upper_band)\ncoverage = np.mean(in_band)\nprint(f\"Empirical coverage: {coverage:.3f} (expected: {confidence_level:.3f})\")\n</code></pre>"},{"location":"tutorials/uncertainty-methods/#understanding-the-analytical-method","title":"Understanding the Analytical Method","text":"<p>The analytical method computes standard errors using:</p> <pre><code># Show the mathematical foundation\nprint(\"=== Analytical Method Details ===\")\nprint(f\"Model: y = B\u03b1 + \u03b5, where \u03b5 ~ N(0, \u03c3\u00b2I)\")\nprint(f\"Standard errors based on: SE(f(x)) = \u03c3 \u221a[b(x)\u1d40(B\u1d40B + \u03bbP\u1d40P)\u207b\u00b9B\u1d40B(B\u1d40B + \u03bbP\u1d40P)\u207b\u00b9b(x)]\")\nprint(f\"Where:\")\nprint(f\"  - B is the basis matrix\")\nprint(f\"  - P is the penalty matrix\")\nprint(f\"  - b(x) is the basis vector at point x\")\nprint(f\"  - \u03c3\u00b2 = {spline.sigma2:.6f} (estimated from residuals)\")\n\n# Visualize pointwise standard errors\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nplt.plot(x_eval, se_analytical, 'b-', linewidth=2, label='Standard Error')\nplt.axhline(y=noise_std, color='g', linestyle='--', label=f'True noise std ({noise_std})')\nplt.xlabel('x')\nplt.ylabel('Standard Error')\nplt.title('Pointwise Standard Errors')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\nplt.subplot(1, 2, 2)\n# Show relative uncertainty\nrelative_se = se_analytical / np.abs(y_pred_analytical)\nplt.plot(x_eval, relative_se, 'purple', linewidth=2, label='Relative SE')\nplt.xlabel('x')\nplt.ylabel('Relative Standard Error')\nplt.title('Relative Uncertainty')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"Mean standard error: {np.mean(se_analytical):.4f}\")\nprint(f\"Min/Max standard error: {np.min(se_analytical):.4f} / {np.max(se_analytical):.4f}\")\n</code></pre>"},{"location":"tutorials/uncertainty-methods/#method-2-bootstrap-methods","title":"Method 2: Bootstrap Methods","text":"<p>Bootstrap methods provide empirical estimates by resampling from the fitted model.</p>"},{"location":"tutorials/uncertainty-methods/#parametric-bootstrap","title":"Parametric Bootstrap","text":"<pre><code># Parametric bootstrap (assumes Gaussian errors)\ny_pred_bootstrap, se_bootstrap = spline.predict(\n    x_eval, \n    return_se=True, \n    se_method='bootstrap',\n    bootstrap_method='parametric',\n    B_boot=500,  # Number of bootstrap samples\n    n_jobs=2     # Parallel processing\n)\n\n# Create confidence bands\nlower_band_boot = y_pred_bootstrap - 1.96 * se_bootstrap\nupper_band_boot = y_pred_bootstrap + 1.96 * se_bootstrap\n\n# Plot comparison\nplt.figure(figsize=(14, 8))\n\nplt.subplot(2, 1, 1)\nplt.scatter(x, y, alpha=0.7, s=30, color='gray', label='Data')\nplt.plot(x_eval, true_eval, 'g--', linewidth=2, label='True function')\nplt.plot(x_eval, y_pred_analytical, 'r-', linewidth=2, label='Fit')\nplt.fill_between(x_eval, lower_band, upper_band, alpha=0.3, color='red', \n                 label='Analytical 95% CI')\nplt.title('Analytical Standard Errors')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\nplt.subplot(2, 1, 2)\nplt.scatter(x, y, alpha=0.7, s=30, color='gray', label='Data')\nplt.plot(x_eval, true_eval, 'g--', linewidth=2, label='True function')\nplt.plot(x_eval, y_pred_bootstrap, 'b-', linewidth=2, label='Fit')\nplt.fill_between(x_eval, lower_band_boot, upper_band_boot, alpha=0.3, color='blue', \n                 label='Bootstrap 95% CI')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Bootstrap Standard Errors')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Compare coverage\nin_band_boot = (true_eval &gt;= lower_band_boot) &amp; (true_eval &lt;= upper_band_boot)\ncoverage_boot = np.mean(in_band_boot)\n\nprint(\"=== Method Comparison ===\")\nprint(f\"Analytical coverage: {coverage:.3f}\")\nprint(f\"Bootstrap coverage:  {coverage_boot:.3f}\")\nprint(f\"Expected coverage:   0.950\")\n</code></pre>"},{"location":"tutorials/uncertainty-methods/#residual-bootstrap","title":"Residual Bootstrap","text":"<pre><code># Residual bootstrap (resamples residuals)\ny_pred_resid_boot, se_resid_boot = spline.predict(\n    x_eval, \n    return_se=True, \n    se_method='bootstrap',\n    bootstrap_method='residual',\n    B_boot=500,\n    n_jobs=2\n)\n\n# Compare all three methods\nmethods_data = [\n    ('Analytical', se_analytical, 'red'),\n    ('Parametric Bootstrap', se_bootstrap, 'blue'),\n    ('Residual Bootstrap', se_resid_boot, 'green')\n]\n\nplt.figure(figsize=(14, 6))\n\nplt.subplot(1, 2, 1)\nfor method_name, se_values, color in methods_data:\n    plt.plot(x_eval, se_values, color=color, linewidth=2, label=method_name)\n\nplt.axhline(y=noise_std, color='black', linestyle='--', alpha=0.7, \n            label=f'True noise std ({noise_std})')\nplt.xlabel('x')\nplt.ylabel('Standard Error')\nplt.title('Standard Error Comparison')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\nplt.subplot(1, 2, 2)\n# Show differences from analytical\nplt.plot(x_eval, se_bootstrap - se_analytical, 'blue', linewidth=2, \n         label='Parametric - Analytical')\nplt.plot(x_eval, se_resid_boot - se_analytical, 'green', linewidth=2, \n         label='Residual - Analytical')\nplt.axhline(y=0, color='black', linestyle='-', alpha=0.5)\nplt.xlabel('x')\nplt.ylabel('SE Difference')\nplt.title('Differences from Analytical Method')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Compute correlation between methods\ncorr_param = np.corrcoef(se_analytical, se_bootstrap)[0, 1]\ncorr_resid = np.corrcoef(se_analytical, se_resid_boot)[0, 1]\n\nprint(f\"Correlation with analytical method:\")\nprint(f\"  Parametric bootstrap: {corr_param:.4f}\")\nprint(f\"  Residual bootstrap:   {corr_resid:.4f}\")\n</code></pre>"},{"location":"tutorials/uncertainty-methods/#bootstrap-distribution-analysis","title":"Bootstrap Distribution Analysis","text":"<pre><code># Analyze bootstrap distributions at specific points\neval_indices = [25, 50, 100, 150]  # Different x positions\neval_points = x_eval[eval_indices]\n\n# Collect bootstrap samples for these points\nn_boot = 200\nbootstrap_samples = {i: [] for i in eval_indices}\n\nprint(\"Generating bootstrap samples...\")\nfor b in range(n_boot):\n    if b % 50 == 0:\n        print(f\"Bootstrap sample {b}/{n_boot}\")\n\n    # Generate bootstrap sample\n    y_boot = spline.predict(x) + np.sqrt(spline.sigma2) * np.random.randn(len(x))\n\n    # Fit new spline\n    spline_boot = PSpline(x, y_boot, nseg=30, lambda_=optimal_lambda)\n    spline_boot.fit()\n\n    # Predict at evaluation points\n    y_pred_boot = spline_boot.predict(eval_points)\n\n    for i, idx in enumerate(eval_indices):\n        bootstrap_samples[idx].append(y_pred_boot[i])\n\n# Plot bootstrap distributions\nfig, axes = plt.subplots(2, 2, figsize=(12, 10))\naxes = axes.ravel()\n\nfor i, idx in enumerate(eval_indices):\n    samples = np.array(bootstrap_samples[idx])\n\n    # Plot histogram\n    axes[i].hist(samples, bins=30, alpha=0.7, density=True, color='skyblue', \n                 edgecolor='black')\n\n    # Add normal approximation\n    mean_sample = np.mean(samples)\n    std_sample = np.std(samples)\n\n    x_norm = np.linspace(samples.min(), samples.max(), 100)\n    y_norm = (1/np.sqrt(2*np.pi*std_sample**2)) * np.exp(-(x_norm - mean_sample)**2/(2*std_sample**2))\n    axes[i].plot(x_norm, y_norm, 'r-', linewidth=2, label='Normal approx.')\n\n    # Add true value and analytical prediction\n    true_val = true_eval[idx]\n    pred_val = y_pred_analytical[idx]\n\n    axes[i].axvline(true_val, color='green', linestyle='--', linewidth=2, \n                   label=f'True ({true_val:.3f})')\n    axes[i].axvline(pred_val, color='red', linestyle='-', linewidth=2, \n                   label=f'Predicted ({pred_val:.3f})')\n\n    axes[i].set_title(f'Bootstrap Distribution at x={eval_points[i]:.2f}')\n    axes[i].set_xlabel('Predicted Value')\n    axes[i].set_ylabel('Density')\n    axes[i].legend()\n    axes[i].grid(True, alpha=0.3)\n\n    print(f\"Point x={eval_points[i]:.2f}: Bootstrap SE = {std_sample:.4f}, \"\n          f\"Analytical SE = {se_analytical[idx]:.4f}\")\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"tutorials/uncertainty-methods/#method-3-bayesian-inference","title":"Method 3: Bayesian Inference","text":"<p>When PyMC is available, you can perform full Bayesian inference.</p> <pre><code># Check if Bayesian methods are available\ntry:\n    import pymc as pm\n    import arviz as az\n    bayesian_available = True\n    print(\"PyMC available - Bayesian methods enabled\")\nexcept ImportError:\n    bayesian_available = False\n    print(\"PyMC not available - skipping Bayesian methods\")\n    print(\"Install with: pip install pymc arviz\")\n\nif bayesian_available:\n    # Bayesian P-spline fitting\n    print(\"Performing Bayesian inference...\")\n\n    # Set up Bayesian model (this might take a moment)\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        trace = spline.bayes_fit(\n            draws=1000,\n            tune=1000, \n            chains=2,\n            target_accept=0.95,\n            return_inferencedata=True\n        )\n\n    # Get Bayesian predictions\n    y_pred_bayes, se_bayes = spline.predict(\n        x_eval, \n        return_se=True, \n        se_method='bayes',\n        bayes_samples=trace\n    )\n\n    # Create credible intervals\n    # Sample from posterior predictive\n    n_samples = 500\n    y_samples = []\n\n    # Extract posterior samples of coefficients\n    alpha_samples = trace.posterior['alpha'].values.reshape(-1, spline.nb)\n\n    for i in range(min(n_samples, len(alpha_samples))):\n        spline_temp = PSpline(x, y, nseg=30, lambda_=optimal_lambda)\n        spline_temp.alpha = alpha_samples[i]\n        y_sample = spline_temp.predict(x_eval)\n        y_samples.append(y_sample)\n\n    y_samples = np.array(y_samples)\n\n    # Compute credible intervals\n    lower_credible = np.percentile(y_samples, 2.5, axis=0)\n    upper_credible = np.percentile(y_samples, 97.5, axis=0)\n\n    # Plot Bayesian results\n    plt.figure(figsize=(14, 10))\n\n    plt.subplot(2, 1, 1)\n    plt.scatter(x, y, alpha=0.7, s=30, color='gray', label='Data')\n    plt.plot(x_eval, true_eval, 'g--', linewidth=2, label='True function')\n    plt.plot(x_eval, y_pred_bayes, 'purple', linewidth=2, label='Bayesian fit')\n    plt.fill_between(x_eval, lower_credible, upper_credible, alpha=0.3, \n                     color='purple', label='95% Credible Interval')\n    plt.title('Bayesian P-Spline')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n\n    # Compare all uncertainty methods\n    plt.subplot(2, 1, 2)\n    plt.plot(x_eval, se_analytical, 'red', linewidth=2, label='Analytical')\n    plt.plot(x_eval, se_bootstrap, 'blue', linewidth=2, label='Bootstrap')\n    plt.plot(x_eval, se_bayes, 'purple', linewidth=2, label='Bayesian')\n    plt.axhline(y=noise_std, color='black', linestyle='--', alpha=0.7, \n                label=f'True noise std ({noise_std})')\n    plt.xlabel('x')\n    plt.ylabel('Standard Error')\n    plt.title('Uncertainty Method Comparison')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n\n    plt.tight_layout()\n    plt.show()\n\n    # Bayesian model diagnostics\n    print(\"=== Bayesian Diagnostics ===\")\n    print(az.summary(trace, var_names=['tau', 'sigma']))\n\n    # Coverage analysis\n    in_credible = (true_eval &gt;= lower_credible) &amp; (true_eval &lt;= upper_credible)\n    coverage_bayes = np.mean(in_credible)\n\n    print(f\"\\n=== Coverage Comparison ===\")\n    print(f\"Analytical (frequentist CI): {coverage:.3f}\")\n    print(f\"Bootstrap (frequentist CI):  {coverage_boot:.3f}\")\n    print(f\"Bayesian (credible interval): {coverage_bayes:.3f}\")\n    print(f\"Expected coverage:           0.950\")\n\nelse:\n    print(\"Skipping Bayesian analysis - PyMC not available\")\n</code></pre>"},{"location":"tutorials/uncertainty-methods/#practical-guidance-when-to-use-each-method","title":"Practical Guidance: When to Use Each Method","text":""},{"location":"tutorials/uncertainty-methods/#performance-comparison","title":"Performance Comparison","text":"<pre><code>import time\n\n# Time each method\nmethods_to_time = [\n    ('Analytical', lambda: spline.predict(x_eval, return_se=True, se_method='analytic')),\n    ('Bootstrap (100)', lambda: spline.predict(x_eval, return_se=True, se_method='bootstrap', \n                                             B_boot=100, n_jobs=1)),\n    ('Bootstrap (500)', lambda: spline.predict(x_eval, return_se=True, se_method='bootstrap', \n                                             B_boot=500, n_jobs=1))\n]\n\nprint(\"=== Performance Comparison ===\")\nprint(f\"{'Method':&lt;20} {'Time (s)':&lt;10} {'Relative Speed':&lt;15}\")\nprint(\"-\" * 50)\n\ntimes = {}\nfor method_name, method_func in methods_to_time:\n    start_time = time.time()\n    _, _ = method_func()\n    end_time = time.time()\n    elapsed = end_time - start_time\n    times[method_name] = elapsed\n\n# Calculate relative speeds\nanalytical_time = times['Analytical']\nfor method_name, elapsed in times.items():\n    relative_speed = analytical_time / elapsed\n    print(f\"{method_name:&lt;20} {elapsed:&lt;10.4f} {relative_speed:&lt;15.1f}x\")\n\nif bayesian_available:\n    print(f\"{'Bayesian':&lt;20} {'~10-60s':&lt;10} {'~0.01x':&lt;15}\")\n</code></pre>"},{"location":"tutorials/uncertainty-methods/#decision-framework","title":"Decision Framework","text":"<pre><code>def recommend_uncertainty_method(n_points, computation_time='medium', \n                               distribution_assumptions='normal'):\n    \"\"\"\n    Recommend uncertainty quantification method based on context.\n    \"\"\"\n    recommendations = []\n\n    print(f\"=== Uncertainty Method Recommendations ===\")\n    print(f\"Data points: {n_points}\")\n    print(f\"Computation time preference: {computation_time}\")\n    print(f\"Distribution assumptions: {distribution_assumptions}\")\n    print()\n\n    if computation_time == 'fast':\n        recommendations.append(\"\u2713 Use ANALYTICAL method\")\n        recommendations.append(\"  - Fastest option (seconds)\")\n        recommendations.append(\"  - Good approximation for well-behaved data\")\n\n    elif computation_time == 'medium':\n        recommendations.append(\"\u2713 Use BOOTSTRAP method\")\n        recommendations.append(\"  - Good balance of accuracy and speed\")\n        recommendations.append(\"  - More robust than analytical\")\n        if n_points &gt; 100:\n            recommendations.append(\"  - Use parametric bootstrap for efficiency\")\n        else:\n            recommendations.append(\"  - Use residual bootstrap for small samples\")\n\n    else:  # 'slow'\n        if bayesian_available:\n            recommendations.append(\"\u2713 Use BAYESIAN method\")\n            recommendations.append(\"  - Most comprehensive uncertainty quantification\")\n            recommendations.append(\"  - Provides full posterior distribution\")\n            recommendations.append(\"  - Best for critical applications\")\n        else:\n            recommendations.append(\"\u2713 Use BOOTSTRAP method (Bayesian not available)\")\n            recommendations.append(\"  - Use high number of bootstrap samples (1000+)\")\n\n    # Additional considerations\n    print(\"Primary recommendations:\")\n    for rec in recommendations:\n        print(rec)\n\n    print()\n    print(\"Additional considerations:\")\n\n    if distribution_assumptions == 'non-normal':\n        print(\"\u2022 Non-normal errors: Prefer bootstrap or Bayesian methods\")\n\n    if n_points &lt; 50:\n        print(\"\u2022 Small sample: Be cautious with bootstrap methods\")\n        print(\"\u2022 Small sample: Consider Bayesian approach with informative priors\")\n\n    if n_points &gt; 1000:\n        print(\"\u2022 Large sample: Analytical method often sufficient\")\n        print(\"\u2022 Large sample: Use parallel bootstrap if more accuracy needed\")\n\n# Example usage\nrecommend_uncertainty_method(len(x), 'medium', 'normal')\n</code></pre>"},{"location":"tutorials/uncertainty-methods/#advanced-uncertainty-topics","title":"Advanced Uncertainty Topics","text":""},{"location":"tutorials/uncertainty-methods/#simultaneous-confidence-bands","title":"Simultaneous Confidence Bands","text":"<p>For simultaneous confidence over the entire curve:</p> <pre><code># Compute simultaneous confidence bands using bootstrap\n# These are wider than pointwise bands\ndef simultaneous_confidence_bands(spline, x_eval, confidence_level=0.95, n_boot=500):\n    \"\"\"\n    Compute simultaneous confidence bands that control family-wise error rate.\n    \"\"\"\n    n_points = len(x_eval)\n    bootstrap_curves = []\n\n    for b in range(n_boot):\n        # Generate bootstrap sample\n        y_boot = spline.predict(spline.x) + np.sqrt(spline.sigma2) * np.random.randn(spline.n)\n\n        # Fit bootstrap spline\n        spline_boot = PSpline(spline.x, y_boot, nseg=spline.nseg, lambda_=spline.lambda_)\n        spline_boot.fit()\n\n        # Predict\n        y_pred_boot = spline_boot.predict(x_eval)\n        bootstrap_curves.append(y_pred_boot)\n\n    bootstrap_curves = np.array(bootstrap_curves)\n\n    # Compute simultaneous bands using max deviation approach\n    y_pred_mean = np.mean(bootstrap_curves, axis=0)\n    deviations = np.abs(bootstrap_curves - y_pred_mean[None, :])\n    max_deviations = np.max(deviations, axis=1)\n\n    # Find quantile that gives desired coverage\n    alpha = 1 - confidence_level\n    critical_value = np.percentile(max_deviations, 100 * (1 - alpha))\n\n    # Create simultaneous bands\n    lower_sim = y_pred_mean - critical_value\n    upper_sim = y_pred_mean + critical_value\n\n    return y_pred_mean, lower_sim, upper_sim, critical_value\n\n# Compute simultaneous bands\ny_pred_sim, lower_sim, upper_sim, crit_val = simultaneous_confidence_bands(\n    spline, x_eval, confidence_level=0.95, n_boot=200\n)\n\n# Compare pointwise vs simultaneous\nplt.figure(figsize=(14, 8))\nplt.scatter(x, y, alpha=0.7, s=30, color='gray', label='Data')\nplt.plot(x_eval, true_eval, 'g--', linewidth=2, label='True function')\nplt.plot(x_eval, y_pred_analytical, 'r-', linewidth=2, label='P-spline fit')\n\n# Pointwise bands\nplt.fill_between(x_eval, lower_band, upper_band, alpha=0.3, color='blue', \n                 label='95% Pointwise CI')\n\n# Simultaneous bands\nplt.fill_between(x_eval, lower_sim, upper_sim, alpha=0.2, color='red', \n                 label='95% Simultaneous CI')\n\nplt.xlabel('x')\nplt.ylabel('y')\nplt.legend()\nplt.title('Pointwise vs Simultaneous Confidence Bands')\nplt.grid(True, alpha=0.3)\nplt.show()\n\nprint(f\"Critical value for simultaneous bands: {crit_val:.4f}\")\nprint(f\"Average pointwise half-width: {np.mean(1.96 * se_analytical):.4f}\")\nprint(f\"Simultaneous half-width: {crit_val:.4f}\")\nprint(f\"Ratio (simultaneous/pointwise): {crit_val / np.mean(1.96 * se_analytical):.2f}\")\n</code></pre>"},{"location":"tutorials/uncertainty-methods/#prediction-intervals-vs-confidence-intervals","title":"Prediction Intervals vs Confidence Intervals","text":"<pre><code># Prediction intervals include both model uncertainty and noise\ndef prediction_intervals(spline, x_eval, confidence_level=0.95, method='analytical'):\n    \"\"\"\n    Compute prediction intervals that account for both model and noise uncertainty.\n    \"\"\"\n    # Get model uncertainty\n    y_pred, se_model = spline.predict(x_eval, return_se=True, se_method=method)\n\n    # Add noise uncertainty\n    se_total = np.sqrt(se_model**2 + spline.sigma2)\n\n    # Create prediction intervals\n    alpha = 1 - confidence_level\n    z_score = 1.96  # For 95% intervals\n\n    lower_pi = y_pred - z_score * se_total\n    upper_pi = y_pred + z_score * se_total\n\n    return y_pred, lower_pi, upper_pi, se_model, se_total\n\n# Compute prediction intervals\ny_pred_pi, lower_pi, upper_pi, se_model_pi, se_total_pi = prediction_intervals(\n    spline, x_eval, confidence_level=0.95\n)\n\n# Plot comparison\nplt.figure(figsize=(14, 8))\nplt.scatter(x, y, alpha=0.7, s=30, color='gray', label='Data')\nplt.plot(x_eval, true_eval, 'g--', linewidth=2, label='True function')\nplt.plot(x_eval, y_pred_pi, 'r-', linewidth=2, label='P-spline fit')\n\n# Confidence intervals (model uncertainty only)\nplt.fill_between(x_eval, y_pred_pi - 1.96*se_model_pi, y_pred_pi + 1.96*se_model_pi, \n                 alpha=0.4, color='blue', label='95% Confidence Interval')\n\n# Prediction intervals (model + noise uncertainty)\nplt.fill_between(x_eval, lower_pi, upper_pi, alpha=0.2, color='red', \n                 label='95% Prediction Interval')\n\nplt.xlabel('x')\nplt.ylabel('y')\nplt.legend()\nplt.title('Confidence Intervals vs Prediction Intervals')\nplt.grid(True, alpha=0.3)\nplt.show()\n\nprint(\"=== Interval Comparison ===\")\nprint(f\"Average confidence interval width: {np.mean(2 * 1.96 * se_model_pi):.4f}\")\nprint(f\"Average prediction interval width: {np.mean(2 * 1.96 * se_total_pi):.4f}\")\nprint(f\"Model uncertainty contribution: {np.mean(se_model_pi**2):.6f}\")\nprint(f\"Noise uncertainty contribution: {spline.sigma2:.6f}\")\nprint(f\"Total uncertainty: {np.mean(se_total_pi**2):.6f}\")\n</code></pre>"},{"location":"tutorials/uncertainty-methods/#summary","title":"Summary","text":"<p>This tutorial covered three main approaches to uncertainty quantification:</p>"},{"location":"tutorials/uncertainty-methods/#method-summary","title":"Method Summary","text":"Method Speed Accuracy Assumptions Use Case Analytical \u26a1\u26a1\u26a1 Good Normal errors Quick assessment Bootstrap \u26a1\u26a1 Very Good Minimal General use Bayesian \u26a1 Excellent Full model Critical applications"},{"location":"tutorials/uncertainty-methods/#key-takeaways","title":"Key Takeaways","text":"<ol> <li>Analytical methods are fastest and work well for well-behaved data</li> <li>Bootstrap methods provide robust estimates with minimal assumptions</li> <li>Bayesian methods offer the most comprehensive uncertainty quantification</li> <li>Choose based on: computational budget, required accuracy, and application criticality</li> <li>Simultaneous confidence bands are wider than pointwise bands</li> <li>Prediction intervals are wider than confidence intervals (include noise)</li> </ol>"},{"location":"tutorials/uncertainty-methods/#recommendations","title":"Recommendations","text":"<ul> <li>Exploratory analysis: Use analytical standard errors</li> <li>Production applications: Use bootstrap methods</li> <li>Critical decisions: Consider Bayesian approaches</li> <li>Multiple comparisons: Use simultaneous confidence bands</li> <li>Forecasting: Use prediction intervals</li> </ul>"},{"location":"tutorials/uncertainty-methods/#next-steps","title":"Next Steps","text":"<ul> <li>Advanced Features: Constraints and specialized techniques  </li> <li>Parameter Selection: Optimizing smoothing parameters</li> <li>Basic Usage: Review fundamental concepts</li> </ul>"},{"location":"user-guide/core-concepts/","title":"Core Concepts","text":"<p>This guide explains the fundamental concepts behind P-splines, helping you understand how they work and when to use them.</p>"},{"location":"user-guide/core-concepts/#what-are-p-splines","title":"What are P-Splines?","text":"<p>P-splines (Penalized B-splines) are a modern approach to smoothing that combines the flexibility of B-spline basis functions with the automatic smoothing capabilities of penalized regression.</p>"},{"location":"user-guide/core-concepts/#the-big-picture","title":"The Big Picture","text":"<p>Traditional smoothing faces a dilemma: - Interpolation: Fits data exactly but follows noise - Heavy smoothing: Reduces noise but may miss important features</p> <p>P-splines solve this by: 1. Using a flexible B-spline basis 2. Adding a penalty for \"roughness\" 3. Automatically balancing fit vs. smoothness</p>"},{"location":"user-guide/core-concepts/#mathematical-foundation","title":"Mathematical Foundation","text":""},{"location":"user-guide/core-concepts/#the-p-spline-model","title":"The P-Spline Model","text":"<p>Given data points \\((x_i, y_i)\\), P-splines fit a smooth function \\(f(x)\\) by solving:</p> \\[\\min_\\alpha \\|y - B\\alpha\\|^2 + \\lambda \\|D_p \\alpha\\|^2\\] <p>Where: - \\(B\\) is the B-spline basis matrix - \\(\\alpha\\) are the B-spline coefficients - \\(D_p\\) is the \\(p\\)-th order difference matrix - \\(\\lambda\\) is the smoothing parameter</p>"},{"location":"user-guide/core-concepts/#key-components","title":"Key Components","text":""},{"location":"user-guide/core-concepts/#1-b-spline-basis-functions","title":"1. B-Spline Basis Functions","text":"<p>B-splines are piecewise polynomials that provide:</p> <p>Local Support: Each basis function is non-zero only over a small interval <pre><code># Example: basis functions have limited influence\nbasis = BSplineBasis(degree=3, n_segments=10, domain=(0, 1))\n# Each function affects only ~4 segments\n</code></pre></p> <p>Computational Efficiency: Lead to sparse matrices for fast computation</p> <p>Smoothness: Degree \\(d\\) B-splines are \\(C^{d-1}\\) smooth</p>"},{"location":"user-guide/core-concepts/#2-difference-penalties","title":"2. Difference Penalties","text":"<p>Instead of penalizing derivatives directly, P-splines penalize differences of coefficients:</p> <p>First-order differences (\\(p=1\\)):  \\(\\(\\Delta^1 \\alpha_j = \\alpha_{j+1} - \\alpha_j\\)\\) Penalizes large changes between adjacent coefficients.</p> <p>Second-order differences (\\(p=2\\)):  \\(\\(\\Delta^2 \\alpha_j = \\alpha_{j+2} - 2\\alpha_{j+1} + \\alpha_j\\)\\) Penalizes changes in the slope (curvature).</p> <p>Higher orders: Control higher-order smoothness properties.</p>"},{"location":"user-guide/core-concepts/#3-the-smoothing-parameter","title":"3. The Smoothing Parameter \u03bb","text":"<p>Controls the bias-variance trade-off:</p> <ul> <li>\u03bb = 0: Interpolation (high variance, low bias)  </li> <li>\u03bb \u2192 \u221e: Maximum smoothness (low variance, high bias)</li> <li>Optimal \u03bb: Minimizes expected prediction error</li> </ul>"},{"location":"user-guide/core-concepts/#practical-understanding","title":"Practical Understanding","text":""},{"location":"user-guide/core-concepts/#how-p-splines-adapt","title":"How P-Splines Adapt","text":"<p>P-splines automatically adapt to local data characteristics:</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom psplines import PSpline\n\n# Create data with varying complexity\nx = np.linspace(0, 10, 100)\ny_smooth = np.sin(x)                    # Smooth region\ny_complex = 5 * np.sin(10*x) * (x &gt; 7)  # Complex region  \ny = y_smooth + y_complex + 0.1 * np.random.randn(100)\n\nspline = PSpline(x, y, nseg=30)\n# P-spline will automatically be smoother in smooth regions\n# and more flexible in complex regions\n</code></pre>"},{"location":"user-guide/core-concepts/#effective-degrees-of-freedom","title":"Effective Degrees of Freedom","text":"<p>A key concept is effective degrees of freedom (EdF):</p> <pre><code>spline.fit()\nprint(f\"Effective DoF: {spline.ED}\")\nprint(f\"Maximum DoF (interpolation): {len(y)}\")\nprint(f\"Minimum DoF (constant): 1\")\n</code></pre> <p>EdF represents the \"complexity\" of the fitted model: - Low EdF: Simple, smooth fits - High EdF: Complex, flexible fits - Optimal EdF: Best predictive performance</p>"},{"location":"user-guide/core-concepts/#residual-analysis","title":"Residual Analysis","text":"<p>Understanding residuals helps assess fit quality:</p> <pre><code># After fitting\ny_pred = spline.predict(x)\nresiduals = y - y_pred\n\n# Good fit characteristics:\n# - Residuals randomly scattered around zero\n# - No systematic patterns\n# - Approximately constant variance\n\nplt.scatter(y_pred, residuals)\nplt.axhline(y=0, color='red', linestyle='--')\nplt.xlabel('Fitted values')\nplt.ylabel('Residuals')\nplt.title('Residual Analysis')\n</code></pre>"},{"location":"user-guide/core-concepts/#parameter-selection-deep-dive","title":"Parameter Selection Deep Dive","text":""},{"location":"user-guide/core-concepts/#the-smoothing-parameter","title":"The Smoothing Parameter \u03bb","text":"<p>The most critical parameter requiring automatic selection:</p>"},{"location":"user-guide/core-concepts/#cross-validation-approach","title":"Cross-Validation Approach","text":"<p>Estimates out-of-sample prediction error:</p> <pre><code>from psplines.optimize import cross_validation\n\n# k-fold cross-validation\noptimal_lambda, cv_score = cross_validation(\n    spline, \n    cv_method='kfold', \n    k_folds=5\n)\n</code></pre> <p>Pros: Robust, directly optimizes prediction Cons: Computationally expensive</p>"},{"location":"user-guide/core-concepts/#aic-approach","title":"AIC Approach","text":"<p>Balances fit quality and model complexity:</p> <pre><code>from psplines.optimize import aic_selection\n\noptimal_lambda, aic_score = aic_selection(spline)\n</code></pre> <p>Pros: Fast, good approximation Cons: Assumes Gaussian errors</p>"},{"location":"user-guide/core-concepts/#l-curve-method","title":"L-Curve Method","text":"<p>Finds the \"corner\" in the trade-off curve:</p> <pre><code>from psplines.optimize import l_curve\n\noptimal_lambda, curvature_info = l_curve(spline)\n</code></pre> <p>Pros: Intuitive geometric interpretation Cons: Not always reliable</p>"},{"location":"user-guide/core-concepts/#number-of-segments-nseg","title":"Number of Segments (nseg)","text":"<p>Controls the flexibility of the basis:</p>"},{"location":"user-guide/core-concepts/#rules-of-thumb","title":"Rules of Thumb","text":"<ul> <li>Start with: <code>nseg = n_data_points / 4</code></li> <li>Minimum: 5-10 (depends on data complexity)</li> <li>Maximum: Usually no more than <code>n_data_points / 2</code></li> </ul>"},{"location":"user-guide/core-concepts/#adaptive-selection","title":"Adaptive Selection","text":"<pre><code># Try different nseg values\nnseg_values = [10, 20, 30, 40]\nbest_score = float('inf')\nbest_nseg = None\n\nfor nseg in nseg_values:\n    spline_test = PSpline(x, y, nseg=nseg)\n    _, cv_score = cross_validation(spline_test)\n\n    if cv_score &lt; best_score:\n        best_score = cv_score\n        best_nseg = nseg\n\nprint(f\"Optimal nseg: {best_nseg}\")\n</code></pre>"},{"location":"user-guide/core-concepts/#penalty-order","title":"Penalty Order","text":"<p>Controls the type of smoothness:</p>"},{"location":"user-guide/core-concepts/#order-1-penalizes-slope-changes","title":"Order 1: Penalizes Slope Changes","text":"<pre><code>spline = PSpline(x, y, penalty_order=1)\n# Results in piecewise-linear-like smoothness\n# Good for: Step functions, trend analysis\n</code></pre>"},{"location":"user-guide/core-concepts/#order-2-penalizes-curvature-changes-default","title":"Order 2: Penalizes Curvature Changes (Default)","text":"<pre><code>spline = PSpline(x, y, penalty_order=2)\n# Results in smooth curves\n# Good for: Most applications, natural-looking curves\n</code></pre>"},{"location":"user-guide/core-concepts/#order-3-penalizes-higher-order-changes","title":"Order 3+: Penalizes Higher-Order Changes","text":"<pre><code>spline = PSpline(x, y, penalty_order=3)\n# Results in very smooth curves\n# Good for: Very smooth phenomena, artistic curves\n</code></pre>"},{"location":"user-guide/core-concepts/#uncertainty-quantification","title":"Uncertainty Quantification","text":""},{"location":"user-guide/core-concepts/#standard-errors","title":"Standard Errors","text":"<p>P-splines provide uncertainty estimates through:</p>"},{"location":"user-guide/core-concepts/#analytical-standard-errors","title":"Analytical Standard Errors","text":"<p>Based on the covariance matrix of coefficients:</p> <pre><code>y_pred, se = spline.predict(x_new, return_se=True, se_method='analytic')\n\n# Create confidence intervals\nlower_ci = y_pred - 1.96 * se\nupper_ci = y_pred + 1.96 * se\n</code></pre> <p>Assumptions: Gaussian errors, correct model specification</p>"},{"location":"user-guide/core-concepts/#bootstrap-standard-errors","title":"Bootstrap Standard Errors","text":"<p>Empirical estimation through resampling:</p> <pre><code>y_pred, se = spline.predict(x_new, return_se=True, \n                           se_method='bootstrap', B_boot=500)\n</code></pre> <p>Advantages: Fewer assumptions, empirical distribution</p>"},{"location":"user-guide/core-concepts/#bayesian-inference","title":"Bayesian Inference","text":"<p>Full posterior distribution (requires PyMC):</p> <pre><code># If PyMC is installed\ntrace = spline.bayes_fit(draws=1000, tune=1000)\ny_pred, se = spline.predict(x_new, return_se=True, \n                           se_method='bayes', bayes_samples=trace)\n</code></pre> <p>Advantages: Full uncertainty quantification, principled approach</p>"},{"location":"user-guide/core-concepts/#confidence-vs-prediction-intervals","title":"Confidence vs. Prediction Intervals","text":"<p>Confidence Intervals: Uncertainty in the mean function <pre><code># Standard error of the fitted curve\ny_pred, se_fit = spline.predict(x_new, return_se=True)\nci_lower = y_pred - 1.96 * se_fit\nci_upper = y_pred + 1.96 * se_fit\n</code></pre></p> <p>Prediction Intervals: Uncertainty for new observations <pre><code># Include both fit uncertainty and noise\nse_pred = np.sqrt(se_fit**2 + spline.sigma2)  # Add noise variance\npi_lower = y_pred - 1.96 * se_pred  \npi_upper = y_pred + 1.96 * se_pred\n</code></pre></p>"},{"location":"user-guide/core-concepts/#advanced-concepts","title":"Advanced Concepts","text":""},{"location":"user-guide/core-concepts/#sparse-matrix-structure","title":"Sparse Matrix Structure","text":"<p>P-splines exploit sparsity for efficiency:</p> <pre><code># B-spline basis matrix is sparse\nprint(f\"Basis matrix shape: {spline.basis_matrix.shape}\")\nprint(f\"Basis matrix density: {spline.basis_matrix.nnz / spline.basis_matrix.size:.3f}\")\n\n# Penalty matrix is also sparse and banded\nP = spline.penalty_matrix\nprint(f\"Penalty matrix bandwidth: ~{2*spline.penalty_order + 1}\")\n</code></pre>"},{"location":"user-guide/core-concepts/#computational-complexity","title":"Computational Complexity","text":"<p>Understanding performance characteristics:</p> <ul> <li>Matrix assembly: \\(O(n \\cdot m)\\) where \\(n\\) = data points, \\(m\\) = basis functions</li> <li>System solution: \\(O(m^3)\\) dense, \\(O(m^{1.5})\\) sparse (typical)  </li> <li>Prediction: \\(O(k \\cdot m)\\) where \\(k\\) = prediction points</li> </ul>"},{"location":"user-guide/core-concepts/#knot-placement","title":"Knot Placement","text":"<p>While P-splines typically use uniform knots, understanding knot placement helps:</p> <pre><code># Examine knot structure\nbasis = spline.basis\nprint(f\"Interior knots: {basis.knots[basis.degree:-basis.degree]}\")\nprint(f\"Boundary knots: {basis.knots[:basis.degree]} and {basis.knots[-basis.degree:]}\")\n</code></pre> <p>Uniform knots: Work well for most applications Adaptive knots: May improve efficiency for irregular data (advanced topic)</p>"},{"location":"user-guide/core-concepts/#when-to-use-p-splines","title":"When to Use P-Splines","text":""},{"location":"user-guide/core-concepts/#ideal-scenarios","title":"Ideal Scenarios","text":"<p>\u2705 Noisy measurements requiring smoothing \u2705 Derivative estimation from data \u2705 Trend extraction from time series \u2705 Interpolation with uncertainty quantification \u2705 Large datasets (sparse matrix efficiency) \u2705 Automatic smoothing without manual parameter tuning</p>"},{"location":"user-guide/core-concepts/#limitations","title":"Limitations","text":"<p>\u274c Highly oscillatory functions (consider wavelets) \u274c Very sparse data (&lt; 10 points per feature) \u274c Categorical predictors (use GAMs or other methods) \u274c Multidimensional smoothing (use tensor products or alternatives)</p>"},{"location":"user-guide/core-concepts/#alternatives-comparison","title":"Alternatives Comparison","text":"Method Flexibility Speed Automation Uncertainty P-splines High Fast Excellent Yes Kernel smoothing Medium Medium Good Limited Gaussian processes High Slow Good Excellent Savitzky-Golay Low Very fast Poor No"},{"location":"user-guide/core-concepts/#summary","title":"Summary","text":"<p>P-splines provide an optimal balance of:</p> <ul> <li>Flexibility: Through B-spline basis functions</li> <li>Smoothness: Through difference penalties  </li> <li>Automation: Through data-driven parameter selection</li> <li>Efficiency: Through sparse matrix computations</li> <li>Uncertainty: Through multiple quantification methods</li> </ul> <p>The key insight is that smoothness can be achieved by penalizing differences of basis coefficients rather than derivatives of the function itself, leading to computationally efficient and statistically principled smoothing.</p>"},{"location":"user-guide/core-concepts/#next-steps","title":"Next Steps","text":"<ul> <li>Quick Start: Get started immediately</li> <li>Basic Usage Tutorial: Hands-on examples</li> <li>Mathematical Background: Detailed theory</li> <li>Examples Gallery: Real-world applications</li> </ul>"},{"location":"user-guide/getting-started/","title":"Getting Started","text":"<p>Welcome to PSplines! This guide will help you get up and running with penalized B-spline smoothing in Python.</p>"},{"location":"user-guide/getting-started/#what-are-p-splines","title":"What are P-Splines?","text":"<p>P-splines (Penalized B-splines) are a powerful and flexible method for smoothing noisy data. They combine:</p> <ul> <li>B-spline basis functions: Provide local flexibility and computational efficiency</li> <li>Difference penalties: Control smoothness by penalizing rough behavior</li> <li>Automatic parameter selection: Data-driven choice of smoothing level</li> </ul> <p>P-splines are particularly useful when you have:</p> <ul> <li>Noisy measurements that need smoothing</li> <li>Need for derivative estimation</li> <li>Desire for uncertainty quantification</li> <li>Large datasets requiring efficient computation</li> </ul>"},{"location":"user-guide/getting-started/#key-concepts","title":"Key Concepts","text":""},{"location":"user-guide/getting-started/#b-spline-basis","title":"B-Spline Basis","text":"<p>B-splines are piecewise polynomials defined over a set of knots. They provide: - Local support: Changes in one region don't affect distant regions - Computational efficiency: Sparse matrices and fast algorithms - Flexibility: Can approximate complex curves with appropriate parameters</p>"},{"location":"user-guide/getting-started/#difference-penalties","title":"Difference Penalties","text":"<p>Instead of penalizing derivatives directly, P-splines penalize differences of adjacent B-spline coefficients: - First-order differences: Penalize large changes in coefficients (roughness of slopes) - Second-order differences: Penalize large changes in slopes (roughness of curvature) - Higher-order differences: Penalize higher-order roughness measures</p>"},{"location":"user-guide/getting-started/#the-p-spline-objective","title":"The P-Spline Objective","text":"<p>P-splines minimize the penalized least squares objective:</p> \\[\\min_\\alpha \\|y - B\\alpha\\|^2 + \\lambda \\|D_p \\alpha\\|^2\\] <p>where: - \\(y\\) is the data vector - \\(B\\) is the B-spline basis matrix - \\(\\alpha\\) are the B-spline coefficients - \\(D_p\\) is the \\(p\\)-th order difference matrix - \\(\\lambda\\) is the smoothing parameter</p>"},{"location":"user-guide/getting-started/#your-first-p-spline","title":"Your First P-Spline","text":"<p>Let's start with a simple example:</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom psplines import PSpline\n\n# Generate some noisy data\nnp.random.seed(42)\nx = np.linspace(0, 2*np.pi, 50)\ny = np.sin(x) + 0.1 * np.random.randn(50)\n\n# Create a P-spline\nspline = PSpline(x, y, nseg=15, lambda_=1.0)\n\n# Fit the spline\nspline.fit()\n\n# Make predictions\nx_new = np.linspace(0, 2*np.pi, 200)\ny_smooth = spline.predict(x_new)\n\n# Plot the results\nplt.figure(figsize=(10, 6))\nplt.scatter(x, y, alpha=0.6, label='Noisy data')\nplt.plot(x_new, y_smooth, 'r-', linewidth=2, label='P-spline fit')\nplt.plot(x_new, np.sin(x_new), 'g--', alpha=0.7, label='True function')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.legend()\nplt.title('Your First P-Spline')\nplt.show()\n</code></pre>"},{"location":"user-guide/getting-started/#understanding-the-parameters","title":"Understanding the Parameters","text":""},{"location":"user-guide/getting-started/#number-of-segments-nseg","title":"Number of Segments (<code>nseg</code>)","text":"<p>Controls the flexibility of the basis: - Fewer segments: More constrained, smoother fits - More segments: More flexible, can capture fine details - Typical range: 10-50 for most applications</p> <pre><code># Compare different numbers of segments\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\nfor i, nseg in enumerate([5, 15, 30]):\n    spline = PSpline(x, y, nseg=nseg, lambda_=1.0)\n    spline.fit()\n    y_fit = spline.predict(x_new)\n\n    axes[i].scatter(x, y, alpha=0.6)\n    axes[i].plot(x_new, y_fit, 'r-', linewidth=2)\n    axes[i].set_title(f'nseg = {nseg}')\n    axes[i].set_xlabel('x')\n    if i == 0:\n        axes[i].set_ylabel('y')\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"user-guide/getting-started/#smoothing-parameter-lambda_","title":"Smoothing Parameter (<code>lambda_</code>)","text":"<p>Controls the trade-off between fit and smoothness: - Small \u03bb: Less smoothing, closer to data - Large \u03bb: More smoothing, smoother curves - Optimal \u03bb: Can be selected automatically (see tutorials)</p> <pre><code># Compare different smoothing parameters\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\nfor i, lam in enumerate([0.01, 1.0, 100.0]):\n    spline = PSpline(x, y, nseg=15, lambda_=lam)\n    spline.fit()\n    y_fit = spline.predict(x_new)\n\n    axes[i].scatter(x, y, alpha=0.6)\n    axes[i].plot(x_new, y_fit, 'r-', linewidth=2)\n    axes[i].set_title(f'\u03bb = {lam}')\n    axes[i].set_xlabel('x')\n    if i == 0:\n        axes[i].set_ylabel('y')\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"user-guide/getting-started/#penalty-order-penalty_order","title":"Penalty Order (<code>penalty_order</code>)","text":"<p>Controls what type of smoothness is enforced: - Order 1: Penalizes large first differences (rough slopes) - Order 2: Penalizes large second differences (rough curvature) - most common - Order 3: Penalizes large third differences (rough jerk)</p>"},{"location":"user-guide/getting-started/#common-workflows","title":"Common Workflows","text":""},{"location":"user-guide/getting-started/#1-basic-smoothing","title":"1. Basic Smoothing","text":"<pre><code># Load your data\n# x, y = load_your_data()\n\n# Create and fit spline\nspline = PSpline(x, y, nseg=20)\nspline.fit()\n\n# Get smooth fit\nx_smooth = np.linspace(x.min(), x.max(), 200)\ny_smooth = spline.predict(x_smooth)\n</code></pre>"},{"location":"user-guide/getting-started/#2-automatic-parameter-selection","title":"2. Automatic Parameter Selection","text":"<pre><code>from psplines.optimize import cross_validation\n\n# Create spline\nspline = PSpline(x, y, nseg=20)\n\n# Find optimal smoothing parameter\noptimal_lambda, cv_score = cross_validation(spline)\nspline.lambda_ = optimal_lambda\nspline.fit()\n</code></pre>"},{"location":"user-guide/getting-started/#3-uncertainty-quantification","title":"3. Uncertainty Quantification","text":"<pre><code># Fit spline\nspline = PSpline(x, y, nseg=20, lambda_=1.0)\nspline.fit()\n\n# Get predictions with uncertainty\ny_pred, se = spline.predict(x_smooth, return_se=True)\n\n# Plot with confidence bands\nplt.fill_between(x_smooth, y_pred - 1.96*se, y_pred + 1.96*se, \n                 alpha=0.3, label='95% CI')\nplt.plot(x_smooth, y_pred, 'r-', label='Fit')\nplt.scatter(x, y, alpha=0.6, label='Data')\nplt.legend()\n</code></pre>"},{"location":"user-guide/getting-started/#4-derivative-estimation","title":"4. Derivative Estimation","text":"<pre><code># Fit spline\nspline = PSpline(x, y, nseg=20, lambda_=1.0)\nspline.fit()\n\n# Compute derivatives\ndy_dx = spline.derivative(x_smooth, deriv_order=1)\nd2y_dx2 = spline.derivative(x_smooth, deriv_order=2)\n\n# Plot derivatives\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\naxes[0].scatter(x, y, alpha=0.6)\naxes[0].plot(x_smooth, spline.predict(x_smooth), 'r-')\naxes[0].set_title('Function')\n\naxes[1].plot(x_smooth, dy_dx, 'g-')\naxes[1].set_title('First Derivative')\n\naxes[2].plot(x_smooth, d2y_dx2, 'm-')\naxes[2].set_title('Second Derivative')\n\nplt.tight_layout()\n</code></pre>"},{"location":"user-guide/getting-started/#next-steps","title":"Next Steps","text":"<p>Now that you understand the basics, explore:</p> <ol> <li>Installation Guide: Detailed installation instructions</li> <li>Quick Start: More examples and use cases</li> <li>Core Concepts: Deeper mathematical understanding</li> <li>Tutorials: Step-by-step walkthroughs</li> <li>Examples: Real-world applications</li> </ol>"},{"location":"user-guide/getting-started/#common-questions","title":"Common Questions","text":""},{"location":"user-guide/getting-started/#q-how-do-i-choose-the-number-of-segments","title":"Q: How do I choose the number of segments?","text":"<p>A: Start with <code>nseg = n/4</code> where <code>n</code> is your data size, then adjust based on your needs. More segments = more flexibility but potentially more overfitting.</p>"},{"location":"user-guide/getting-started/#q-how-do-i-choose-the-smoothing-parameter","title":"Q: How do I choose the smoothing parameter?","text":"<p>A: Use automatic selection methods like <code>cross_validation()</code> or <code>aic()</code>. These provide data-driven choices for the optimal smoothing level.</p>"},{"location":"user-guide/getting-started/#q-when-should-i-use-different-penalty-orders","title":"Q: When should I use different penalty orders?","text":"<p>A:  - Order 2 (default) works well for most applications - Order 1 for piecewise linear trends - Order 3 for very smooth curves</p>"},{"location":"user-guide/getting-started/#q-how-do-i-handle-large-datasets","title":"Q: How do I handle large datasets?","text":"<p>A: P-splines are naturally efficient with sparse matrices. For very large datasets, consider: - Using fewer segments initially - Batching the data if memory is limited - Using the bootstrap uncertainty only when needed</p>"},{"location":"user-guide/getting-started/#q-can-i-constrain-the-fit-at-boundaries","title":"Q: Can I constrain the fit at boundaries?","text":"<p>A: Yes! Use the <code>constraints</code> parameter to specify derivative constraints at boundaries. See the Advanced Features tutorial.</p>"},{"location":"user-guide/installation/","title":"Installation","text":"<p>This guide covers the various ways to install PSplines and its dependencies.</p>"},{"location":"user-guide/installation/#requirements","title":"Requirements","text":"<p>PSplines requires Python 3.10 or later and depends on several scientific computing packages:</p>"},{"location":"user-guide/installation/#core-dependencies","title":"Core Dependencies","text":"<ul> <li>NumPy (\u2265 1.21): Array operations and linear algebra</li> <li>SciPy (\u2265 1.7): Sparse matrices, optimization, and spline functions</li> <li>Matplotlib (\u2265 3.4): Basic plotting functionality</li> </ul>"},{"location":"user-guide/installation/#optional-dependencies","title":"Optional Dependencies","text":"<p>For full functionality, you may want:</p> <ul> <li>PyMC (\u2265 5.0): Bayesian inference capabilities</li> <li>PyTensor (\u2265 2.0): Backend for PyMC</li> <li>ArviZ (\u2265 0.12): Bayesian diagnostics and visualization</li> <li>Joblib (\u2265 1.0): Parallel processing for bootstrap methods</li> </ul>"},{"location":"user-guide/installation/#installation-methods","title":"Installation Methods","text":""},{"location":"user-guide/installation/#method-1-using-pip-recommended","title":"Method 1: Using pip (Recommended)","text":"<p>The easiest way to install PSplines is using pip:</p> <pre><code>pip install psplines\n</code></pre> <p>This installs PSplines and all required dependencies.</p>"},{"location":"user-guide/installation/#install-with-optional-dependencies","title":"Install with Optional Dependencies","text":"<p>To install with all optional dependencies for full functionality:</p> <pre><code>pip install psplines[full]\n</code></pre> <p>Or install specific optional dependencies:</p> <pre><code>pip install psplines[bayesian]  # For Bayesian features\npip install psplines[parallel]  # For parallel processing\n</code></pre>"},{"location":"user-guide/installation/#method-2-using-condamamba","title":"Method 2: Using conda/mamba","text":"<p>If you're using conda or mamba:</p> <pre><code>conda install -c conda-forge psplines\n</code></pre> <p>Or with mamba:</p> <pre><code>mamba install -c conda-forge psplines\n</code></pre>"},{"location":"user-guide/installation/#method-3-development-installation","title":"Method 3: Development Installation","text":"<p>For development or to get the latest features:</p> <pre><code>git clone https://github.com/graysonbellamy/psplines.git\ncd psplines\npip install -e .\n</code></pre> <p>For development with all tools:</p> <pre><code>git clone https://github.com/graysonbellamy/psplines.git\ncd psplines\npip install -e .[dev]\n</code></pre>"},{"location":"user-guide/installation/#method-4-using-uv-fast-and-modern","title":"Method 4: Using uv (Fast and Modern)","text":"<p>If you have uv installed:</p> <pre><code>uv add psplines\n</code></pre> <p>For development:</p> <pre><code>git clone https://github.com/graysonbellamy/psplines.git\ncd psplines\nuv sync --dev\n</code></pre>"},{"location":"user-guide/installation/#verifying-your-installation","title":"Verifying Your Installation","text":"<p>After installation, verify that PSplines works correctly:</p> <pre><code>import psplines\nprint(f\"PSplines version: {psplines.__version__}\")\n\n# Run a simple test\nimport numpy as np\nfrom psplines import PSpline\n\n# Generate test data\nnp.random.seed(42)\nx = np.linspace(0, 1, 20)\ny = np.sin(2 * np.pi * x) + 0.1 * np.random.randn(20)\n\n# Create and fit spline\nspline = PSpline(x, y, nseg=10)\nspline.fit()\n\nprint(\"\u2713 PSplines installed successfully!\")\nprint(f\"  - Effective DoF: {spline.ED:.2f}\")\nprint(f\"  - Residual variance: {spline.sigma2:.4f}\")\n</code></pre>"},{"location":"user-guide/installation/#testing-optional-features","title":"Testing Optional Features","text":""},{"location":"user-guide/installation/#bayesian-functionality","title":"Bayesian Functionality","text":"<pre><code>try:\n    import pymc\n    import arviz\n    print(\"\u2713 Bayesian features available\")\n\n    # Test Bayesian fitting (this may take a moment)\n    trace = spline.bayes_fit(draws=100, tune=100, chains=1)\n    print(\"\u2713 Bayesian inference working\")\nexcept ImportError as e:\n    print(f\"\u2717 Bayesian features not available: {e}\")\n</code></pre>"},{"location":"user-guide/installation/#parallel-processing","title":"Parallel Processing","text":"<pre><code>try:\n    from joblib import Parallel, delayed\n    print(\"\u2713 Parallel processing available\")\n\n    # Test bootstrap with parallel processing\n    x_new = np.linspace(0, 1, 10)\n    y_pred, se = spline.predict(x_new, return_se=True, \n                               se_method=\"bootstrap\", B_boot=100, n_jobs=2)\n    print(\"\u2713 Parallel bootstrap working\")\nexcept ImportError as e:\n    print(f\"\u2717 Parallel processing not available: {e}\")\n</code></pre>"},{"location":"user-guide/installation/#environment-specific-instructions","title":"Environment-Specific Instructions","text":""},{"location":"user-guide/installation/#jupyter-notebooks","title":"Jupyter Notebooks","text":"<p>PSplines works great in Jupyter notebooks. For the best experience:</p> <pre><code>pip install psplines matplotlib ipywidgets\njupyter notebook\n</code></pre>"},{"location":"user-guide/installation/#google-colab","title":"Google Colab","text":"<p>In Google Colab, simply run:</p> <pre><code>!pip install psplines\nimport psplines\n</code></pre>"},{"location":"user-guide/installation/#docker","title":"Docker","text":"<p>Create a Dockerfile for containerized usage:</p> <pre><code>FROM python:3.11-slim\n\n# Install system dependencies\nRUN apt-update &amp;&amp; apt-get install -y gcc g++ &amp;&amp; rm -rf /var/lib/apt/lists/*\n\n# Install Python packages\nRUN pip install psplines matplotlib jupyter\n\n# Set working directory\nWORKDIR /workspace\n\n# Start Jupyter by default\nCMD [\"jupyter\", \"notebook\", \"--ip=0.0.0.0\", \"--allow-root\", \"--no-browser\"]\n</code></pre>"},{"location":"user-guide/installation/#virtual-environments","title":"Virtual Environments","text":""},{"location":"user-guide/installation/#using-venv","title":"Using venv","text":"<pre><code>python -m venv psplines_env\nsource psplines_env/bin/activate  # On Windows: psplines_env\\Scripts\\activate\npip install psplines\n</code></pre>"},{"location":"user-guide/installation/#using-conda","title":"Using conda","text":"<pre><code>conda create -n psplines python=3.11\nconda activate psplines\nconda install -c conda-forge psplines\n</code></pre>"},{"location":"user-guide/installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user-guide/installation/#common-issues","title":"Common Issues","text":""},{"location":"user-guide/installation/#issue-no-module-named-psplines","title":"Issue: \"No module named 'psplines'\"","text":"<p>Solution: Ensure you've activated the correct environment and PSplines is installed:</p> <pre><code>pip list | grep psplines\npython -c \"import psplines; print('OK')\"\n</code></pre>"},{"location":"user-guide/installation/#issue-import-errors-with-scipynumpy","title":"Issue: Import errors with SciPy/NumPy","text":"<p>Solution: Update to compatible versions:</p> <pre><code>pip install --upgrade numpy scipy\n</code></pre>"},{"location":"user-guide/installation/#issue-bayesian-features-not-working","title":"Issue: Bayesian features not working","text":"<p>Solution: Install PyMC and its dependencies:</p> <pre><code>pip install pymc arviz pytensor\n</code></pre>"},{"location":"user-guide/installation/#issue-slow-performance","title":"Issue: Slow performance","text":"<p>Solution: Ensure you have optimized BLAS libraries:</p> <pre><code># Check current BLAS configuration\npython -c \"import numpy; numpy.show_config()\"\n\n# Install optimized BLAS (choose one)\nconda install mkl  # Intel MKL\nconda install openblas  # OpenBLAS\n</code></pre>"},{"location":"user-guide/installation/#issue-memory-errors-with-large-datasets","title":"Issue: Memory errors with large datasets","text":"<p>Solutions: 1. Use fewer segments: <code>nseg=min(n//10, 50)</code> 2. Use single precision: Convert arrays to <code>float32</code> 3. Process data in chunks if memory is very limited</p>"},{"location":"user-guide/installation/#platform-specific-notes","title":"Platform-Specific Notes","text":""},{"location":"user-guide/installation/#windows","title":"Windows","text":"<ul> <li>Install Microsoft C++ Build Tools if compilation fails</li> <li>Use Anaconda for easier dependency management</li> </ul>"},{"location":"user-guide/installation/#macos","title":"macOS","text":"<ul> <li>On Apple Silicon (M1/M2), use conda-forge for best compatibility:   <pre><code>conda install -c conda-forge psplines\n</code></pre></li> </ul>"},{"location":"user-guide/installation/#linux","title":"Linux","text":"<ul> <li>Install development headers if building from source:   <pre><code>sudo apt-get install python3-dev libopenblas-dev  # Ubuntu/Debian\nsudo yum install python3-devel openblas-devel     # CentOS/RHEL\n</code></pre></li> </ul>"},{"location":"user-guide/installation/#performance-optimization","title":"Performance Optimization","text":""},{"location":"user-guide/installation/#for-maximum-performance","title":"For maximum performance:","text":"<ol> <li>Use optimized BLAS: Install MKL or OpenBLAS</li> <li>Compile from source: May give slight performance improvements</li> <li>Use appropriate data types: <code>float64</code> for precision, <code>float32</code> for speed</li> <li>Enable parallel processing: Set <code>n_jobs=-1</code> for bootstrap methods</li> </ol>"},{"location":"user-guide/installation/#check-your-numpy-configuration","title":"Check your NumPy configuration:","text":"<pre><code>import numpy as np\nnp.show_config()  # Shows BLAS/LAPACK configuration\n</code></pre>"},{"location":"user-guide/installation/#getting-help","title":"Getting Help","text":"<p>If you encounter issues not covered here:</p> <ol> <li>Check the FAQ: See Common Questions</li> <li>Search GitHub Issues: Issues Page</li> <li>Ask for help: Open a new issue with:</li> <li>Your operating system and Python version</li> <li>Complete error messages</li> <li>Minimal example that reproduces the problem</li> </ol>"},{"location":"user-guide/installation/#next-steps","title":"Next Steps","text":"<p>After successful installation:</p> <ol> <li>Getting Started: Learn the basics</li> <li>Quick Start: Jump into examples</li> <li>Tutorials: Comprehensive guides</li> <li>Examples: Real-world applications</li> </ol>"},{"location":"user-guide/quick-start/","title":"Quick Start","text":"<p>Get up and running with PSplines in minutes! This guide provides the essentials to start smoothing your data.</p>"},{"location":"user-guide/quick-start/#installation","title":"Installation","text":"<p>Install PSplines using pip:</p> <pre><code>pip install psplines\n</code></pre> <p>For all features including Bayesian inference: <pre><code>pip install psplines[full]\n</code></pre></p>"},{"location":"user-guide/quick-start/#5-minute-example","title":"5-Minute Example","text":"<p>Here's a complete example that demonstrates the core PSplines workflow:</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom psplines import PSpline\nfrom psplines.optimize import cross_validation\n\n# 1. Generate some noisy data\nnp.random.seed(42)\nx = np.linspace(0, 2*np.pi, 50)\ny = np.sin(x) + 0.1 * np.random.randn(50)\n\n# 2. Create a P-spline\nspline = PSpline(x, y, nseg=20)\n\n# 3. Find optimal smoothing parameter\noptimal_lambda, _ = cross_validation(spline)\nspline.lambda_ = optimal_lambda\n\n# 4. Fit the spline\nspline.fit()\n\n# 5. Make predictions\nx_new = np.linspace(0, 2*np.pi, 200)\ny_pred = spline.predict(x_new)\n\n# 6. Get uncertainty estimates\ny_pred_with_se, se = spline.predict(x_new, return_se=True)\n\n# 7. Plot results\nplt.figure(figsize=(10, 6))\nplt.scatter(x, y, alpha=0.6, label='Noisy data')\nplt.plot(x_new, np.sin(x_new), 'g--', label='True function')\nplt.plot(x_new, y_pred, 'r-', linewidth=2, label='P-spline fit')\nplt.fill_between(x_new, y_pred_with_se - 1.96*se, y_pred_with_se + 1.96*se, \n                 alpha=0.3, color='red', label='95% Confidence')\nplt.legend()\nplt.title('P-Spline Smoothing')\nplt.show()\n\n# 8. Check model properties\nprint(f\"Effective degrees of freedom: {spline.ED:.2f}\")\nprint(f\"Optimal lambda: {optimal_lambda:.6f}\")\nprint(f\"Residual variance: {spline.sigma2:.6f}\")\n</code></pre> <p>That's it! You've successfully fitted a P-spline, optimized parameters, and quantified uncertainty.</p>"},{"location":"user-guide/quick-start/#key-concepts-in-60-seconds","title":"Key Concepts in 60 Seconds","text":""},{"location":"user-guide/quick-start/#p-splines-b-splines-penalties","title":"P-Splines = B-splines + Penalties","text":"<p>P-splines combine two powerful ideas:</p> <ol> <li>B-spline basis: Flexible piecewise polynomials</li> <li>Difference penalties: Smooth by penalizing roughness</li> </ol> <p>The result: automatic smoothing that adapts to your data.</p>"},{"location":"user-guide/quick-start/#three-essential-parameters","title":"Three Essential Parameters","text":"<ul> <li><code>nseg</code>: Number of segments (more = more flexible)</li> <li><code>lambda_</code>: Smoothing parameter (higher = smoother)</li> <li><code>penalty_order</code>: Type of smoothness (2 = penalize curvature)</li> </ul>"},{"location":"user-guide/quick-start/#automatic-parameter-selection","title":"Automatic Parameter Selection","text":"<p>Don't guess parameters - let the data decide:</p> <pre><code>from psplines.optimize import cross_validation, aic_selection\n\n# Cross-validation (recommended)\noptimal_lambda, cv_score = cross_validation(spline)\n\n# AIC (faster alternative)\noptimal_lambda, aic_score = aic_selection(spline)\n</code></pre>"},{"location":"user-guide/quick-start/#common-use-cases","title":"Common Use Cases","text":""},{"location":"user-guide/quick-start/#smoothing-noisy-measurements","title":"Smoothing Noisy Measurements","text":"<pre><code># Your experimental data\ntime_points = np.array([0, 1, 2, 3, 4, 5])\nmeasurements = np.array([1.0, 2.1, 2.9, 4.2, 4.8, 6.1])\n\n# Quick smooth\nspline = PSpline(time_points, measurements, nseg=10)\noptimal_lambda, _ = cross_validation(spline)\nspline.lambda_ = optimal_lambda\nspline.fit()\n\n# Get smooth curve\nsmooth_time = np.linspace(0, 5, 100)\nsmooth_measurements = spline.predict(smooth_time)\n</code></pre>"},{"location":"user-guide/quick-start/#derivative-estimation","title":"Derivative Estimation","text":"<pre><code># After fitting your spline...\nfirst_derivative = spline.derivative(smooth_time, deriv_order=1)\nsecond_derivative = spline.derivative(smooth_time, deriv_order=2)\n\nplt.subplot(3, 1, 1)\nplt.plot(smooth_time, spline.predict(smooth_time), label='Function')\n\nplt.subplot(3, 1, 2)\nplt.plot(smooth_time, first_derivative, label=\"f'(x)\")\n\nplt.subplot(3, 1, 3)\nplt.plot(smooth_time, second_derivative, label=\"f''(x)\")\n</code></pre>"},{"location":"user-guide/quick-start/#time-series-analysis","title":"Time Series Analysis","text":"<pre><code># Smooth time series data\ndates = pd.date_range('2020-01-01', periods=365, freq='D')\nvalues = your_time_series_data  # Your data here\n\n# Convert dates to numeric for fitting\nx_numeric = np.arange(len(dates))\n\n# Fit P-spline\nspline = PSpline(x_numeric, values, nseg=52)  # Weekly segments\noptimal_lambda, _ = cross_validation(spline)\nspline.lambda_ = optimal_lambda\nspline.fit()\n\n# Extract trend\ntrend = spline.predict(x_numeric)\n</code></pre>"},{"location":"user-guide/quick-start/#parameter-selection-guide","title":"Parameter Selection Guide","text":""},{"location":"user-guide/quick-start/#quick-rules-of-thumb","title":"Quick Rules of Thumb","text":"<p>Number of segments (<code>nseg</code>): - Start with <code>nseg = n_data_points / 4</code> - Minimum: 5-10 segments - Maximum: Usually no more than 50</p> <p>Smoothing parameter (<code>lambda_</code>): - Use automatic selection (cross-validation) - If manual: try values between 0.1 and 100</p>"},{"location":"user-guide/quick-start/#when-to-use-what","title":"When to Use What","text":"Scenario Recommendation Exploratory analysis <code>nseg=20</code>, cross-validation Very noisy data Higher <code>lambda_</code> or fewer segments Smooth underlying function More segments, moderate <code>lambda_</code> Need fast results AIC selection, fewer segments Critical application Cross-validation, bootstrap uncertainty"},{"location":"user-guide/quick-start/#error-handling-and-validation","title":"Error Handling and Validation","text":""},{"location":"user-guide/quick-start/#input-validation","title":"Input Validation","text":"<p>PSplines automatically validates inputs:</p> <pre><code>try:\n    spline = PSpline(x, y, nseg=20)\n    spline.fit()\nexcept ValueError as e:\n    print(f\"Input error: {e}\")\n</code></pre>"},{"location":"user-guide/quick-start/#model-diagnostics","title":"Model Diagnostics","text":"<p>Always check your model:</p> <pre><code># After fitting\nprint(f\"R\u00b2 \u2248 {1 - np.var(y - spline.predict(x)) / np.var(y):.3f}\")\nprint(f\"Effective DoF: {spline.ED:.1f}\")\n\n# Plot residuals\nresiduals = y - spline.predict(x)\nplt.scatter(spline.predict(x), residuals)\nplt.axhline(y=0, color='red', linestyle='--')\nplt.xlabel('Fitted values')\nplt.ylabel('Residuals')\n</code></pre>"},{"location":"user-guide/quick-start/#next-steps","title":"Next Steps","text":"<p>Now that you have the basics:</p> <ol> <li>Getting Started Guide: Deeper understanding of concepts</li> <li>Basic Usage Tutorial: Step-by-step walkthroughs  </li> <li>Parameter Selection Tutorial: Master optimization methods</li> <li>Examples Gallery: Real-world applications</li> <li>API Reference: Complete function documentation</li> </ol>"},{"location":"user-guide/quick-start/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user-guide/quick-start/#common-issues","title":"Common Issues","text":"<p>\"Spline too wiggly\": Increase <code>lambda_</code> or reduce <code>nseg</code> <pre><code>spline.lambda_ *= 10  # More smoothing\n</code></pre></p> <p>\"Spline too smooth\": Decrease <code>lambda_</code> or increase <code>nseg</code> <pre><code>spline.lambda_ /= 10  # Less smoothing\n</code></pre></p> <p>\"Fitting fails\": Check for: - Duplicate x values - Missing/infinite values - Very small datasets (need at least 5-10 points)</p> <p>\"Slow performance\":  - Reduce <code>nseg</code> for large datasets - Use AIC instead of cross-validation - Consider subsampling very large datasets</p>"},{"location":"user-guide/quick-start/#getting-help","title":"Getting Help","text":"<ul> <li>Documentation: Browse the full documentation</li> <li>Examples: Check the examples gallery  </li> <li>Issues: Report bugs on GitHub Issues</li> <li>Questions: Use GitHub Discussions</li> </ul> <p>Ready to dive deeper? Check out the comprehensive tutorials and examples to master P-spline smoothing for your specific applications!</p>"}]}